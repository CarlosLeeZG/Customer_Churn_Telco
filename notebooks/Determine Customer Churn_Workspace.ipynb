{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "- Query from 5 tables on `ai300_capstone` database\n",
    "- Data Dictionary: https://heicodersacademy.notion.site/Data-Dictionary-fc1b2fd7c2ad420494b9194ff44fc4be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carlo\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# General\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import warnings, os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data Prep\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# EDA\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import scipy.stats as ss\n",
    "\n",
    "# Feature Engr\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Train Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "# Hyperparameters Optimisation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import optuna\n",
    "from optuna.pruners import HyperbandPruner\n",
    "\n",
    "# Ensemble Learning\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Deployment\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>account_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>senior_citizen</th>\n",
       "      <th>married</th>\n",
       "      <th>num_dependents</th>\n",
       "      <th>tenure_months</th>\n",
       "      <th>num_referrals</th>\n",
       "      <th>has_internet_service</th>\n",
       "      <th>internet_type</th>\n",
       "      <th>has_unlimited_data</th>\n",
       "      <th>has_phone_service</th>\n",
       "      <th>has_multiple_lines</th>\n",
       "      <th>has_premium_tech_support</th>\n",
       "      <th>has_online_security</th>\n",
       "      <th>has_online_backup</th>\n",
       "      <th>has_device_protection</th>\n",
       "      <th>contract_type</th>\n",
       "      <th>paperless_billing</th>\n",
       "      <th>payment_method</th>\n",
       "      <th>avg_long_distance_fee_monthly</th>\n",
       "      <th>total_long_distance_fee</th>\n",
       "      <th>avg_gb_download_monthly</th>\n",
       "      <th>stream_tv</th>\n",
       "      <th>stream_movie</th>\n",
       "      <th>stream_music</th>\n",
       "      <th>total_monthly_fee</th>\n",
       "      <th>total_charges_quarter</th>\n",
       "      <th>total_refunds</th>\n",
       "      <th>area_id</th>\n",
       "      <th>city</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>population</th>\n",
       "      <th>status</th>\n",
       "      <th>churn_label</th>\n",
       "      <th>churn_category</th>\n",
       "      <th>churn_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002-ORFBO</td>\n",
       "      <td>93225</td>\n",
       "      <td>XSWV-PAYXZ</td>\n",
       "      <td>Female</td>\n",
       "      <td>37</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Cable</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>One Year</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>42.39</td>\n",
       "      <td>381.51</td>\n",
       "      <td>16</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>65.60</td>\n",
       "      <td>593.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>649</td>\n",
       "      <td>Frazier Park</td>\n",
       "      <td>34.827662</td>\n",
       "      <td>-118.999073</td>\n",
       "      <td>4498</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>No</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0003-MKNFE</td>\n",
       "      <td>91206</td>\n",
       "      <td>VFUN-NFDPJ</td>\n",
       "      <td>Male</td>\n",
       "      <td>46</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Cable</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-Month</td>\n",
       "      <td>No</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>10.69</td>\n",
       "      <td>96.21</td>\n",
       "      <td>10</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>59.90</td>\n",
       "      <td>542.40</td>\n",
       "      <td>38.33</td>\n",
       "      <td>184</td>\n",
       "      <td>Glendale</td>\n",
       "      <td>34.162515</td>\n",
       "      <td>-118.203869</td>\n",
       "      <td>31297</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>No</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0004-TLHLJ</td>\n",
       "      <td>92627</td>\n",
       "      <td>NFEJ-WVYXF</td>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fiber Optic</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Month-to-Month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Bank Withdrawal</td>\n",
       "      <td>33.65</td>\n",
       "      <td>134.60</td>\n",
       "      <td>30</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>73.90</td>\n",
       "      <td>280.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>535</td>\n",
       "      <td>Costa Mesa</td>\n",
       "      <td>33.645672</td>\n",
       "      <td>-117.922613</td>\n",
       "      <td>62069</td>\n",
       "      <td>Churned</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Competitor</td>\n",
       "      <td>Competitor had better devices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0011-IGKFF</td>\n",
       "      <td>94553</td>\n",
       "      <td>VOAC-QSDER</td>\n",
       "      <td>Male</td>\n",
       "      <td>78</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fiber Optic</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Month-to-Month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Bank Withdrawal</td>\n",
       "      <td>27.82</td>\n",
       "      <td>361.66</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>98.00</td>\n",
       "      <td>1237.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>960</td>\n",
       "      <td>Martinez</td>\n",
       "      <td>38.014457</td>\n",
       "      <td>-122.115432</td>\n",
       "      <td>46677</td>\n",
       "      <td>Churned</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Dissatisfaction</td>\n",
       "      <td>Product dissatisfaction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0013-EXCHZ</td>\n",
       "      <td>93010</td>\n",
       "      <td>BFIN-DLMOA</td>\n",
       "      <td>Female</td>\n",
       "      <td>75</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fiber Optic</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-Month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>7.38</td>\n",
       "      <td>22.14</td>\n",
       "      <td>11</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>83.90</td>\n",
       "      <td>267.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>607</td>\n",
       "      <td>Camarillo</td>\n",
       "      <td>34.227846</td>\n",
       "      <td>-119.079903</td>\n",
       "      <td>42853</td>\n",
       "      <td>Churned</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Dissatisfaction</td>\n",
       "      <td>Network reliability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7038</th>\n",
       "      <td>9987-LUTYD</td>\n",
       "      <td>91941</td>\n",
       "      <td>HDCC-OWUTY</td>\n",
       "      <td>Female</td>\n",
       "      <td>20</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One Year</td>\n",
       "      <td>No</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>46.68</td>\n",
       "      <td>606.84</td>\n",
       "      <td>59</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>55.15</td>\n",
       "      <td>742.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>302</td>\n",
       "      <td>La Mesa</td>\n",
       "      <td>32.759327</td>\n",
       "      <td>-116.997260</td>\n",
       "      <td>44652</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>No</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7039</th>\n",
       "      <td>9992-RRAMN</td>\n",
       "      <td>95367</td>\n",
       "      <td>TUYF-QYLVG</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fiber Optic</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-Month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Bank Withdrawal</td>\n",
       "      <td>16.20</td>\n",
       "      <td>356.40</td>\n",
       "      <td>17</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>85.10</td>\n",
       "      <td>1873.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1207</td>\n",
       "      <td>Riverbank</td>\n",
       "      <td>37.734971</td>\n",
       "      <td>-120.954271</td>\n",
       "      <td>16525</td>\n",
       "      <td>Churned</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Dissatisfaction</td>\n",
       "      <td>Product dissatisfaction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7040</th>\n",
       "      <td>9992-UJOEL</td>\n",
       "      <td>95432</td>\n",
       "      <td>PCTD-RXANG</td>\n",
       "      <td>Male</td>\n",
       "      <td>22</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-Month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>18.62</td>\n",
       "      <td>37.24</td>\n",
       "      <td>51</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>50.30</td>\n",
       "      <td>92.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1242</td>\n",
       "      <td>Elk</td>\n",
       "      <td>39.108252</td>\n",
       "      <td>-123.645121</td>\n",
       "      <td>383</td>\n",
       "      <td>Joined</td>\n",
       "      <td>No</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7041</th>\n",
       "      <td>9993-LHIEB</td>\n",
       "      <td>92075</td>\n",
       "      <td>JKGN-GMTXE</td>\n",
       "      <td>Male</td>\n",
       "      <td>21</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Cable</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Two Year</td>\n",
       "      <td>No</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2.12</td>\n",
       "      <td>142.04</td>\n",
       "      <td>58</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>67.85</td>\n",
       "      <td>4627.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>343</td>\n",
       "      <td>Solana Beach</td>\n",
       "      <td>33.001813</td>\n",
       "      <td>-117.263628</td>\n",
       "      <td>12173</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>No</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7042</th>\n",
       "      <td>9995-HOTOH</td>\n",
       "      <td>96125</td>\n",
       "      <td>MPLL-MUTJZ</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Cable</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Two Year</td>\n",
       "      <td>No</td>\n",
       "      <td>Bank Withdrawal</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>59.00</td>\n",
       "      <td>3707.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1610</td>\n",
       "      <td>Sierra City</td>\n",
       "      <td>39.600599</td>\n",
       "      <td>-120.636358</td>\n",
       "      <td>348</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>No</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7043 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     customer_id  zip_code  account_id  gender  age senior_citizen married  \\\n",
       "0     0002-ORFBO     93225  XSWV-PAYXZ  Female   37             No     Yes   \n",
       "1     0003-MKNFE     91206  VFUN-NFDPJ    Male   46             No      No   \n",
       "2     0004-TLHLJ     92627  NFEJ-WVYXF    Male   50             No      No   \n",
       "3     0011-IGKFF     94553  VOAC-QSDER    Male   78            Yes     Yes   \n",
       "4     0013-EXCHZ     93010  BFIN-DLMOA  Female   75            Yes     Yes   \n",
       "...          ...       ...         ...     ...  ...            ...     ...   \n",
       "7038  9987-LUTYD     91941  HDCC-OWUTY  Female   20             No      No   \n",
       "7039  9992-RRAMN     95367  TUYF-QYLVG    Male   40             No     Yes   \n",
       "7040  9992-UJOEL     95432  PCTD-RXANG    Male   22             No      No   \n",
       "7041  9993-LHIEB     92075  JKGN-GMTXE    Male   21             No     Yes   \n",
       "7042  9995-HOTOH     96125  MPLL-MUTJZ    Male   36             No     Yes   \n",
       "\n",
       "      num_dependents  tenure_months  num_referrals has_internet_service  \\\n",
       "0                  0              9              2                  Yes   \n",
       "1                  0              9              0                  Yes   \n",
       "2                  0              4              0                  Yes   \n",
       "3                  0             13              1                  Yes   \n",
       "4                  0              3              3                  Yes   \n",
       "...              ...            ...            ...                  ...   \n",
       "7038               0             13              0                  Yes   \n",
       "7039               0             22              1                  Yes   \n",
       "7040               0              2              0                  Yes   \n",
       "7041               0             67              5                  Yes   \n",
       "7042               0             63              1                  Yes   \n",
       "\n",
       "     internet_type has_unlimited_data has_phone_service has_multiple_lines  \\\n",
       "0            Cable                Yes               Yes                 No   \n",
       "1            Cable                 No               Yes                Yes   \n",
       "2      Fiber Optic                Yes               Yes                 No   \n",
       "3      Fiber Optic                Yes               Yes                 No   \n",
       "4      Fiber Optic                Yes               Yes                 No   \n",
       "...            ...                ...               ...                ...   \n",
       "7038           DSL                Yes               Yes                 No   \n",
       "7039   Fiber Optic                Yes               Yes                Yes   \n",
       "7040           DSL                Yes               Yes                 No   \n",
       "7041         Cable                Yes               Yes                 No   \n",
       "7042         Cable                Yes                No                 No   \n",
       "\n",
       "     has_premium_tech_support has_online_security has_online_backup  \\\n",
       "0                         Yes                  No               Yes   \n",
       "1                          No                  No                No   \n",
       "2                          No                  No                No   \n",
       "3                          No                  No               Yes   \n",
       "4                         Yes                  No                No   \n",
       "...                       ...                 ...               ...   \n",
       "7038                      Yes                 Yes                No   \n",
       "7039                       No                  No                No   \n",
       "7040                       No                  No               Yes   \n",
       "7041                      Yes                 Yes                No   \n",
       "7042                       No                 Yes               Yes   \n",
       "\n",
       "     has_device_protection   contract_type paperless_billing   payment_method  \\\n",
       "0                       No        One Year               Yes      Credit Card   \n",
       "1                       No  Month-to-Month                No      Credit Card   \n",
       "2                      Yes  Month-to-Month               Yes  Bank Withdrawal   \n",
       "3                      Yes  Month-to-Month               Yes  Bank Withdrawal   \n",
       "4                       No  Month-to-Month               Yes      Credit Card   \n",
       "...                    ...             ...               ...              ...   \n",
       "7038                    No        One Year                No      Credit Card   \n",
       "7039                    No  Month-to-Month               Yes  Bank Withdrawal   \n",
       "7040                    No  Month-to-Month               Yes      Credit Card   \n",
       "7041                   Yes        Two Year                No      Credit Card   \n",
       "7042                   Yes        Two Year                No  Bank Withdrawal   \n",
       "\n",
       "      avg_long_distance_fee_monthly  total_long_distance_fee  \\\n",
       "0                             42.39                   381.51   \n",
       "1                             10.69                    96.21   \n",
       "2                             33.65                   134.60   \n",
       "3                             27.82                   361.66   \n",
       "4                              7.38                    22.14   \n",
       "...                             ...                      ...   \n",
       "7038                          46.68                   606.84   \n",
       "7039                          16.20                   356.40   \n",
       "7040                          18.62                    37.24   \n",
       "7041                           2.12                   142.04   \n",
       "7042                           0.00                     0.00   \n",
       "\n",
       "      avg_gb_download_monthly stream_tv stream_movie stream_music  \\\n",
       "0                          16       Yes           No           No   \n",
       "1                          10        No          Yes          Yes   \n",
       "2                          30        No           No           No   \n",
       "3                           4       Yes          Yes           No   \n",
       "4                          11       Yes           No           No   \n",
       "...                       ...       ...          ...          ...   \n",
       "7038                       59        No           No          Yes   \n",
       "7039                       17        No          Yes          Yes   \n",
       "7040                       51        No           No           No   \n",
       "7041                       58        No          Yes          Yes   \n",
       "7042                        5       Yes          Yes          Yes   \n",
       "\n",
       "      total_monthly_fee  total_charges_quarter  total_refunds  area_id  \\\n",
       "0                 65.60                 593.30           0.00      649   \n",
       "1                 59.90                 542.40          38.33      184   \n",
       "2                 73.90                 280.85           0.00      535   \n",
       "3                 98.00                1237.85           0.00      960   \n",
       "4                 83.90                 267.40           0.00      607   \n",
       "...                 ...                    ...            ...      ...   \n",
       "7038              55.15                 742.90           0.00      302   \n",
       "7039              85.10                1873.70           0.00     1207   \n",
       "7040              50.30                  92.75           0.00     1242   \n",
       "7041              67.85                4627.65           0.00      343   \n",
       "7042              59.00                3707.60           0.00     1610   \n",
       "\n",
       "              city   latitude   longitude  population   status churn_label  \\\n",
       "0     Frazier Park  34.827662 -118.999073        4498   Stayed          No   \n",
       "1         Glendale  34.162515 -118.203869       31297   Stayed          No   \n",
       "2       Costa Mesa  33.645672 -117.922613       62069  Churned         Yes   \n",
       "3         Martinez  38.014457 -122.115432       46677  Churned         Yes   \n",
       "4        Camarillo  34.227846 -119.079903       42853  Churned         Yes   \n",
       "...            ...        ...         ...         ...      ...         ...   \n",
       "7038       La Mesa  32.759327 -116.997260       44652   Stayed          No   \n",
       "7039     Riverbank  37.734971 -120.954271       16525  Churned         Yes   \n",
       "7040           Elk  39.108252 -123.645121         383   Joined          No   \n",
       "7041  Solana Beach  33.001813 -117.263628       12173   Stayed          No   \n",
       "7042   Sierra City  39.600599 -120.636358         348   Stayed          No   \n",
       "\n",
       "       churn_category                   churn_reason  \n",
       "0                                                     \n",
       "1                                                     \n",
       "2          Competitor  Competitor had better devices  \n",
       "3     Dissatisfaction        Product dissatisfaction  \n",
       "4     Dissatisfaction            Network reliability  \n",
       "...               ...                            ...  \n",
       "7038                                                  \n",
       "7039  Dissatisfaction        Product dissatisfaction  \n",
       "7040                                                  \n",
       "7041                                                  \n",
       "7042                                                  \n",
       "\n",
       "[7043 rows x 40 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENDPOINT = 'heicoders-playground.c2ced10ceyki.ap-southeast-1.rds.amazonaws.com'\n",
    "PORT = 3306\n",
    "USERNAME = 'student300'\n",
    "PASSWORD = 'heicoders_AI300'\n",
    "DBNAME = 'ai300_capstone'\n",
    "\n",
    "database_conn = create_engine(f'mysql+pymysql://{USERNAME}:{PASSWORD}@{ENDPOINT}/{DBNAME}')\n",
    "\n",
    "## use \"Using\" will eliminate the duplicated joined columns\n",
    "query = \"\"\"\n",
    "    SELECT * \n",
    "        FROM customer c\n",
    "        LEFT JOIN account a\n",
    "            USING (customer_id)\n",
    "        LEFT JOIN account_usage au\n",
    "            USING(account_id)\n",
    "        LEFT JOIN city\n",
    "            USING(zip_code)\n",
    "        LEFT JOIN churn_status as cs\n",
    "            USING(customer_id)\n",
    "        ORDER BY c.customer_id, a.account_id\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, database_conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(r\"Data\\Customer_Churn_Data.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "## a. Data Types & NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7043 entries, 0 to 7042\n",
      "Data columns (total 40 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   customer_id                    7043 non-null   object \n",
      " 1   zip_code                       7043 non-null   int64  \n",
      " 2   account_id                     7043 non-null   object \n",
      " 3   gender                         7043 non-null   object \n",
      " 4   age                            7043 non-null   int64  \n",
      " 5   senior_citizen                 7043 non-null   object \n",
      " 6   married                        7043 non-null   object \n",
      " 7   num_dependents                 7043 non-null   int64  \n",
      " 8   tenure_months                  7043 non-null   int64  \n",
      " 9   num_referrals                  7043 non-null   int64  \n",
      " 10  has_internet_service           7043 non-null   object \n",
      " 11  internet_type                  7043 non-null   object \n",
      " 12  has_unlimited_data             7043 non-null   object \n",
      " 13  has_phone_service              7043 non-null   object \n",
      " 14  has_multiple_lines             7043 non-null   object \n",
      " 15  has_premium_tech_support       7043 non-null   object \n",
      " 16  has_online_security            7043 non-null   object \n",
      " 17  has_online_backup              7043 non-null   object \n",
      " 18  has_device_protection          7043 non-null   object \n",
      " 19  contract_type                  7043 non-null   object \n",
      " 20  paperless_billing              7043 non-null   object \n",
      " 21  payment_method                 7043 non-null   object \n",
      " 22  avg_long_distance_fee_monthly  7043 non-null   float64\n",
      " 23  total_long_distance_fee        7043 non-null   float64\n",
      " 24  avg_gb_download_monthly        7043 non-null   int64  \n",
      " 25  stream_tv                      7043 non-null   object \n",
      " 26  stream_movie                   7043 non-null   object \n",
      " 27  stream_music                   7043 non-null   object \n",
      " 28  total_monthly_fee              7043 non-null   float64\n",
      " 29  total_charges_quarter          7043 non-null   float64\n",
      " 30  total_refunds                  7043 non-null   float64\n",
      " 31  area_id                        7043 non-null   int64  \n",
      " 32  city                           7043 non-null   object \n",
      " 33  latitude                       7043 non-null   float64\n",
      " 34  longitude                      7043 non-null   float64\n",
      " 35  population                     7043 non-null   int64  \n",
      " 36  status                         7043 non-null   object \n",
      " 37  churn_label                    7043 non-null   object \n",
      " 38  churn_category                 7043 non-null   object \n",
      " 39  churn_reason                   7043 non-null   object \n",
      "dtypes: float64(7), int64(8), object(25)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- No NULL values but there are blank values\n",
    "- `zip_code`, `area_id`, `lattitude` & `longitude` are having incorrect data type. To be converted from float/int to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zip_code     object\n",
       "area_id      object\n",
       "latitude     object\n",
       "longitude    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_ls = ['zip_code', 'area_id', 'latitude', 'longitude']\n",
    "\n",
    "for dt in dt_ls:\n",
    "    df[dt] = df[dt].astype(str)\n",
    "\n",
    "df[dt_ls].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Info about US Postal Code:</b>\n",
    "- 1st Digit: Group of states or a specific region within a state\n",
    "- 2nd Digit: Narrows down the area within the state or region\n",
    "- 3rd Digit Further refines the area, typically specifying a city or a group of nearby cities.\n",
    "- 4th digit indicates a specific geographic segment within the city or region.\n",
    "- 5th digit, also known as the ZIP code suffix, represents a specific post office or delivery area\n",
    "\n",
    "To reduce the cardinality of zip code column, so that the com has enough RAM to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['93', '91', '92', '94', '95', '96', '90'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['zip_code'] = df['zip_code'].str[:2]\n",
    "df['zip_code'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Explore Numeric Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>7043.0</td>\n",
       "      <td>46.509726</td>\n",
       "      <td>16.750352</td>\n",
       "      <td>19.00</td>\n",
       "      <td>32.000</td>\n",
       "      <td>46.00</td>\n",
       "      <td>60.000</td>\n",
       "      <td>80.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_dependents</th>\n",
       "      <td>7043.0</td>\n",
       "      <td>0.468692</td>\n",
       "      <td>0.962802</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tenure_months</th>\n",
       "      <td>7043.0</td>\n",
       "      <td>32.386767</td>\n",
       "      <td>24.542061</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.000</td>\n",
       "      <td>29.00</td>\n",
       "      <td>55.000</td>\n",
       "      <td>72.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_referrals</th>\n",
       "      <td>7043.0</td>\n",
       "      <td>1.951867</td>\n",
       "      <td>3.001199</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.000</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_long_distance_fee_monthly</th>\n",
       "      <td>7043.0</td>\n",
       "      <td>22.958954</td>\n",
       "      <td>15.448113</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.210</td>\n",
       "      <td>22.89</td>\n",
       "      <td>36.395</td>\n",
       "      <td>49.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_long_distance_fee</th>\n",
       "      <td>7043.0</td>\n",
       "      <td>749.099262</td>\n",
       "      <td>846.660055</td>\n",
       "      <td>0.00</td>\n",
       "      <td>70.545</td>\n",
       "      <td>401.44</td>\n",
       "      <td>1191.100</td>\n",
       "      <td>3564.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_gb_download_monthly</th>\n",
       "      <td>7043.0</td>\n",
       "      <td>20.515405</td>\n",
       "      <td>20.418940</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.000</td>\n",
       "      <td>17.00</td>\n",
       "      <td>27.000</td>\n",
       "      <td>85.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_monthly_fee</th>\n",
       "      <td>7043.0</td>\n",
       "      <td>64.761692</td>\n",
       "      <td>30.090047</td>\n",
       "      <td>18.25</td>\n",
       "      <td>35.500</td>\n",
       "      <td>70.35</td>\n",
       "      <td>89.850</td>\n",
       "      <td>118.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_charges_quarter</th>\n",
       "      <td>7043.0</td>\n",
       "      <td>2280.381264</td>\n",
       "      <td>2266.220462</td>\n",
       "      <td>18.80</td>\n",
       "      <td>400.150</td>\n",
       "      <td>1394.55</td>\n",
       "      <td>3786.600</td>\n",
       "      <td>8684.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_refunds</th>\n",
       "      <td>7043.0</td>\n",
       "      <td>1.962182</td>\n",
       "      <td>7.902614</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>49.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>population</th>\n",
       "      <td>7043.0</td>\n",
       "      <td>22139.603294</td>\n",
       "      <td>21152.392837</td>\n",
       "      <td>11.00</td>\n",
       "      <td>2344.000</td>\n",
       "      <td>17554.00</td>\n",
       "      <td>36125.000</td>\n",
       "      <td>105285.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                count          mean           std    min  \\\n",
       "age                            7043.0     46.509726     16.750352  19.00   \n",
       "num_dependents                 7043.0      0.468692      0.962802   0.00   \n",
       "tenure_months                  7043.0     32.386767     24.542061   1.00   \n",
       "num_referrals                  7043.0      1.951867      3.001199   0.00   \n",
       "avg_long_distance_fee_monthly  7043.0     22.958954     15.448113   0.00   \n",
       "total_long_distance_fee        7043.0    749.099262    846.660055   0.00   \n",
       "avg_gb_download_monthly        7043.0     20.515405     20.418940   0.00   \n",
       "total_monthly_fee              7043.0     64.761692     30.090047  18.25   \n",
       "total_charges_quarter          7043.0   2280.381264   2266.220462  18.80   \n",
       "total_refunds                  7043.0      1.962182      7.902614   0.00   \n",
       "population                     7043.0  22139.603294  21152.392837  11.00   \n",
       "\n",
       "                                    25%       50%        75%        max  \n",
       "age                              32.000     46.00     60.000      80.00  \n",
       "num_dependents                    0.000      0.00      0.000       9.00  \n",
       "tenure_months                     9.000     29.00     55.000      72.00  \n",
       "num_referrals                     0.000      0.00      3.000      11.00  \n",
       "avg_long_distance_fee_monthly     9.210     22.89     36.395      49.99  \n",
       "total_long_distance_fee          70.545    401.44   1191.100    3564.72  \n",
       "avg_gb_download_monthly           3.000     17.00     27.000      85.00  \n",
       "total_monthly_fee                35.500     70.35     89.850     118.75  \n",
       "total_charges_quarter           400.150   1394.55   3786.600    8684.80  \n",
       "total_refunds                     0.000      0.00      0.000      49.79  \n",
       "population                     2344.000  17554.00  36125.000  105285.00  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(exclude = ['object']).describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABMsAAAOlCAYAAABzNGAGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzde3yP9f/H8efHzpsZG9uMsdGccsz5kJHzsagkOZVKCS3kkGQkaiRFqSTk2IkSEsJKyFIKlcO3OcVMDhuase39+6Pt+vnYxsZmG4/77fa53Xze1+u6rvd1fT6f67Xr5X1dl80YYwQAAAAAAABAhfK6AwAAAAAAAEB+QbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAgFTNmjWTzWbTxo0b87orkqSgoCDZbDYdOHDArj2/9VPKn33KSZ9//rkaNGggDw8P2Ww22Wy2vO7SLeXAgQOy2WwKCgrK667cVDm93XPnzpXNZlPfvn3ztB8AABR0FMsAALeEtMJS2qtQoUIqUqSIAgMD1apVK7344ov6/fffb0pfpk2bpvDwcJ05c+amrC+3bdy4UeHh4bdsIexa1q5dqwceeEA//vijypQpo8aNG6tx48bXnC+tAGGz2eTu7q5jx45lGuvo6JhhYRQ5Y82aNbLZbHJzc1N8fPw142NjY+Xk5CSbzaaoqKib0EMAAJCfUCwDANxSQkJC1LhxYzVq1EgVKlSQg4OD1q1bp1deeUV33nmnHnjgAZ08eTLDecuUKaOKFSvK3d39hvowbdo0jRs37oaLZeXLl1fFihXl5OR0Q8u5URs3btS4ceOuWizLqX2XH82cOVOSNGXKFP3xxx/atGmTNm3alK1lJCQkaNKkSbnRvVuCk5OTKlasqPLly+fK8lu2bKmAgABduHBBn3/++TXjlyxZoqSkJFWsWFF169bNlT5Jub/dAADg+jjmdQcAAMhJL7zwQrpLkP755x8tXLhQEyZM0Oeff67du3dr69at8vLysov76KOPbmJPr+3bb7/N6y5kWX7bdznpzz//lCS1b9/+updRqFAhvf/++xoxYoRKlSqVU127ZZQqVcraz7mhUKFC6tGjh6ZMmaIFCxbo0UcfvWr8ggULJEm9evXKtT5Jub/dAADg+jCyDABwyytevLieffZZ/fTTTypZsqT+/PNPhYWF5XW3UEAkJCRIktzc3K5rfgcHB3Xr1k2JiYmaOHFiTnYN2ZBW+Nq4caOOHj2aadzevXsVFRUlm82mRx555GZ1DwAA5CMUywAAt42yZcvqnXfekfTfyJHDhw/bTc/sJvVJSUl68803Va9ePXl6esrFxUUBAQFq1KiRxo4da11umXZz7YMHD0qSgoOD7e6jlrbcjRs3ymazqVmzZkpKSlJERISqVasmd3d3uxtsZ3aD/8tt27ZNHTp0kLe3tzw8PNSoUSN98cUXGcZe6yb8ffv2lc1m09y5c602m82mcePGSZLGjRtntz2Xj+C72rKNMVqwYIFCQ0NVtGhRubm5qVKlShoxYoROnTqVYV8uv4n+119/raZNm8rT01NeXl5q166dfvnll0z3ydWcP39eEyZMUPXq1eXh4aEiRYqofv36evvtt5WUlGQXm7ZNafv/8s8zPDw8W+sdO3asChUqpA8++CDd9+5qrvUdyGy/X97+22+/6d5771Xx4sVVpEgRtWzZUj/99JMV+/3336tt27by9vaWp6enOnTocNXRTv/++69ee+011alTR0WKFJG7u7tq1qypyZMnKzExMV18eHi4tc9OnDihgQMHKigoSE5OTtZ36Fo3mE9KStKsWbPUvHlz+fj4yNXVVeXKldP999+vL7/88qr7ME316tVVvXp1paSkaNGiRZnGpY0qu/vuu63+bN26VcOHD1edOnXk6+srFxcXBQYGqlevXtq9e3eGy7nR7d61a5fGjh2rhg0bqmTJknJ2dlbJkiXVtWtXbd68+Zrbe/bsWQ0ZMkRBQUHW/ho9erT+/fffa857paSkJL377rtq0qSJihYtKldXV1WqVEkvvvhipveA++qrr9SmTRsVL15cTk5OKlGihKpXr65Bgwbpjz/+yHYfAAC4mSiWAQBuK507d1ZAQICSkpK0Zs2aLM3TvXt3hYWFKSoqSn5+fqpRo4YcHR21bds2jR8/3ipk+Pn5qXHjxnJxcZEk1alTx7oZfOPGjdNd9mmM0X333acRI0YoISFBVapUUeHChbO8Ld9//73uvvtufffddypfvry8vLy0ZcsWdenSRVOnTs3ycq6mcePGCgwMlCQFBgbabU+FChWuOb8xRj179lSvXr303XffycfHR1WqVFF0dLQiIiJ011136a+//sp0/nfffVcdOnTQ/v37VaFCBSUnJ2v16tVq2rRpti9fO3HihBo2bKgxY8Zo9+7duuOOO1S6dGlt27ZNAwcOVPv27XXhwgUrvlq1apl+nmXKlMnWuitVqqSHH35YFy9e1IQJE7I174348ccf1aBBA3333XcKCgpSoUKF9O233+qee+7R7t279emnn+qee+7RL7/8ouDgYKWkpGjVqlVq2rSpjh8/nm55f//9t+rWrauRI0fq119/lZ+fn4KCgrR7924NHz5cLVu2tEbiXenEiROqU6eO3n33XXl5ealKlSpycHC45jacPn1azZo105NPPqmNGzfK09NT1apV0/nz57V06VI9++yzWd4faaPL0gpiGVm4cKFdrCT17NlTkydP1oEDB+Tn56fKlSvr7NmzWrBggerWrXvV+/ld73aHhYVp/Pjx+vPPP1WsWDFVq1ZNSUlJWrZsmZo2bXrVgl9iYqJCQ0M1bdo0FS5cWCEhITpw4IAmTpyoFi1aZKtgFh8frxYtWujpp5/Wli1bVLRoUYWEhCg6OlqvvPKKGjRooNjYWLt5ZsyYoc6dO2vNmjVycnJSzZo1VaxYMe3bt08zZszQN998k+X1AwCQJwwAALeAsmXLGklmzpw514y9//77jSTTv39/u/bQ0FAjyWzYsMFq++mnn4wkExgYaH7//Xe7+Li4ODNr1ixz6NChDPsSHR2d4fo3bNhgJBkHBwfj6+trNm/ebE1LSEi45nLS+uno6Gi6d+9uzp07Z4wxJiUlxbz11lvWtB07dlxz+y7Xp0+fDPfh2LFjjSQzduzYDOe72rKnT59uJBlPT0+zZs0aq/3YsWOmcePGRpKpX79+uuVJMpKMu7u7XX/i4+NNixYtjCTz0EMPZdqfjKR97nfeeafZv3+/1R4VFWX8/PyMJDN8+PB0813r88xMdHS09TkbY8zevXuNg4ODcXJySrcsBweHDNdxrXVntt/T2p2cnMyQIUNMYmKiMcaYCxcumHvvvddIMs2aNTNFixY1r7/+uklOTjbGGHP69GlTr169DPdFcnKyadSokZFkunfvbmJiYqxphw8fNnfffbeRZIYNG2Y3X9r3x8HBwTRs2NAcPnzYmpb2fU/bV2XLlk23jffdd5+RZMqXL2+2bt1qN23fvn0mIiIiw32TkaNHj1r7eteuXemm//DDD0aScXV1NWfOnLHa582bZ/73v//ZxV66dMl88MEHxtHR0ZQrV87ahzm13Z9++qn57bff7NpSUlLMF198YQoXLmyKFCli4uPj7abPmTPH+v2XKlXK7hiwc+dOExgYmOFndLV+dO/e3UgyLVq0sNsHp06dMl27djWSzAMPPGC3X4oVK2YcHR3NsmXL0u2zr776ykRGRqZbDwAA+QnFMgDALSE7xbKwsDAjyXTp0sWuPaPCw+LFi40k89xzz2W7L9cqlkkyn3/+ebaXk9ZPX19fu+JamrQT2N69e19z+y6X08WylJQU6+T8jTfeSDfPkSNHjLOzs5Fkvv32W7tpaftn0KBB6eb77bffjCTj5eWVaX+utHfvXmOz2Ywk8/PPP6eb/sknnxhJxsPDI10BIqeKZcb8/z7u16+fXWxuFctq1aplUlJS7Kbt2bPH2r/33ntvumWuXr3aSDLVq1e3a1++fLmRZOrWrWsuXbqUbr6jR4+awoULm8KFC5t///3Xak/7/ri4uJi///47w+3IrFizbds2a969e/dmOG92tW7d2kgyI0eOTDft6aefNpLMgw8+mOXl9ezZ00gyP/zwg137jWz3tbz44otGklm4cKFde1qxTJJZunRpuvnSPsMrv+eZ9ePXX3+12q/8XRhjzPnz501gYKCx2WzmwIEDxpj/CuFp3z0AAAoqLsMEANx2PDw8JP13T59rSbsE8dtvv830/lrXy8vLS/fee+91z9+vXz+5urqmax8wYIAk5fmlTn/88YcOHz4sV1dXPfHEE+mmlypVSvfff78kZXpJ7OOPP56urVq1anJ1dVVcXJxOnjyZpb6sXbtWxhg1adJEtWrVSjf9/vvvV+nSpXX+/Hn98MMPWVrm9RgzZowcHR01b968q15+mlMeffRR695vaSpUqCB3d3dJ/32HrpS2f67s39KlSyX9d287R8f0D1QvWbKk6tatq3Pnzmn79u3pprds2VIBAQHZ6n/a/ci6dOmikJCQbM2bmbTLKxctWiRjjNV+6dIlffLJJ3Yxl/vzzz81duxYde3aVc2aNVOTJk3UpEkTRUZGSpJ+/fXXDNd3Pdud5tChQ3r11VfVrVs33XPPPdY6P/7446uus1SpUhkeWzp27KgyZcpk+Xu+bNkySVK3bt3k6emZbrq7u7tatmwpY4y+//57SVKJEiXk4uKivXv3Zto/AADyu/R/6QAAcIs7d+6cJKlIkSLXjG3YsKHq16+vH3/8UYGBgWrVqpWaNm2q0NBQ3XXXXekKEdkREhKSpXsXZaZy5cpXbT9+/Lji4+OztJ25Ye/evZKkMmXKWAXKK9155512sVcqX758hu0lSpTQ4cOHde7cOfn4+GS5L1WqVMlweqFChVSpUiUdOXJEe/fuVdu2ba+5zOtRvnx59e7dWx9++KFefvllzZkzJ1fWc/n6MlK8eHEdOnQow+klSpSQ9P+/kzQ7d+6UJM2cOTPT+2Wl7ee///473bTMvq9Xk3Yj+AYNGmR73sx06dJFhQsX1qFDh/T999+radOmkv57kMTJkydVvHjxdJ//pEmT9OKLLyolJSXT5WZWTL+e7ZakefPm6amnnrK7j15W11mxYkUVKpT+/8RtNpsqVqyoQ4cOZel7nvaZL1u2LNOHCqQ90CTtM3dwcNDgwYM1efJk3XXXXWrcuLGaN2+uu+++W02aNMmwwA8AQH7DyDIAwG3n0KFDkiRfX99rxhYqVEhff/21nn32Wbm5uenLL7/U0KFDVadOHQUHB9s9OTK7MisgZVVm/b+8PSuj53JLWrHlavvZz89PUub9zGwfpRUCLh8ZlNt9ySljxoyRk5OT5s+fr3379uXqutJGkF0prcib0fTMCsBxcXGS/ntK4w8//JDh68SJE5KU4U3+r+f7nvakxaJFi2Z73sx4eHioS5cukuxv9J/27+7du8vJyclq/+677/TCCy/IZrNp0qRJ2r17t86dO6eUlBQZYzR69GhJ/41My2x92fW///1PTzzxhC5cuKChQ4fql19+UXx8vLXOWbNmXXWdOfU9T/vM9+/fn+lnfuTIEUn2n/mrr76qadOmqXz58vr+++81fvx4tWrVSn5+fho1alSGT00FACA/oVgGALitpKSkaMuWLZKkevXqZWmeYsWKadq0aTpx4oR++eUXvfnmm2revLkOHjyoRx99VJ999lludjlTaYWJq7VffulUWhEkswLT+fPnc7B3sp7seeWT8i6X9sTFjC7xulX7EhQUpL59+yo5OVnjx4+/auzN/syuJm0fpl3SerVX3759c2SdaZ/FmTNncmR5adIus/zss8+UmJio+Ph4ffXVV3bT0qQ9HfP555/XyJEjVaVKFXl4eFifzeHDh3O0b5L0ySef6NKlS+revbumTJmimjVrytPTM8vrzOzYIP3/byAr3/O0z3zWrFnX/MzDw8Ot+QoVKqRnn31We/fuVXR0tObNm6fu3bvrwoULevXVVzV06NBrrhsAgLxEsQwAcFv54osvFBMTIycnJ7Vu3Tpb89psNtWsWVODBw/W+vXrNXLkSEmyRnlcHnczpF2illm7n5+f3SWYaSNcMjuR3r9/f4bt17s9FSpUkPTfSL4rL+lLs3v3brvY3JK2/N9//z3D6SkpKfrzzz9vSl8k6cUXX5Szs7MWL16sPXv2ZBp3rc/sf//7X670LyNpl7Du2rXrpq0z7TLdrVu35uhyW7RooVKlSun06dNatWqVPvvsM124cEEVKlRIV0Q/cOCAJKlRo0YZLis37st1o+vcs2dPhpeMGmOs71tWvuc58ZkHBQWpd+/eWrx4sZYvXy5J+vDDD696SSsAAHmNYhkA4LZx8OBBDRw4UJLUu3dvlSpV6oaWl3YfpaNHj9q1u7m5Scr4UrScNHv27AwvZ3rnnXckKV0xsFy5cpKkqKiodPP89NNPmZ6AX+/2VK5cWWXKlNGFCxf0wQcfpJt+9OhRff7555KkNm3aZGvZ2dW6dWvZbDZt2rRJv/zyS7rpS5cu1ZEjR+Th4aHGjRvnal+k/+7j1q9fPyUnJ2vcuHGZxl3tM/v88891+vTpXOvjlbp27SpJeu+99656H62cdN9990n6r8idk4XBQoUKqUePHpL+u/wy7RLMjG7sn/b9Txt5eLk1a9bkSrHsauv8888/rVFwmTly5EiGMStXrtTBgwez/D2//HLVrD5M42rSjpkJCQk39bsLAEB2USwDANzy/vnnH7311luqU6eOjh07pipVqmjq1KlZmnfhwoV6+eWXrZEeaU6ePKm33npLknTXXXfZTUsrcKQ9JS+3nDx5Uv369bMuxTPG6J133tHSpUvl4OCgIUOG2MW3a9dO0n8j4bZt22a179u3T3369MnwCYfS/2/P5s2blZSUlOX+2Ww2Pf/885KksWPH6ttvv7WmHT9+XN27d9fFixfVoEEDNW/ePMvLvR533HGHVezp3bu33ZMef/75Zw0ePFiSNHDgwFy/DDPNCy+8IBcXF3388cdKTk7OMCbtM4uIiLC7v1lUVJQGDx5sd2+t3NalSxc1aNBAf/75pzp16pRuJGJiYqJWrlypxx57LMfWWbt2bXXp0kUXLlxQu3bt0hUN9+/frylTplzXstMKYytWrFBkZKRsNpseeeSRdHFNmjSR9N99uKKjo632qKgoPfbYY7lyw/q0db7zzjvasWOH1b537149+OCDcnZ2vur8jo6OGjRokHWDfum/UZVp/1nw1FNPZel7XqdOHXXr1k0nT55Uq1at0hWak5OTtXHjRj3yyCNW4f73339X//79FRUVZXf5cGJiol555RVJUtmyZbP0YA4AAPKMAQDgFlC2bFkjyYSEhJjGjRubxo0bmzp16pigoCAjyXo9+OCD5uTJkxkuIzQ01EgyGzZssNreeOMNa95SpUqZunXrmqpVqxpnZ2er7eDBg3bL+eijj6x5qlatakJDQ01oaKj55ZdfjDHGbNiwwUgyoaGhWdqm6OjoDPs5fvx44+zsbDw9PU2dOnVMQECAtd6IiIh0y0tJSTEtW7Y0kkyhQoVMxYoVTdWqVU2hQoVM06ZNTY8ePYwkM2fOHLv54uLiTLFixYwkU7JkSdO4cWMTGhpqJk2adNV9l7bOtOVKMnfccYe56667rP1XpkwZ87///S9dX9Pis7tvriY2NtZUq1bNSDIODg6mRo0apkqVKta6WrZsaRISEnJkXcYYEx0dba0rMwMHDrT7fl65joSEBHPnnXcaScbR0dFUrVrVVKhQwUgy3bt3z3S/Z9ae1W3KbP8fPXrU1KpVy+7zrF+/vqlSpYr1mfr5+dnNM3bsWCPJjB07NtP9kLavypYtm27aqVOnTMOGDa11BgUFmTp16hg/P79M58mqGjVqWMu9++67M4yJi4sz5cqVM5KMs7OzqVatmqlYsaKRZKpUqWKGDBmS4fbdyHZfunTJNGjQwPr+VK5c2VStWtXYbDZTsmRJM2HCBCPJ9OnTx26+OXPmWN+NWrVqGZvNZqpWrWqqVatmbDabkWTq1q1rzp07l6V+GGPM2bNnTatWraz9VKZMGVO/fn1TrVo14+bmZrWn/XZ++eUXq61o0aLmrrvuMrVq1TJeXl7WPly1alWm+wQAgPyAkWUAgFvKvn37rKe0/fnnn0pKSlLLli01evRo/f777/rkk0/k7e2d5eXdf//9eu2119SqVSs5ODho586dOnbsmKpWraoJEyZo165dKlOmjN08vXr10ptvvqnq1avrf//7nyIjIxUZGZnjNym/++679f3336tJkybav3+/Tp8+rQYNGmjp0qXWiK7L2Ww2LVu2TEOGDFFAQICio6N1/vx5jRo1SmvWrMl0lFKRIkW0Zs0atWvXTomJidqyZYsiIyOte3xdjc1m04IFC/TRRx/p7rvvVmxsrHbv3q2yZcvq+eef188//2yNXMttJUqU0JYtWzR+/HhVrlxZe/fu1cGDB1W3bl1Nnz5dq1atypVRQlfzwgsvXHWdrq6uWr9+vfr16ydvb2/t27dPhQoV0pQpU6wbz99MJUuW1JYtW/TOO++oadOmOnnypH755RedPXtW9erV07hx47Rhw4YcXWexYsUUGRmpt99+W40bN9bp06e1a9cuubu764EHHtCMGTOue9mXX3bZs2fPDGOKFCmiTZs2qXfv3ipSpIj27NmjixcvasiQIdqyZUuujER0dHTUN998o0GDBsnPz0/79+/XmTNn1K9fP23fvv2al5C7uLgoMjJSzz77rOLj47Vnzx6VKVNGI0eO1IYNG7L1hM7ChQtr9erVWrhwodq0aaN///1XP//8s/755x9Vr15dI0aM0LZt26zvcUhIiGbNmqUHH3xQJUqU0N69e7Vv3z6VKlVKTz31lH7//XdrxCQAAPmVzZgsPnMdAAAAAAAAuMUxsgwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAACAfCgoKUt++ffO6GzmmWbNmatasWa6uY/PmzQoPD9eZM2dydT0AkBM4ZuUfq1atUnh4eIbTbDabBg4ceHM7hDxHsQwAAAC3hM2bN2vcuHGceAIoEDhm5R+rVq3SuHHj8robyEcolgEAAAAAgAz9+++/ed0F4KajWAZkwf79+/Xoo48qJCRE7u7uKlWqlDp16qSdO3emi929e7dat24td3d3lShRQs8884xWrlwpm82mjRs32sWuW7dOLVq0UJEiReTu7q7GjRvr22+/vUlbBQC3nvDwcNlsNu3evVsPP/ywvLy85Ofnp8cee0xxcXGSpAMHDshms2nu3Lnp5rfZbHaXYaQt77ffftODDz4oLy8veXt7a8iQIUpKStKePXvUtm1beXp6KigoSBEREdnu86VLlzR8+HD5+/vL3d1dTZo00bZt2zKMjYmJUf/+/VW6dGk5OzsrODhY48aNU1JSkhWTtn0RERF65ZVXVKZMGbm6uqpOnToZ5ph9+/apR48e8vX1lYuLiypXrqy3337bLmbjxo2y2WxavHixRo8erYCAABUpUkQtW7bUnj177GKNMYqIiFDZsmXl6uqqu+66S19//XWG2xMfH69hw4YpODhYzs7OKlWqlMLCwnT+/Hm7uLRLYObPn6/KlSvL3d1dNWrU0IoVK6yY8PBwPf/885Kk4OBg2Ww2u9y7fv16NWvWTD4+PnJzc1OZMmV0//33cxIIIE9c65j18ccfq2HDhvLw8FDhwoXVpk0b/fLLL3bL6Nu3rwoXLqz9+/erffv2Kly4sAIDAzV06FAlJiZacWnH8CvPRTLKh2nL3Llzp1q3bi1PT0+1aNFCknTx4kVNmDBBlSpVkouLi0qUKKFHH31UJ06cyPa232huPXTokHr27GmXu15//XWlpKSk274pU6Zo6tSpCg4OVuHChdWwYUNt3brVbpvT8l7a52Cz2XTgwAG7dV4tB0nSiRMn9OSTTyowMNDaP40bN9a6deuytX+QPzjmdQeAguDo0aPy8fHRq6++qhIlSujUqVOaN2+e6tevr19++UUVK1aUJB07dkyhoaHy8PDQzJkz5evrq8WLF2d4jfuCBQvUu3dv3XvvvZo3b56cnJz03nvvqU2bNvrmm2+spAQAyL77779fDz30kPr166edO3dq1KhRkqQPP/zwupbXrVs39ezZU/3799fatWsVERGhS5cuad26dRowYICGDRumRYsWacSIEbrjjjvUtWvXLC/7iSee0EcffaRhw4apVatW2rVrl7p27aqzZ8/axcXExKhevXoqVKiQXnrpJZUvX15btmzRhAkTdODAAc2ZM8cufsaMGSpbtqymTZumlJQURUREqF27doqMjFTDhg0lSb///rsaNWqkMmXK6PXXX5e/v7+++eYbDR48WP/884/Gjh1rt8wXXnhBjRs31gcffKD4+HiNGDFCnTp10h9//CEHBwdJ0rhx4zRu3Dj169dPDzzwgA4fPqwnnnhCycnJVr6U/hupEBoaqiNHjuiFF15Q9erVtXv3br300kvauXOn1q1bJ5vNZsWvXLlSUVFRGj9+vAoXLqyIiAh16dJFe/bsUbly5fT444/r1KlTmj59upYuXaqSJUtKkqpUqaIDBw6oQ4cOuvvuu/Xhhx+qaNGi+vvvv7V69WpdvHhR7u7uWf68ACAnXO2YNXHiRL344ot69NFH9eKLL+rixYuaPHmy7r77bm3btk1VqlSxlnPp0iV17txZ/fr109ChQ/Xdd9/p5ZdflpeXl1566aXr6tvFixfVuXNn9e/fXyNHjlRSUpJSUlJ077336vvvv9fw4cPVqFEjHTx4UGPHjlWzZs30008/yc3NLVvrud7ceuLECTVq1EgXL17Uyy+/rKCgIK1YsULDhg3T//73P73zzjt263n77bdVqVIlTZs2TZI0ZswYtW/fXtHR0fLy8tKYMWN0/vx5ffbZZ9qyZYs1X9pnIl07B0lSr1699PPPP+uVV15RhQoVdObMGf388886efLk9XwMyGsGQLYlJSWZixcvmpCQEPPcc89Z7c8//7yx2Wxm9+7ddvFt2rQxksyGDRuMMcacP3/eeHt7m06dOtnFJScnmxo1aph69erl+jYAwK1o7NixRpKJiIiwax8wYIBxdXU1KSkpJjo62kgyc+bMSTe/JDN27Nh0y3v99dft4mrWrGkkmaVLl1ptly5dMiVKlDBdu3bNcn//+OMPI8kulxhjzMKFC40k06dPH6utf//+pnDhwubgwYN2sVOmTDGSrNyTtn0BAQEmISHBiouPjzfe3t6mZcuWVlubNm1M6dKlTVxcnN0yBw4caFxdXc2pU6eMMcZs2LDBSDLt27e3i/vkk0+MJLNlyxZjjDGnT582rq6upkuXLnZxP/zwg5FkQkNDrbZJkyaZQoUKmaioKLvYzz77zEgyq1atstokGT8/PxMfH2+1xcTEmEKFCplJkyZZbZMnTzaSTHR0dIbL3LFjhwGA/CKjY9ahQ4eMo6OjGTRokF3s2bNnjb+/v+nWrZvV1qdPHyPJfPLJJ3ax7du3NxUrVrTepx3D085F0mSUD9OW+eGHH9rFLl682Egyn3/+uV17VFSUkWTeeeedLG/3jebWkSNHGknmxx9/tJv/6aefNjabzezZs8du+6pVq2aSkpKsuG3bthlJZvHixVbbM888YzIrj2Q1BxUuXNiEhYVleT8gf+MyTCALkpKSNHHiRFWpUkXOzs5ydHSUs7Oz9u3bpz/++MOKi4yMVNWqVe3+t0eSHn74Ybv3mzdv1qlTp9SnTx8lJSVZr5SUFLVt21ZRUVHpLkEBAGRd586d7d5Xr15dFy5cUGxs7HUtr2PHjnbvK1euLJvNpnbt2lltjo6OuuOOO3Tw4MEsL3fDhg2SpEceecSuvVu3bnJ0tL8AYMWKFWrevLkCAgLsckdaHyIjI+3iu3btKldXV+u9p6enOnXqpO+++07Jycm6cOGCvv32W3Xp0kXu7u52y2zfvr0uXLhgd5mKlPF+lWRt85YtW3ThwoV029OoUSOVLVs23fZUrVpVNWvWtFt3mzZtMrxcqHnz5vL09LTe+/n5ydfXN0v7u2bNmnJ2dtaTTz6pefPm6a+//rrmPACQF7755hslJSWpd+/edsdGV1dXhYaGpjs22mw2derUya6tevXq2cpFGbn//vvt3q9YsUJFixZVp06d7PpVs2ZN+fv7p+tXVlxvbl2/fr2qVKmievXq2c3ft29fGWO0fv16u/YOHTpYo5+l9LkrK7KSg+rVq6e5c+dqwoQJ2rp1qy5dupTl5SP/4TJMIAuGDBmit99+WyNGjFBoaKiKFSumQoUK6fHHH1dCQoIVd/LkSQUHB6eb38/Pz+798ePHJUkPPPBApus8deqUPDw8cmgLAOD24uPjY/fexcVFkuyO2dnh7e1t997Z2Vnu7u52xai09vj4+CwvN+3SDH9/f7t2R0fHdNtw/PhxffXVV3JycspwWf/884/d+yuXmdZ28eJFnTt3TufOnVNSUpKmT5+u6dOnZ2mZ19qvmW1PRm3Hjx/X/v37s7w9V647bf1Z+UzLly+vdevWKSIiQs8884zOnz+vcuXKafDgwXr22WevOT8A3Cxp5wl169bNcHqhQvbjXTLKRS4uLrpw4cJ198Hd3V1FihRJ168zZ87I2dk5w3muPGZnxfXm1pMnTyooKCjd8gICAqzpl8uJvwmykoM+/vhjTZgwQR988IHGjBmjwoULq0uXLoqIiMgwLyJ/o1gGZEHa/cUmTpxo1/7PP/+oaNGi1nsfHx8rwV0uJibG7n3x4sUlSdOnT1eDBg0yXOeVBTYAQM5I+yP88psfS+n/uL4Z0v74jomJUalSpaz2pKSkdP0pXry4qlevrldeeSXDZaWdJKS5MvektTk7O6tw4cJycnKSg4ODevXqpWeeeSbDZWb0H0BZ3Z6M1n35yU3x4sXl5uaW6X3k0nJlTrn77rt19913Kzk5WT/99JOmT5+usLAw+fn5qXv37jm6LgC4XmnHvs8++yzdiNzrlVney6zAdfn9Ii/vl4+Pj1avXp3hPJePusptPj4+OnbsWLr2o0ePSsr5/JFVxYsX17Rp0zRt2jQdOnRIy5cv18iRIxUbG5vpfkP+RbEMyAKbzWb9D0SalStX6u+//9Ydd9xhtYWGhmrKlCn6/fff7S7FXLJkid28jRs3VtGiRfX7779nePN/AEDu8fPzk6urq3777Te79i+//PKm96VZs2aSpIULF6p27dpW+yeffGL3hEvpv8tVVq1apfLly6tYsWLXXPbSpUs1efJk6yTp7Nmz+uqrr3T33XfLwcFB7u7uat68uX755RdVr14909EC2dGgQQO5urpq4cKFdpfwbN68WQcPHrQrlnXs2FETJ06Uj49PtotymcnKaAEHBwfVr19flSpV0sKFC/Xzzz9TLAOQJzI6ZrVp00aOjo763//+l+5SyOuVduz97bff1KZNG6t9+fLlWV5Gx44dtWTJEiUnJ6t+/fo50q/r1aJFC02aNEk///yz7rrrLqv9o48+ks1mU/PmzbO9zMs/i+w+qCAjZcqU0cCBA/Xtt9/qhx9+uOHl4eajWAZkQceOHTV37lxVqlRJ1atX1/bt2zV58mSVLl3aLi4sLEwffvih2rVrp/Hjx8vPz0+LFi3Sn3/+Ken/h00XLlxY06dPV58+fXTq1Ck98MAD8vX11YkTJ/Trr7/qxIkTmjlz5k3fTgC4HdhsNvXs2VMffvihypcvrxo1amjbtm1atGjRTe9L5cqV1bNnT02bNk1OTk5q2bKldu3apSlTpqS7BGb8+PFau3atGjVqpMGDB6tixYq6cOGCDhw4oFWrVundd9+1y0sODg5q1aqVhgwZopSUFL322muKj4/XuHHjrJg333xTTZo00d13362nn35aQUFBOnv2rPbv36+vvvoq3X1frqVYsWIaNmyYJkyYoMcff1wPPvigDh8+rPDw8HSXoISFhenzzz9X06ZN9dxzz6l69epKSUnRoUOHtGbNGg0dOjTbJ2TVqlWztqtPnz5ycnJSxYoVtXDhQq1fv14dOnRQmTJldOHCBWtEW8uWLbO1DgDIKZkds8aPH6/Ro0frr7/+Utu2bVWsWDEdP35c27Ztk4eHh91xPCv8/f3VsmVLTZo0ScWKFVPZsmX17bffaunSpVleRvfu3bVw4UK1b99ezz77rOrVqycnJycdOXJEGzZs0L333qsuXbpkq1/X67nnntNHH32kDh06aPz48SpbtqxWrlypd955R08//bQqVKiQ7WWmfRavvfaa2rVrJwcHh2z9R1JcXJyaN2+uHj16qFKlSvL09FRUVJRWr16drSdkI/+gWAZkwZtvviknJydNmjRJ586d01133aWlS5fqxRdftIsLCAhQZGSkwsLC9NRTT8nd3V1dunTR+PHj1adPH7tLNnv27KkyZcooIiJC/fv319mzZ+Xr66uaNWuqb9++N3cDAeA28/rrr0uSIiIidO7cOd1zzz1asWJFhvdAyW2zZ8+Wn5+f5s6dq7feeks1a9bU559/nm60U8mSJfXTTz/p5Zdf1uTJk3XkyBF5enoqODjYOpm63MCBA3XhwgUNHjxYsbGxuvPOO7Vy5Uo1btzYiqlSpYp+/vlnvfzyy3rxxRcVGxurokWLKiQkRO3bt7+u7Rk/frw8PDz0zjvvaP78+apUqZLeffddTZkyxS7Ow8ND33//vV599VW9//77io6Olpubm8qUKaOWLVte12fRrFkzjRo1SvPmzdOsWbOUkpKiDRs2qGbNmlqzZo3Gjh2rmJgYFS5cWFWrVtXy5cvVunXr69pOALhRmR2zRo0apSpVqujNN9/U4sWLlZiYKH9/f9WtW1dPPfXUda1r/vz5GjRokEaMGKHk5GR16tRJixcvVp06dbI0v4ODg5YvX64333xT8+fP16RJk+To6KjSpUsrNDTUKjbdDCVKlNDmzZs1atQojRo1SvHx8SpXrpwiIiI0ZMiQ61pmjx499MMPP+idd97R+PHjZYxRdHR0lnORq6ur6tevr/nz5+vAgQO6dOmSypQpoxEjRmj48OHX1SfkLZsxxuR1J4Bb3ZNPPqnFixfr5MmTOXKZCwAAmTlw4ICCg4M1efJkDRs2LK+7AwAAUOAwsgzIYePHj1dAQIDKlSunc+fOacWKFfrggw/04osvUigDAAAAACCfo1gG5DAnJyfr8pikpCSFhIRo6tSpPJoeAG4zycnJutoAfpvNJgcHh5vYIwAAck9KSopSUlKuGuPoSAkCBQOXYQIAAOSCoKAgHTx4MNPpoaGh2rhx483rEAAAuSg8PPyaDx/Izn3AgLxEsQwAACAX7Ny5U4mJiZlO9/T0VMWKFW9ijwAAyD1Hjx7V0aNHrxqTnSdMAnmJYhkAAAAAAACQ6pa9YDglJUVHjx6Vp6enbDZbXncHAPKEMUZnz55VQECAChUqlNfduS2RjwCAfJQfkI8AIOv56JYtlh09elSBgYF53Q0AyBcOHz6s0qVL53U3bkvkIwD4f+SjvEM+AoD/d618dMsWyzw9PSX9twOKFCmSx70BgLwRHx+vwMBA65iIm498BADko/yAfAQAWc9Ht2yxLG1ocZEiRUgGAG57XG6Rd8hHAPD/yEd5h3wEAP/vWvmIGwYAAAAAAAAAqSiWAQAAAAAAAKkolgEAAAAAAACpKJYBAAAAAAAAqSiWAQAAAAAAAKlu2adh3qigkStv+joPvNrhpq8TAJC/kY8AAHktL3KRRD4CkHcYWQYAAAAAAACkolgGAAAAAAAApKJYBgAAAAAAAKSiWAYAAAAAAACkolgGAAAAAAAApKJYBgAAAAAAAKSiWAYAAAAAAACkolgGAAAAAAAApKJYBgAAAAAAAKSiWAYAAAAAAACkolgGAAAAAAAApKJYBgAAAAAAAKSiWAYAAAAAAACkolgGAAAAAAAApMrxYllSUpJefPFFBQcHy83NTeXKldP48eOVkpJixRhjFB4eroCAALm5ualZs2bavXu33XISExM1aNAgFS9eXB4eHurcubOOHDmS090FAAAAgOvy3XffqVOnTgoICJDNZtMXX3xhN71v376y2Wx2rwYNGtjFZOW85/Tp0+rVq5e8vLzk5eWlXr166cyZM7m8dQBw+8rxYtlrr72md999VzNmzNAff/yhiIgITZ48WdOnT7diIiIiNHXqVM2YMUNRUVHy9/dXq1atdPbsWSsmLCxMy5Yt05IlS7Rp0yadO3dOHTt2VHJyck53GQAAAACy7fz586pRo4ZmzJiRaUzbtm117Ngx67Vq1Sq76Vk57+nRo4d27Nih1atXa/Xq1dqxY4d69eqVa9sFALc7x5xe4JYtW3TvvfeqQ4cOkqSgoCAtXrxYP/30k6T/RpVNmzZNo0ePVteuXSVJ8+bNk5+fnxYtWqT+/fsrLi5Os2fP1vz589WyZUtJ0oIFCxQYGKh169apTZs2Od1t3AaCRq7Mk/UeeLVDnqwXAAAAuatdu3Zq167dVWNcXFzk7++f4bSsnPf88ccfWr16tbZu3ar69etLkmbNmqWGDRtqz549qlixYs5uFAAg50eWNWnSRN9++6327t0rSfr111+1adMmtW/fXpIUHR2tmJgYtW7d2prHxcVFoaGh2rx5syRp+/btunTpkl1MQECAqlatasVcKTExUfHx8XYvAAAAAMhLGzdulK+vrypUqKAnnnhCsbGx1rSsnPds2bJFXl5eVqFMkho0aCAvL69Mz40kzo8A4EbkeLFsxIgRevjhh1WpUiU5OTmpVq1aCgsL08MPPyxJiomJkST5+fnZzefn52dNi4mJkbOzs4oVK5ZpzJUmTZpkXcPv5eWlwMDAnN40AAAAAMiydu3aaeHChVq/fr1ef/11RUVF6Z577lFiYqKkrJ33xMTEyNfXN92yfX19Mz03kjg/AoAbkeOXYX788cdasGCBFi1apDvvvFM7duxQWFiYAgIC1KdPHyvOZrPZzWeMSdd2pavFjBo1SkOGDLHex8fHkxDysby6JBIAAAC4WR566CHr31WrVlWdOnVUtmxZrVy50rolTUauPO/J6BzoWudPnB8BwPXL8WLZ888/r5EjR6p79+6SpGrVqungwYOaNGmS+vTpY12vHxMTo5IlS1rzxcbGWqPN/P39dfHiRZ0+fdruf1liY2PVqFGjDNfr4uIiFxeXnN4cAAAAAMgRJUuWVNmyZbVv3z5JWTvv8ff31/Hjx9Mt68SJE+mu1rkc50cAcP1y/DLMf//9V4UK2S/WwcFBKSkpkqTg4GD5+/tr7dq11vSLFy8qMjLSSgi1a9eWk5OTXcyxY8e0a9euTItlAAAAAJCfnTx5UocPH7YGDWTlvKdhw4aKi4vTtm3brJgff/xRcXFxnBsBQC7J8ZFlnTp10iuvvKIyZcrozjvv1C+//KKpU6fqsccek/TfEOKwsDBNnDhRISEhCgkJ0cSJE+Xu7q4ePXpIkry8vNSvXz8NHTpUPj4+8vb21rBhw1StWjXrKTEAAAAAkJfOnTun/fv3W++jo6O1Y8cOeXt7y9vbW+Hh4br//vtVsmRJHThwQC+88IKKFy+uLl26SMraeU/lypXVtm1bPfHEE3rvvfckSU8++aQ6duzIkzABIJfkeLFs+vTpGjNmjAYMGKDY2FgFBASof//+eumll6yY4cOHKyEhQQMGDNDp06dVv359rVmzRp6enlbMG2+8IUdHR3Xr1k0JCQlq0aKF5s6dKwcHh5zuMgAAAABk208//aTmzZtb79PuEdanTx/NnDlTO3fu1EcffaQzZ86oZMmSat68uT7++ONsn/csXLhQgwcPtp6a2blzZ82YMeMmbSUA3H5sxhiT153IDfHx8fLy8lJcXJyKFCmS7fnz4gb0B17tcNPXmVdupxv858XnyvcXaW70WIgbRz4CAPJRfnAjn0Fe/e1OPgKQ07J6LMzxkWUoeG6nwhUAAAAAAMDV5PgN/gEAAAAAAICCipFlQC5j5B4AAAAAAAUHI8sAAAAAAACAVIwsy0cYgQQAAAAAAJC3GFkGAAAAAAAApKJYBgAAAAAAAKSiWAYAAAAAAACkolgGAAAAAAAApKJYBgAo0CZNmiSbzaawsDCrzRij8PBwBQQEyM3NTc2aNdPu3bvt5ktMTNSgQYNUvHhxeXh4qHPnzjpy5IhdzOnTp9WrVy95eXnJy8tLvXr10pkzZ27CVgEAAADIKxTLAAAFVlRUlN5//31Vr17drj0iIkJTp07VjBkzFBUVJX9/f7Vq1Upnz561YsLCwrRs2TItWbJEmzZt0rlz59SxY0clJydbMT169NCOHTu0evVqrV69Wjt27FCvXr1u2vYBAAAAuPkolgEACqRz587pkUce0axZs1SsWDGr3RijadOmafTo0eratauqVq2qefPm6d9//9WiRYskSXFxcZo9e7Zef/11tWzZUrVq1dKCBQu0c+dOrVu3TpL0xx9/aPXq1frggw/UsGFDNWzYULNmzdKKFSu0Z8+ePNlmAAAAALmPYhkAoEB65pln1KFDB7Vs2dKuPTo6WjExMWrdurXV5uLiotDQUG3evFmStH37dl26dMkuJiAgQFWrVrVitmzZIi8vL9WvX9+KadCggby8vKyYjCQmJio+Pt7uBQAAAKDgcMzrDgAAkF1LlizRzz//rKioqHTTYmJiJEl+fn527X5+fjp48KAV4+zsbDciLS0mbf6YmBj5+vqmW76vr68Vk5FJkyZp3Lhx2dsgAAAAAPkGI8sAAAXK4cOH9eyzz2rBggVydXXNNM5ms9m9N8aka7vSlTEZxV9rOaNGjVJcXJz1Onz48FXXCQAAACB/oVgGAChQtm/frtjYWNWuXVuOjo5ydHRUZGSk3nrrLTk6Olojyq4c/RUbG2tN8/f318WLF3X69Omrxhw/fjzd+k+cOJFu1NrlXFxcVKRIEbsXAAAAgIKDYhkAoEBp0aKFdu7cqR07dlivOnXq6JFHHtGOHTtUrlw5+fv7a+3atdY8Fy9eVGRkpBo1aiRJql27tpycnOxijh07pl27dlkxDRs2VFxcnLZt22bF/Pjjj4qLi7NiAAAAANx6uGcZAKBA8fT0VNWqVe3aPDw85OPjY7WHhYVp4sSJCgkJUUhIiCZOnCh3d3f16NFDkuTl5aV+/fpp6NCh8vHxkbe3t4YNG6Zq1apZDwyoXLmy2rZtqyeeeELvvfeeJOnJJ59Ux44dVbFixZu4xQAAAABuJoplAIBbzvDhw5WQkKABAwbo9OnTql+/vtasWSNPT08r5o033pCjo6O6deumhIQEtWjRQnPnzpWDg4MVs3DhQg0ePNh6ambnzp01Y8aMm749AAAAAG4eimUAgAJv48aNdu9tNpvCw8MVHh6e6Tyurq6aPn26pk+fnmmMt7e3FixYkEO9BAAAAFAQcM8yAAAAAAAAIBXFMgAAAAAAACBVrhTL/v77b/Xs2VM+Pj5yd3dXzZo1tX37dmu6MUbh4eEKCAiQm5ubmjVrpt27d9stIzExUYMGDVLx4sXl4eGhzp0768iRI7nRXQAAAAAAAEBSLhTLTp8+rcaNG8vJyUlff/21fv/9d73++usqWrSoFRMREaGpU6dqxowZioqKkr+/v1q1aqWzZ89aMWFhYVq2bJmWLFmiTZs26dy5c+rYsaOSk5NzussAAAAAAACApFy4wf9rr72mwMBAzZkzx2oLCgqy/m2M0bRp0zR69Gh17dpVkjRv3jz5+flp0aJF6t+/v+Li4jR79mzNnz9fLVu2lCQtWLBAgYGBWrdundq0aZNuvYmJiUpMTLTex8fH5/SmAQAAAAAA4BaX4yPLli9frjp16ujBBx+Ur6+vatWqpVmzZlnTo6OjFRMTo9atW1ttLi4uCg0N1ebNmyVJ27dv16VLl+xiAgICVLVqVSvmSpMmTZKXl5f1CgwMzOlNAwAAAAAAwC0ux4tlf/31l2bOnKmQkBB98803euqppzR48GB99NFHkqSYmBhJkp+fn918fn5+1rSYmBg5OzurWLFimcZcadSoUYqLi7Nehw8fzulNAwAAAAAAwC0uxy/DTElJUZ06dTRx4kRJUq1atbR7927NnDlTvXv3tuJsNpvdfMaYdG1XulqMi4uLXFxcbrD3AAAAAAAAyKqgkStv+joPvNohV5ef48WykiVLqkqVKnZtlStX1ueffy5J8vf3l/Tf6LGSJUtaMbGxsdZoM39/f128eFGnT5+2G10WGxurRo0a5XSXAeSAvDhASrl/kAQAAAAA3F5y/DLMxo0ba8+ePXZte/fuVdmyZSVJwcHB8vf319q1a63pFy9eVGRkpFUIq127tpycnOxijh07pl27dlEsAwAAAAAAQK7J8ZFlzz33nBo1aqSJEyeqW7du2rZtm95//329//77kv67/DIsLEwTJ05USEiIQkJCNHHiRLm7u6tHjx6SJC8vL/Xr109Dhw6Vj4+PvL29NWzYMFWrVs16OiYAAAAAAACQ03K8WFa3bl0tW7ZMo0aN0vjx4xUcHKxp06bpkUcesWKGDx+uhIQEDRgwQKdPn1b9+vW1Zs0aeXp6WjFvvPGGHB0d1a1bNyUkJKhFixaaO3euHBwccrrLAAAAAAAAgKRcKJZJUseOHdWxY8dMp9tsNoWHhys8PDzTGFdXV02fPl3Tp0/PhR4CAAAAAAAA6eX4PcsAAAAA4Hbw3XffqVOnTgoICJDNZtMXX3xhN90Yo/DwcAUEBMjNzU3NmjXT7t277WISExM1aNAgFS9eXB4eHurcubOOHDliF3P69Gn16tVLXl5e8vLyUq9evXTmzJlc3joAuH1RLAMAAACA63D+/HnVqFFDM2bMyHB6RESEpk6dqhkzZigqKkr+/v5q1aqVzp49a8WEhYVp2bJlWrJkiTZt2qRz586pY8eOSk5OtmJ69OihHTt2aPXq1Vq9erV27NihXr165fr2AcDtKlcuwwQAAACAW127du3Url27DKcZYzRt2jSNHj1aXbt2lSTNmzdPfn5+WrRokfr376+4uDjNnj1b8+fPtx5ktmDBAgUGBmrdunVq06aN/vjjD61evVpbt25V/fr1JUmzZs1Sw4YNtWfPHlWsWPHmbCwA3EYYWQYAAAAAOSw6OloxMTFq3bq11ebi4qLQ0FBt3rxZkrR9+3ZdunTJLiYgIEBVq1a1YrZs2SIvLy+rUCZJDRo0kJeXlxWTkcTERMXHx9u9AABZQ7EMAAAAAHJYTEyMJMnPz8+u3c/Pz5oWExMjZ2dnFStW7Koxvr6+6Zbv6+trxWRk0qRJ1j3OvLy8FBgYeEPbAwC3E4plAAAAAJBLbDab3XtjTLq2K10Zk1H8tZYzatQoxcXFWa/Dhw9ns+cAcPuiWAYAAAAAOczf31+S0o3+io2NtUab+fv76+LFizp9+vRVY44fP55u+SdOnEg3au1yLi4uKlKkiN0LAJA1FMsAAAAAIIcFBwfL399fa9eutdouXryoyMhINWrUSJJUu3ZtOTk52cUcO3ZMu3btsmIaNmyouLg4bdu2zYr58ccfFRcXZ8UAAHIWT8MEAAAAgOtw7tw57d+/33ofHR2tHTt2yNvbW2XKlFFYWJgmTpyokJAQhYSEaOLEiXJ3d1ePHj0kSV5eXurXr5+GDh0qHx8feXt7a9iwYapWrZr1dMzKlSurbdu2euKJJ/Tee+9Jkp588kl17NiRJ2ECQC6hWAYA2RQ0cuVNX+eBVzvc9HUCAICr++mnn9S8eXPr/ZAhQyRJffr00dy5czV8+HAlJCRowIABOn36tOrXr681a9bI09PTmueNN96Qo6OjunXrpoSEBLVo0UJz586Vg4ODFbNw4UINHjzYempm586dNWPGjJu0lQBw+6FYBgAAAADXoVmzZjLGZDrdZrMpPDxc4eHhmca4urpq+vTpmj59eqYx3t7eWrBgwY10FQCQDdyzDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEhFsQwAAAAAAABIRbEMAAAAAAAASEWxDAAAAAAAAEjlmNcdAIAbETRyZV53AQAAAABwC2FkGQAAAAAAAJCKYhkAAAAAAACQKteLZZMmTZLNZlNYWJjVZoxReHi4AgIC5ObmpmbNmmn37t128yUmJmrQoEEqXry4PDw81LlzZx05ciS3uwsAAAAAAIDbWK4Wy6KiovT++++revXqdu0RERGaOnWqZsyYoaioKPn7+6tVq1Y6e/asFRMWFqZly5ZpyZIl2rRpk86dO6eOHTsqOTk5N7sMAAAAAACA21iuFcvOnTunRx55RLNmzVKxYsWsdmOMpk2bptGjR6tr166qWrWq5s2bp3///VeLFi2SJMXFxWn27Nl6/fXX1bJlS9WqVUsLFizQzp07tW7dutzqMgAAAAAAAG5zuVYse+aZZ9ShQwe1bNnSrj06OloxMTFq3bq11ebi4qLQ0FBt3rxZkrR9+3ZdunTJLiYgIEBVq1a1Yq6UmJio+Ph4uxcAAAAAAACQHY65sdAlS5bo559/VlRUVLppMTExkiQ/Pz+7dj8/Px08eNCKcXZ2thuRlhaTNv+VJk2apHHjxuVE9wEAAAAAAHCbyvFi2eHDh/Xss89qzZo1cnV1zTTOZrPZvTfGpGu70tViRo0apSFDhljv4+PjFRgYmI2eAwAAAADyi6CRK2/6Og+82uGmrxNA/pPjl2Fu375dsbGxql27thwdHeXo6KjIyEi99dZbcnR0tEaUXTlCLDY21prm7++vixcv6vTp05nGXMnFxUVFihSxewEAAAAAAADZkePFshYtWmjnzp3asWOH9apTp44eeeQR7dixQ+XKlZO/v7/Wrl1rzXPx4kVFRkaqUaNGkqTatWvLycnJLubYsWPatWuXFQMAAAAAAADktBy/DNPT01NVq1a1a/Pw8JCPj4/VHhYWpokTJyokJEQhISGaOHGi3N3d1aNHD0mSl5eX+vXrp6FDh8rHx0fe3t4aNmyYqlWrlu6BAQAAAAAAAEBOyZUb/F/L8OHDlZCQoAEDBuj06dOqX7++1qxZI09PTyvmjTfekKOjo7p166aEhAS1aNFCc+fOlYODQ150GQAAAAAAALeBHL8MMyMbN27UtGnTrPc2m03h4eE6duyYLly4oMjIyHSj0VxdXTV9+nSdPHlS//77r7766itu2A8AkPTfE5Dr1q0rT09P+fr66r777tOePXvsYowxCg8PV0BAgNzc3NSsWTPt3r3bLiYxMVGDBg1S8eLF5eHhoc6dO+vIkSN2MadPn1avXr3k5eUlLy8v9erVS2fOnMntTQQAAACQR25KsQwAgJwUGRmpZ555Rlu3btXatWuVlJSk1q1b6/z581ZMRESEpk6dqhkzZigqKkr+/v5q1aqVzp49a8WEhYVp2bJlWrJkiTZt2qRz586pY8eOSk5OtmJ69OihHTt2aPXq1Vq9erV27NihXr163dTtBQAAAHDz5MllmAAA3IjVq1fbvZ8zZ458fX21fft2NW3aVMYYTZs2TaNHj1bXrl0lSfPmzZOfn58WLVqk/v37Ky4uTrNnz9b8+fOt+2EuWLBAgYGBWrdundq0aaM//vhDq1ev1tatW1W/fn1J0qxZs9SwYUPt2bNHFStWvLkbDgAAACDXMbIMAFDgxcXFSZK8vb0lSdHR0YqJiVHr1q2tGBcXF4WGhmrz5s2SpO3bt+vSpUt2MQEBAapataoVs2XLFnl5eVmFMklq0KCBvLy8rJgrJSYmKj4+3u4FAAAAoOCgWAYAKNCMMRoyZIiaNGli3f8yJiZGkuTn52cX6+fnZ02LiYmRs7OzihUrdtUYX1/fdOv09fW1Yq40adIk6/5mXl5e3G8TAAAAKGAolgEACrSBAwfqt99+0+LFi9NNs9lsdu+NMenarnRlTEbxV1vOqFGjFBcXZ70OHz6clc0AAAAAkE9QLAMAFFiDBg3S8uXLtWHDBpUuXdpq9/f3l6R0o79iY2Ot0Wb+/v66ePGiTp8+fdWY48ePp1vviRMn0o1aS+Pi4qIiRYrYvQAAAAAUHBTLAAAFjjFGAwcO1NKlS7V+/XoFBwfbTQ8ODpa/v7/Wrl1rtV28eFGRkZFq1KiRJKl27dpycnKyizl27Jh27dplxTRs2FBxcXHatm2bFfPjjz8qLi7OigEAAABwa+FpmACAAueZZ57RokWL9OWXX8rT09MaQebl5SU3NzfZbDaFhYVp4sSJCgkJUUhIiCZOnCh3d3f16NHDiu3Xr5+GDh0qHx8feXt7a9iwYapWrZr1dMzKlSurbdu2euKJJ/Tee+9Jkp588kl17NiRJ2ECAAAAtyiKZQCAAmfmzJmSpGbNmtm1z5kzR3379pUkDR8+XAkJCRowYIBOnz6t+vXra82aNfL09LTi33jjDTk6Oqpbt25KSEhQixYtNHfuXDk4OFgxCxcu1ODBg62nZnbu3FkzZszI3Q0EAAAAkGcolgEAChxjzDVjbDabwsPDFR4enmmMq6urpk+frunTp2ca4+3trQULFlxPNwEAAAAUQBTLAAAAAACQFDRyZZ6s98CrHfJkvQAyxg3+AQAAACCXhIeHy2az2b3Sntos/TdaOjw8XAEBAXJzc1OzZs20e/duu2UkJiZq0KBBKl68uDw8PNS5c2cdOXLkZm8KANw2GFkGAAAAALnozjvv1Lp166z3l98bMyIiQlOnTtXcuXNVoUIFTZgwQa1atdKePXus+2yGhYXpq6++0pIlS+Tj46OhQ4eqY8eO2r59u92yUHDlxYg2RrMBmaNYBgAAAAC5yNHR0W40WRpjjKZNm6bRo0era9eukqR58+bJz89PixYtUv/+/RUXF6fZs2dr/vz51tOaFyxYoMDAQK1bt05t2rS5qdsCALcDLsMEAAAAgFy0b98+BQQEKDg4WN27d9dff/0lSYqOjlZMTIz1xGVJcnFxUWhoqDZv3ixJ2r59uy5dumQXExAQoKpVq1oxGUlMTFR8fLzdCwCQNYwsAwAAAIBcUr9+fX300UeqUKGCjh8/rgkTJqhRo0bavXu3YmJiJEl+fn528/j5+engwYOSpJiYGDk7O6tYsWLpYtLmz8ikSZM0bty4HN4aAFmVVw+LQM5gZBkAAAAA5JJ27drp/vvvV7Vq1dSyZUutXPnfCfS8efOsGJvNZjePMSZd25WuFTNq1CjFxcVZr8OHD9/AVgDA7YWRZQAAAABwk3h4eKhatWrat2+f7rvvPkn/jR4rWbKkFRMbG2uNNvP399fFixd1+vRpu9FlsbGxatSoUabrcXFxkYuLS+5sBG4JPFQAyBwjywAAAADgJklMTNQff/yhkiVLKjg4WP7+/lq7dq01/eLFi4qMjLQKYbVr15aTk5NdzLFjx7Rr166rFssAANePkWUAAAAAkEuGDRumTp06qUyZMoqNjdWECRMUHx+vPn36yGazKSwsTBMnTlRISIhCQkI0ceJEubu7q0ePHpIkLy8v9evXT0OHDpWPj4+8vb01bNgw67JOANfG/cOQXRTLAAAAACCXHDlyRA8//LD++ecflShRQg0aNNDWrVtVtmxZSdLw4cOVkJCgAQMG6PTp06pfv77WrFkjT09PaxlvvPGGHB0d1a1bNyUkJKhFixaaO3euHBwc8mqzgOtC0QoFBcUyAAAAAMglS5Ysuep0m82m8PBwhYeHZxrj6uqq6dOna/r06TncOwBARnL8nmWTJk1S3bp15enpKV9fX913333as2ePXYwxRuHh4QoICJCbm5uaNWum3bt328UkJiZq0KBBKl68uDw8PNS5c2cdOXIkp7sLAAAAAAAAWHK8WBYZGalnnnlGW7du1dq1a5WUlKTWrVvr/PnzVkxERISmTp2qGTNmKCoqSv7+/mrVqpXOnj1rxYSFhWnZsmVasmSJNm3apHPnzqljx45KTk7O6S4DAAAAAAAAknLhMszVq1fbvZ8zZ458fX21fft2NW3aVMYYTZs2TaNHj1bXrl0lSfPmzZOfn58WLVqk/v37Ky4uTrNnz9b8+fOtm1YuWLBAgYGBWrdundq0aZPT3QYAAAAAAAByfmTZleLi4iRJ3t7ekqTo6GjFxMSodevWVoyLi4tCQ0O1efNmSdL27dt16dIlu5iAgABVrVrVirlSYmKi4uPj7V4AAAAAAABAduRqscwYoyFDhqhJkyaqWrWqJCkmJkaS5OfnZxfr5+dnTYuJiZGzs7OKFSuWacyVJk2aJC8vL+sVGBiY05sDAAAAAACAW1yuFssGDhyo3377TYsXL043zWaz2b03xqRru9LVYkaNGqW4uDjrdfjw4evvOAAAAAAAAG5LuVYsGzRokJYvX64NGzaodOnSVru/v78kpRshFhsba4028/f318WLF3X69OlMY67k4uKiIkWK2L0AAAAAAACA7MjxYpkxRgMHDtTSpUu1fv16BQcH200PDg6Wv7+/1q5da7VdvHhRkZGRatSokSSpdu3acnJysos5duyYdu3aZcUAAAAAAAAAOS3Hn4b5zDPPaNGiRfryyy/l6elpjSDz8vKSm5ubbDabwsLCNHHiRIWEhCgkJEQTJ06Uu7u7evToYcX269dPQ4cOlY+Pj7y9vTVs2DBVq1bNejomAAAAAAAAkNNyvFg2c+ZMSVKzZs3s2ufMmaO+fftKkoYPH66EhAQNGDBAp0+fVv369bVmzRp5enpa8W+88YYcHR3VrVs3JSQkqEWLFpo7d64cHBxyussAAAAAAACApFwolhljrhljs9kUHh6u8PDwTGNcXV01ffp0TZ8+PQd7BwAAAAAAAGQuV5+GCQAAAAAAABQkFMsAAAAAAACAVBTLAAAAAAAAgFQUywAAAAAAAIBUFMsAAAAAAACAVBTLAAAAAAAAgFQUywAAAAAAAIBUFMsAAAAAAACAVBTLAAAAAAAAgFQUywAAAAAAAIBUjnndAQAAkL8EjVx509d54NUON32dAAAAQEYolgEAgDyXFwW6vEJhEAAAIH/jMkwAAAAAAAAgFcUyAAAAAAAAIBXFMgAAAAAAACAVxTIAAAAAAAAgFcUyAAAAAAAAIBXFMgAAAAAAACAVxTIAAAAAAAAgFcUyAAAAAAAAIBXFMgAAAAAAACAVxTIAAAAAAAAgVb4vlr3zzjsKDg6Wq6urateure+//z6vuwQAuM2QiwAA+QH5CABujnxdLPv4448VFham0aNH65dfftHdd9+tdu3a6dChQ3ndNQDAbYJcBADID8hHAHDz2IwxJq87kZn69evrrrvu0syZM622ypUr67777tOkSZPsYhMTE5WYmGi9j4uLU5kyZXT48GEVKVIk2+uuOvab6+84AOSwXePaXNd88fHxCgwM1JkzZ+Tl5ZXDvbo9ZCcXSeQjXNv1/p6Bgox8dOPyMh+RiwDkN7l+fmTyqcTEROPg4GCWLl1q1z548GDTtGnTdPFjx441knjx4sWLVwavw4cP36zD9y0lu7nIGPIRL168eF3tRT66PuQjXrx48crZ17XykaPyqX/++UfJycny8/Oza/fz81NMTEy6+FGjRmnIkCHW+5SUFJ06dUo+Pj6y2Ww33J+06uP1jgzIa/Q/b9H/vHU7998Yo7NnzyogICCXendry24uknI2HxX0725OYB+wDyT2QZqCvB/IRzfmZuajgvw9y2/YlzmHfZkz2I9Zz0f5tliW5soDuTEmw4O7i4uLXFxc7NqKFi2a4/0pUqRIgf5S0f+8Rf/z1u3afy53uXFZzUVS7uSjgv7dzQnsA/aBxD5IU1D3A/noxt3MfFRQv2f5Efsy57Avc8btvh+zko/y7Q3+ixcvLgcHh3T/UxIbG5vuf1QAAMgN5CIAQH5APgKAmyvfFsucnZ1Vu3ZtrV271q597dq1atSoUR71CgBwOyEXAQDyA/IRANxc+foyzCFDhqhXr16qU6eOGjZsqPfff1+HDh3SU089ddP74uLiorFjx6YbylxQ0P+8Rf/zFv3HjcjLXMRnzz6Q2AcS+yAN++H2drPyEd+znMO+zDnsy5zBfsw6mzHG5HUnruadd95RRESEjh07pqpVq+qNN95Q06ZN87pbAIDbCLkIAJAfkI8A4ObI98UyAAAAAAAA4GbJt/csAwAAAAAAAG42imUAAAAAAABAKoplAAAAAAAAQCqKZQAAAAAAAEAqimWXmTRpkurWrStPT0/5+vrqvvvu0549e+xijDEKDw9XQECA3Nzc1KxZM+3evTuPemxv5syZql69uooUKaIiRYqoYcOG+vrrr63p+bnvGZk0aZJsNpvCwsKstvy8DeHh4bLZbHYvf39/a3p+7nuav//+Wz179pSPj4/c3d1Vs2ZNbd++3Zqe37chKCgo3Wdgs9n0zDPPSMr//U9KStKLL76o4OBgubm5qVy5cho/frxSUlKsmPy+Dcg577zzjoKDg+Xq6qratWvr+++/z+su3TRZyce3m4xy4u3iWrnpVpeV3ADkpNs5/1yPgn4OmZ8VtPPB/Kagn9vlOQNLmzZtzJw5c8yuXbvMjh07TIcOHUyZMmXMuXPnrJhXX33VeHp6ms8//9zs3LnTPPTQQ6ZkyZImPj4+D3v+n+XLl5uVK1eaPXv2mD179pgXXnjBODk5mV27dhlj8nffr7Rt2zYTFBRkqlevbp599lmrPT9vw9ixY82dd95pjh07Zr1iY2Ot6fm578YYc+rUKVO2bFnTt29f8+OPP5ro6Gizbt06s3//fismv29DbGys3f5fu3atkWQ2bNhgjMn//Z8wYYLx8fExK1asMNHR0ebTTz81hQsXNtOmTbNi8vs2IGcsWbLEODk5mVmzZpnff//dPPvss8bDw8McPHgwr7t2U2QlH99OMsuJt4Os5KZbXVZyA5BTbvf8cz0K+jlkflUQzwfzk1vh3C6vUSy7itjYWCPJREZGGmOMSUlJMf7+/ubVV1+1Yi5cuGC8vLzMu+++m1fdvKpixYqZDz74oED1/ezZsyYkJMSsXbvWhIaGWgfH/L4NY8eONTVq1MhwWn7vuzHGjBgxwjRp0iTT6QVhG6707LPPmvLly5uUlJQC0f8OHTqYxx57zK6ta9eupmfPnsaYgvkZ4PrUq1fPPPXUU3ZtlSpVMiNHjsyjHuWtK/Px7SSznHi7uFZuuh1cKzcAOYn8c+NuhXPIvFZQzwfzk1vx3O5m4zLMq4iLi5MkeXt7S5Kio6MVExOj1q1bWzEuLi4KDQ3V5s2b86SPmUlOTtaSJUt0/vx5NWzYsED1/ZlnnlGHDh3UsmVLu/aCsA379u1TQECAgoOD1b17d/3111+SCkbfly9frjp16ujBBx+Ur6+vatWqpVmzZlnTC8I2XO7ixYtasGCBHnvsMdlstgLR/yZNmujbb7/V3r17JUm//vqrNm3apPbt20sqeJ8Brs/Fixe1fft2u89Zklq3bn3bfs5X5uPbSWY58XZxrdx0O7hWbgByCvknZxTkc8j8oiCfD+YXt9q5XV5wzOsO5FfGGA0ZMkRNmjRR1apVJUkxMTGSJD8/P7tYPz8/HTx48Kb3MSM7d+5Uw4YNdeHCBRUuXFjLli1TlSpVrC98fu67JC1ZskQ///yzoqKi0k3L7/u/fv36+uijj1ShQgUdP35cEyZMUKNGjbR79+5833dJ+uuvvzRz5kwNGTJEL7zwgrZt26bBgwfLxcVFvXv3LhDbcLkvvvhCZ86cUd++fSXl/++PJI0YMUJxcXGqVKmSHBwclJycrFdeeUUPP/ywpIKxDbhx//zzj5KTkzP8nNO+A7eTjPLx7eJqOfF2ca3cdDu4Vm4Acgr558YV1HPI/KQgnw/mJ7fauV1eoFiWiYEDB+q3337Tpk2b0k2z2Wx2740x6drySsWKFbVjxw6dOXNGn3/+ufr06aPIyEhren7u++HDh/Xss89qzZo1cnV1zTQuv25Du3btrH9Xq1ZNDRs2VPny5TVv3jw1aNBAUv7tuySlpKSoTp06mjhxoiSpVq1a2r17t2bOnGl3QpKft+Fys2fPVrt27RQQEGDXnp/7//HHH2vBggVatGiR7rzzTu3YsUNhYWEKCAhQnz59rLj8vA3IOXzO/7laPr6VZTUn3uqymptuZVnNDUBOIf9cv4J6DplfFPTzwfzkVju3ywtchpmBQYMGafny5dqwYYNKly5ttac92fDK/1mJjY1NV5HNK87OzrrjjjtUp04dTZo0STVq1NCbb75ZIPq+fft2xcbGqnbt2nJ0dJSjo6MiIyP11ltvydHR0epnft6Gy3l4eKhatWrat29fgdj/JUuWVJUqVezaKleurEOHDkkqGN//NAcPHtS6dev0+OOPW20Fof/PP/+8Ro4cqe7du6tatWrq1auXnnvuOU2aNElSwdgG3LjixYvLwcGBz1mZ5+PbwbVyYnJycl538aa4Vm66HVwrNwA5hfxzYwryOWR+caudD+alW+ncLq9QLLuMMUYDBw7U0qVLtX79egUHB9tNDw4Olr+/v9auXWu1Xbx4UZGRkWrUqNHN7m6WGGOUmJhYIPreokUL7dy5Uzt27LBederU0SOPPKIdO3aoXLly+X4bLpeYmKg//vhDJUuWLBD7v3Hjxukec713716VLVtWUsH6/s+ZM0e+vr7q0KGD1VYQ+v/vv/+qUCH7w7KDg4NSUlIkFYxtwI1zdnZW7dq17T5nSVq7du1t8zlfKx/fDq6VEx0cHPK6izfFtXLT7eBauQHIKeSf63MrnkPmlVvtfDAv3UrndnnmJj9QIF97+umnjZeXl9m4caM5duyY9fr333+tmFdffdV4eXmZpUuXmp07d5qHH3443zxeddSoUea7774z0dHR5rfffjMvvPCCKVSokFmzZo0xJn/3PTNXPvkrP2/D0KFDzcaNG81ff/1ltm7dajp27Gg8PT3NgQMHjDH5u+/G/Pd4ZkdHR/PKK6+Yffv2mYULFxp3d3ezYMECKya/b4MxxiQnJ5syZcqYESNGpJuW3/vfp08fU6pUKbNixQoTHR1tli5daooXL26GDx9uxeT3bUDOWLJkiXFycjKzZ882v//+uwkLCzMeHh7W8eRWl5V8fDu6HZ+GmZXcdKvLSm4Acsrtnn+uR0E/h8zvCtL5YH5yq5zb5SWKZZeRlOFrzpw5VkxKSooZO3as8ff3Ny4uLqZp06Zm586dedfpyzz22GOmbNmyxtnZ2ZQoUcK0aNHCKpQZk7/7npkrD475eRseeughU7JkSePk5GQCAgJM165dze7du63p+bnvab766itTtWpV4+LiYipVqmTef/99u+kFYRu++eYbI8ns2bMn3bT83v/4+Hjz7LPPmjJlyhhXV1dTrlw5M3r0aJOYmGjF5PdtQM55++23rWP6XXfdZT2C/naQlXx8O7odi2XGXDs33eqykhuAnHQ755/rUdDPIfO7gnQ+mN/cCud2eclmjDE3eTAbAAAAAAAAkC9xzzIAAAAAAAAgFcUyAAAAAAAAIBXFMgAAAAAAACAVxTIAAAAAAAAgFcUyAAAAAAAAIBXFMiAPHDhwQB06dJC3t7dsNpvCwsLyukuWAwcOyGazae7cuXndFQC3sLlz58pms+nAgQN53ZUbsnHjRtlsNm3cuNFq69u3r4KCgrK1nKNHjyo8PFw7duzI0f7lF/k57+W1zZs3Kzw8XGfOnEk3LSgoSB07drzmMjL6HgLI2NV+c1mxaNEiTZs27Yb60KxZMzVr1ixb81xPbsmPwsPDZbPZ7NqCgoLUt2/fbC3nRj/H/O7bb79VnTp15OHhIZvNpi+++CKvu3TboVgG5IHnnntOP/74oz788ENt2bJFzz33XF53CQCQQ8aMGaNly5Zla56jR49q3Lhxt2yxjLyXuc2bN2vcuHG37AkfkN/c6G8uJ4plsLds2TKNGTMmW/PcysdOY4y6desmJycnLV++XFu2bFFoaGhed+u245jXHQBuBQkJCXJzc8ty/K5du1SvXj3dd999ObJ+Y4wuXLiQYR8SEhLk6uqa7n9wAAC5o3z58nndhXwnp/MeAODWUatWrbzuQr5y9OhRnTp1Sl26dFGLFi3yuju3LUaWIU+lDcPdvXu3Hn74YXl5ecnPz0+PPfaY4uLiJF39skCbzabw8PB0y/vtt9/04IMPysvLS97e3hoyZIiSkpK0Z88etW3bVp6engoKClJERES2+5x2ScTSpUtVq1Ytubq6aty4cZKkmJgY9e/fX6VLl5azs7OCg4M1btw4JSUlSfr/yyT279+vr7/+Wjabze4ypPj4eA0bNkzBwcFydnZWqVKlFBYWpvPnz6fb7oEDB+rdd99V5cqV5eLionnz5lmXNa1Zs0aPPfaYSpQoIXd3dyUmJmr//v169NFHFRISInd3d5UqVUqdOnXSzp07r7nNJ06c0JNPPqnAwEC5uLioRIkSaty4sdatW5ft/Qfg5snK7/7EiRNydnbO8H90//zzT9lsNr311ltW26ZNm9SwYUO5urqqVKlSGjNmjD744IMcu6Tyww8/VI0aNeTq6ipvb2916dJFf/zxh11M3759VbhwYe3fv1/t27dX4cKFFRgYqKFDhyoxMdEu9siRI3rggQfk6empokWL6pFHHlFUVNR1XW7+559/qm3btnJ3d1fx4sX11FNP6ezZs+niMrpU5tNPP1X9+vXl5eUld3d3lStXTo899pik/3JD3bp1JUmPPvqolRvS8ttPP/2k7t27KygoSG5ubgoKCtLDDz+sgwcP2q0jLQds2LBBTz/9tIoXLy4fHx917dpVR48eTdfPRYsWqWHDhipcuLAKFy6smjVravbs2XYx69atU4sWLVSkSBG5u7urcePG+vbbb7O8z3Iq7xlj9M4776hmzZpyc3NTsWLF9MADD+ivv/7Kcl8u30fr16/XE088IR8fHxUpUkS9e/fW+fPnFRMTo27duqlo0aIqWbKkhg0bpkuXLtkt49SpUxowYIBKlSolZ2dnlStXTqNHj0733UvL1fPnz1flypXl7u6uGjVqaMWKFVZMeHi4nn/+eUlScHCwtX+uvJxy9erVuuuuu+Tm5qZKlSrpww8/vOp2zp8/XzabTVu2bEk3bfz48XJycsrwOwHc6q72m0tJSVFERIQqVaokFxcX+fr6qnfv3jpy5Ig1f7NmzbRy5UodPHjQmvfy/5AeN26c6tevL29vbxUpUkR33XWXZs+eLWNMrmzPhQsXNGrUKLtj6DPPPJNutFXa+UtWjiU5medXrlypmjVrysXFRcHBwZoyZUqGcVdehpmSkqIJEyaoYsWKcnNzU9GiRVW9enW9+eabkq597Pz444/VunVrlSxZUm5ubqpcubJGjhyZLrdk5++JxMREjR8/XpUrV5arq6t8fHzUvHlzbd682YrJiVwVHh6u0qVLS5JGjBghm81m9zfFvn371KNHD/n6+srFxUWVK1fW22+/nW45Wc2vuAoD5KGxY8caSaZixYrmpZdeMmvXrjVTp041Li4u5tFHHzXGGBMdHW0kmTlz5qSbX5IZO3Zshst7+eWXzdq1a83w4cONJDNw4EBTqVIl89Zbb5m1a9eaRx991Egyn3/+ebb6XLZsWVOyZElTrlw58+GHH5oNGzaYbdu2mWPHjpnAwEBTtmxZ895775l169aZl19+2bi4uJi+ffsaY4yJi4szW7ZsMf7+/qZx48Zmy5YtZsuWLebChQvm/PnzpmbNmqZ48eJm6tSpZt26debNN980Xl5e5p577jEpKSl2212qVClTvXp1s2jRIrN+/Xqza9cuM2fOHGvak08+ab7++mvz2WefmaSkJBMZGWmGDh1qPvvsMxMZGWmWLVtm7rvvPuPm5mb+/PNPa9kZ7e82bdqYEiVKmPfff99s3LjRfPHFF+all14yS5Ysyda+A3BzZfV336VLFxMYGGiSk5Pt5h8+fLhxdnY2//zzjzHGmF9//dW4urqa6tWrmyVLlpjly5eb9u3bm6CgICPJREdHZ7lvacery+eZOHGikWQefvhhs3LlSvPRRx+ZcuXKGS8vL7N3714rrk+fPsbZ2dlUrlzZTJkyxaxbt8689NJLxmazmXHjxllx586dM3fccYfx9vY2b7/9tvnmm2/Mc889Z4KDgzPNK5mJiYkxvr6+plSpUmbOnDlm1apV5pFHHjFlypQxksyGDRvs+le2bFnr/ebNm43NZjPdu3c3q1atMuvXrzdz5swxvXr1Msb8lxvS9seLL75o5YbDhw8bY4z59NNPzUsvvWSWLVtmIiMjzZIlS0xoaKgpUaKEOXHiRLp9Wq5cOTNo0CDzzTffmA8++MAUK1bMNG/e3G57xowZYySZrl27mk8//dSsWbPGTJ061YwZM8aKmT9/vrHZbOa+++4zS5cuNV999ZXp2LGjcXBwMOvWrcvSfsupvPfEE08YJycnM3ToULN69WqzaNEiU6lSJePn52diYmKy/Dmm7aPg4GAzdOhQs2bNGvPaa68ZBwcH8/DDD5u77rrLTJgwwaxdu9aMGDHCSDKvv/66NX9CQoKpXr268fDwMFOmTDFr1qwxY8aMMY6OjqZ9+/Z265JkgoKCTL169cwnn3xiVq1aZZo1a2YcHR3N//73P2OMMYcPHzaDBg0ykszSpUut/RMXF2eM+e9vjtKlS5sqVaqYjz76yHzzzTfmwQcfNJJMZGSkta4NGzbYfQ8TExONv7+/eeSRR+z6dOnSJRMQEGAefPDBLO8z4FZytd/ck08+aZ0zrF692rz77rumRIkSJjAw0DrW7t692zRu3Nj4+/tb827ZssVaft++fc3s2bPN2rVrzdq1a83LL79s3Nzc7HKTMcaEhoaa0NDQbPX9ytySkpJi2rRpYxwdHc2YMWPMmjVrzJQpU4yHh4epVauWuXDhghWb1WNJTub5devWGQcHB9OkSROzdOlS8+mnn5q6detaefNyZcuWNX369LHeT5o0yTg4OJixY8eab7/91qxevdpMmzbNhIeHG2Oufex8+eWXzRtvvGFWrlxpNm7caN59910THBycLhdm9e+JS5cumebNmxtHR0czbNgws2rVKrN8+XLzwgsvmMWLF1txOZGrDh8+bJYuXWokmUGDBpktW7aYn3/+2Rjz3/fPy8vLVKtWzXz00UdmzZo1ZujQoaZQoULWvjHGZCu/InMUy5Cn0opbERERdu0DBgwwrq6uJiUl5bqKZZf/YWuMMTVr1rQOpmkuXbpkSpQoYbp27ZqtPpctW9Y4ODiYPXv22LX379/fFC5c2Bw8eNCufcqUKUaS2b17t90yOnToYBc3adIkU6hQIRMVFWXX/tlnnxlJZtWqVXbb7eXlZU6dOmUXm3YS0Lt372tuR1JSkrl48aIJCQkxzz33nNWe0f4uXLiwCQsLu+YyAeRvmf3uly9fbiSZNWvW2MUGBASY+++/32p78MEHjYeHh12BJjk52VSpUuWGi2WnT582bm5u6QoOhw4dMi4uLqZHjx5WW58+fYwk88knn9jFtm/f3lSsWNF6//bbbxtJ5uuvv7aL69+/f7aLZSNGjDA2m83s2LHDrr1Vq1bXLJal5YEzZ85kuvyoqKgs9ykpKcmcO3fOeHh4mDfffNNqT9unAwYMsIuPiIgwksyxY8eMMcb89ddfxsHBIV0h5XLnz5833t7eplOnTnbtycnJpkaNGqZevXrX7OflbiTvbdmyJcPcfvjwYePm5maGDx+e5X6k7aNBgwbZtd93331Gkpk6dapde82aNc1dd91lvX/33Xcz/O699tpr6X5Dkoyfn5+Jj4+32mJiYkyhQoXMpEmTrLbJkydn+vspW7ascXV1tfvbIiEhwXh7e5v+/ftbbVcWy4z5728iZ2dnc/z4cavt448/TndyDNxuMvrN/fHHHxkeP3/88UcjybzwwgtWW4cOHeyO8ZlJTk42ly5dMuPHjzc+Pj52BYqcKJatXr06w/OotN/5+++/b7Vl9ViSk3m+fv36JiAgwCQkJFht8fHxxtvb+5rFso4dO5qaNWtedflXO3ZeLiUlxVy6dMlERkYaSebXX3+1pmX174mPPvrISDKzZs3KdD05mavSzscmT55s196mTRtTunRpqyiYZuDAgcbV1dU6N8zOeSUyx2WYyBc6d+5s97569eq6cOGCYmNjr2t5Vz45qnLlyrLZbGrXrp3V5ujoqDvuuCPdZSxZUb16dVWoUMGubcWKFWrevLkCAgKUlJRkvdLWGRkZedVlrlixQlWrVlXNmjXt5m/Tpk2Gl2Tcc889KlasWIbLuv/++9O1JSUlaeLEiapSpYqcnZ3l6OgoZ2dn7du3L90lTleqV6+e5s6dqwkTJmjr1q3pLkkBkD9l9Xffrl07+fv7a86cOVbbN998o6NHj1qXCkr/HcfuueceFS9e3GorVKiQunXrdsN93bJlixISEtI9DSswMFD33HNPukv/bDabOnXqZNdWvXp1u2N6ZGSkPD091bZtW7u4hx9+ONv927Bhg+68807VqFHDrr1Hjx7XnDftEstu3brpk08+0d9//52tdZ87d04jRozQHXfcIUdHRzk6Oqpw4cI6f/58hsfvjHKqJGvfrF27VsnJyXrmmWcyXefmzZt16tQp9enTxy4npaSkqG3btoqKirrhSzmymvdWrFghm82mnj172sX5+/urRo0a1/UEyIz+TpCkDh06pGu//Du1fv16eXh46IEHHrCLS/veXvk9bd68uTw9Pa33fn5+8vX1zdbfHjVr1lSZMmWs966urqpQocI1l/H0009LkmbNmmW1zZgxQ9WqVVPTpk2zvH7gdrBhwwZJSpeD6tWrp8qVK2f58vP169erZcuW8vLykoODg5ycnPTSSy/p5MmT131ec7V1ZdTnBx98UB4eHun6nJVjSU7l+fPnzysqKkpdu3aVq6ur1e7p6Zkud2ekXr16+vXXXzVgwAB98803io+Pz9b6//rrL/Xo0UP+/v7W55B2g/wr82ZW/p74+uuv5erqavc30ZVyI1dd7sKFC/r222/VpUsXubu7262jffv2unDhgrZu3Wr1JTvnlcgYxTLkCz4+PnbvXVxcJP13c/rr4e3tbffe2dlZ7u7udgfrtPYLFy5ke/klS5ZM13b8+HF99dVXcnJysnvdeeedkqR//vnnqss8fvy4fvvtt3Tze3p6yhiTbv6M+nC1aUOGDNGYMWN033336auvvtKPP/6oqKgo1ahR45r7+eOPP1afPn30wQcfqGHDhvL29lbv3r0VExNz1fkA5K2s/u4dHR3Vq1cvLVu2zLrPydy5c1WyZEm1adPGijt58qT8/PzSrSejtuw6efKkpIyPXwEBAdb0NBkd011cXOyO6TnZ35MnT8rf3z9de0ZtV2ratKm++OILJSUlqXfv3ipdurSqVq2qxYsXZ2ndPXr00IwZM/T444/rm2++0bZt2xQVFaUSJUpkePy+Vk49ceKEJFn3RMnI8ePHJUkPPPBAurz02muvyRijU6dOZan/V1tHVvLe8ePHZYyRn59futitW7deM79mJKO/EzJrv/I75e/vn+6hOb6+vnJ0dEz3Pb3ys5D++zyy8/fN9S7Dz89PDz30kN577z0lJyfrt99+0/fff6+BAwdmed3A7SK7OSgj27ZtU+vWrSX9V6T+4YcfFBUVpdGjR0u6/vOazJw8eVKOjo4qUaKEXbvNZpO/v/91HY9yKm+ePn1aKSkp1503R40apSlTpmjr1q1q166dfHx81KJFC/3000/XnPfcuXO6++679eOPP2rChAnauHGjoqKitHTpUknpP4es/D1x4sQJBQQEqFChzMsnuZGrLnfy5EklJSVp+vTp6Zbfvn17SbLLm9k5r0TGeBom8r20g9eVN1nMStLKLRk9WbJ48eKqXr26XnnllQznCQgIuOoyixcvLjc3t0xv2nv5//Bk1oerTVuwYIF69+6tiRMn2rX/888/Klq06DX7Nm3aNE2bNk2HDh3S8uXLNXLkSMXGxmr16tVXnRdA3snO7/7RRx/V5MmTtWTJEj300ENavny5wsLC5ODgYMX4+PhYRZTL5UThPO2P+GPHjqWbdvTo0XTHwKwuc9u2benar6e/Pj4+Gc6X1WXde++9uvfee5WYmKitW7dq0qRJ6tGjh4KCgtSwYcNM54uLi9OKFSs0duxYjRw50mpPTEy87mJV2onVkSNHFBgYmGFM2v6ePn26GjRokGHMjRZJs5r3ihcvLpvNpu+//94q/F0uo7bc4uPjox9//FHGGLtcGxsbq6SkpOv6nuamZ599VvPnz9eXX36p1atXWw+5AGDv8hx05X8kZDUHLVmyRE5OTlqxYoVd8eWLL77I0b6m8fHxUVJSkk6cOGFXMDPGKCYmxhrVnN1l5kSeL1asmGw223XnTUdHRw0ZMkRDhgzRmTNntG7dOr3wwgtq06aNDh8+LHd390znXb9+vY4ePaqNGzdao8kkpXvoQXaUKFFCmzZtUkpKSqYFs9zOVcWKFZODg4N69eqV6cjw4OBgqy/ZOa9ExiiWId/z8/OTq6urfvvtN7v2L7/8Mo96lLGOHTtq1apVKl++fKaXR15r/okTJ8rHx8c60OUkm82W7iC9cuVK/f3337rjjjuyvJwyZcpo4MCB+vbbb/XDDz/kdDcB5KDs/O4rV66s+vXra86cOUpOTlZiYqIeffRRu5jQ0FCtWrVK//zzj/WHVkpKij799NMb7mvDhg3l5uamBQsW6MEHH7Tajxw5ovXr16e77C0rQkND9cknn+jrr7+2uwx/yZIl2V5W8+bNFRERoV9//dXuUsxFixZlazkuLi4KDQ1V0aJF9c033+iXX35Rw4YNMx1RbbPZZIxJ9zl+8MEHSk5OzvZ2SFLr1q3l4OCgmTNnZlqoa9y4sYoWLarff/8910YiZTXvdezYUa+++qr+/vvvHLnk90a0aNFCn3zyib744gt16dLFav/oo4+s6dl1o6Ppr6Z27dpq1KiRXnvtNe3atUtPPvmkPDw8cnw9QEGS0W/unnvukfTffzJdXmSKiorSH3/8YY0OS5s/o9+rzWaTo6Oj3X8yJSQkaP78+Tm+DdJ/x5uIiAgtWLBAzz33nNX++eef6/z589d1PMqpPO/h4aF69epp6dKlmjx5slU8PHv2rL766qtsLato0aJ64IEH9PfffyssLEwHDhxQlSpVrpo3pfTFqffeey9b671cu3bttHjxYs2dOzfTSzFzO1e5u7urefPm+uWXX1S9enVrRHRmfcnN88rbBcUy5Htp135/+OGHKl++vGrUqKFt27Zl+wQlt40fP15r165Vo0aNNHjwYFWsWFEXLlzQgQMHtGrVKr377rtXveQlLCxMn3/+uZo2barnnntO1atXV0pKig4dOqQ1a9Zo6NChql+//nX3r2PHjpo7d64qVaqk6tWra/v27Zo8efJV+yT9N6qhefPm6tGjhypVqiRPT09FRUVp9erV6tq163X3B0Duy+7v/rHHHlP//v119OhRNWrUSBUrVrSbPnr0aH311Vdq0aKFRo8eLTc3N7377rvWvauudnnCtRQtWlRjxozRCy+8oN69e+vhhx/WyZMnNW7cOLm6umrs2LHZXmafPn30xhtvqGfPnpowYYLuuOMOff311/rmm2+y3d+wsDB9+OGH6tChgyZMmCA/Pz8tXLhQf/755zXnfemll3TkyBG1aNFCpUuX1pkzZ/Tmm2/a3UOlfPnycnNz08KFC1W5cmUVLlxYAQEBCggIUNOmTTV58mQVL15cQUFBioyM1OzZs685KjgzQUFBeuGFF/Tyyy8rISFBDz/8sLy8vPT777/rn3/+0bhx41S4cGFNnz5dffr00alTp/TAAw/I19dXJ06c0K+//qoTJ05o5syZ17X+NFnNe40bN9aTTz6pRx99VD/99JOaNm0qDw8PHTt2TJs2bVK1atWs+3Pltt69e+vtt99Wnz59dODAAVWrVk2bNm3SxIkT1b59e7Vs2TLby6xWrZok6c0331SfPn3k5OSkihUr2t3r7EY8++yzeuihh2Sz2TRgwIAcWSZQkGX2m3vyySc1ffp0FSpUSO3atdOBAwc0ZswYBQYG2hWjqlWrpqVLl2rmzJmqXbu2ChUqpDp16qhDhw6aOnWqevTooSeffFInT57UlClTcm30a6tWrdSmTRuNGDFC8fHxaty4sX777TeNHTtWtWrVUq9evbK9zJzM8y+//LLatm2rVq1aaejQoUpOTtZrr70mDw+Pa46M7tSpk6pWrao6deqoRIkSOnjwoKZNm6ayZcsqJCREUuafY6NGjVSsWDE99dRTGjt2rJycnLRw4UL9+uuv2d4faR5++GHNmTNHTz31lPbs2aPmzZsrJSVFP/74oypXrqzu3bvflFz15ptvqkmTJrr77rv19NNPKygoSGfPntX+/fv11VdfWfexy+3zyttGnj1aADD///TKy5+4Ykz6p6TFxcWZxx9/3Pj5+RkPDw/TqVMnc+DAgUyfhnnl8vr06WM8PDzSrT80NNTceeed2epzRk/0SnPixAkzePBgExwcbJycnIy3t7epXbu2GT16tDl37tw1l3Hu3Dnz4osvmooVKxpnZ2fr0cDPPfec3eOGJZlnnnkm3fxp++3KJ58Y89+T5vr162d8fX2Nu7u7adKkifn+++/TPY3nyqdhXrhwwTz11FOmevXqpkiRIsbNzc1UrFjRjB071pw/fz6ruw1AHsjq7z5NXFyccXNzu+oTn77//ntTv3594+LiYvz9/c3zzz9vPQnwak97vNKVx/k0H3zwgalevbp1DLz33nvtniZsTObH9LQccLlDhw6Zrl27msKFCxtPT09z//33m1WrVhlJ5ssvv8xyf40x5vfffzetWrUyrq6uxtvb2/Tr1898+eWX13wa5ooVK0y7du1MqVKljLOzs/H19TXt27c333//vd3yFy9ebCpVqmScnJzs8tuRI0fM/fffb4oVK2Y8PT1N27Ztza5du9I9PSyzHJDRkxKN+e/pXnXr1jWurq6mcOHCplatWumexhkZGWk6dOhgvL29jZOTkylVqpTp0KGD+fTTT7O172407xljzIcffmjq169vPDw8jJubmylfvrzp3bu3+emnn7Lcj8z2UXb+fjh58qR56qmnTMmSJY2jo6MpW7asGTVqlLlw4YJdXGa5+srPzRhjRo0aZQICAkyhQoXsPqvM9tuVv+HMPmNjjElMTDQuLi6mbdu26aYBt6uMfnPJycnmtddeMxUqVDBOTk6mePHipmfPnubw4cN28546dco88MADpmjRosZms9nlnQ8//NBUrFjRuLi4mHLlyplJkyaZ2bNnp8t3OfE0TGP+e6LliBEjTNmyZY2Tk5MpWbKkefrpp83p06ft4rJ6LDEm5/K8Mf89bTstp5cpU8a8+uqrGebqK4+Lr7/+umnUqJEpXry4NW+/fv3MgQMH7ObL7Ni5efNm07BhQ+Pu7m5KlChhHn/8cfPzzz+ne+p0dv6eSEhIMC+99JIJCQkxzs7OxsfHx9xzzz1m8+bNdnE5kasyexpm2rTHHnvMlCpVyjg5OZkSJUqYRo0amQkTJtjFZSe/ImM2Y4y5GUU5AABwa2rdurUOHDigvXv35nVXsmTixIl68cUXdejQoWuOrgUKuq+++kqdO3fWypUrrZtAA0B2FLQ8D+QELsMEAABZNmTIENWqVUuBgYE6deqUFi5cqLVr12r27Nl53bUMzZgxQ5JUqVIlXbp0SevXr9dbb72lnj17UijDLe3333/XwYMHNXToUNWsWdPuvn0AkJmClueB3EKxDEiVnJysqw20tNlsdjfsBIDbUXJysl566SXFxMTIZrOpSpUqmj9/vnr27CnpvxsBp6SkXHUZjo43788Pd3d3vfHGGzpw4IASExNVpkwZjRgxQi+++KKk/54adq0b5Ts4OFz1CcS3s6SkpKtOL1So0A3dyy6r+BzTGzBggH744Qfdddddmjdv3m217UBBkt/OQQpani9IyFUFC5dhAqmCgoJ08ODBTKeHhoZq48aNN69DAFAAhYeHa9y4cVeNiY6OVlBQ0M3p0DVs3LhRzZs3v2rMnDlz1Ldv35vToQLkwIED13zK1tixYxUeHp7rfZk7d266p7deacOGDWrWrFmu9wUAsqNZs2aKjIzMdHrZsmV14MCBm9ehayhoeT4/IVcVLBTLgFQ7d+5UYmJiptM9PT3TPRkOAGDv6NGjOnr06FVjrvXI85vp7Nmz2rNnz1VjgoOD5ePjc5N6VHBcvHhRv/3221Vj0p7omdtOnjyp6Ojoq8bk5BMmASCn7NmzR2fPns10uouLi/Xkx/ygoOX5/IRcVbBQLAMAAAAAAABS5f5NJAAAAAAAAIAC4pa9815KSoqOHj0qT09PbpAH4LZljNHZs2cVEBBwU26yjfTIRwBAPsoPyEcAkPV8dMsWy44eParAwMC87gYA5AuHDx9W6dKl87obtyXyEQD8P/JR3iEfAcD/u1Y+umWLZWk3xTt8+LCKFCmSx70BgLwRHx+vwMBAbhSah8hHAEA+yg/IRwCQ9Xx0yxbL0oYWFylShGQA4LbH5RZ5h3wEAP+PfJR3yEcA8P+ulY+4YQAAAAAAAACQimIZAAAAAAAAkIpiGQAAAAAAAJCKYhkAAAAAAACQ6pa9wf+NChq58qav88CrHW76OgEA+Rv5CACQ1/IiF0nkIwB5h5FlAAAAAAAAQCqKZQAAAAAAAEAqimUAAAAAAABAKoplAAAAAAAAQCqKZQAAAAAAAEAqimUAAAAAAABAKoplAAAAAAAAQCqKZQAAAAAAAEAqimUAAAAAAABAKoplAAAAAAAAQCqKZQAAAAAAAEAqimUAAAAAcB1mzpyp6tWrq0iRIipSpIgaNmyor7/+2ppujFF4eLgCAgLk5uamZs2aaffu3XbLSExM1KBBg1S8eHF5eHioc+fOOnLkiF3M6dOn1atXL3l5ef0fe/ceF1Wd/3H8PXIZLuIkGIwkKrbmJdA1bRFrF0xFTfLn2mZlkW6uWqbGqutmdqG2pGzX3MWyMldNNNqLdrMlMZVyvWZRaubWpqUJYoZ4yUDx+/sjONsIKDe5zLyej8c8Hs45nznn+/0K58P5zPecI4fDoaSkJB09erQ+uggAHoliGQAAAADUQJs2bfTEE0/o/fff1/vvv6/rrrtO//d//2cVxGbPnq05c+Zo3rx52rZtm5xOpwYMGKDjx49b20hOTtbKlSuVkZGhDRs26MSJE0pMTFRJSYkVM3LkSOXk5CgzM1OZmZnKyclRUlJSvfcXADyFd0M3AAAAAACaohtuuMHl/eOPP6758+dr8+bN6tq1q+bOnauZM2dq+PDhkqQlS5YoLCxMy5cv1/jx41VYWKiFCxdq6dKl6t+/vyQpPT1dERERWrNmjQYOHKjdu3crMzNTmzdvVkxMjCRpwYIFio2N1Z49e9SpU6f67TQAeABmlgEAAABALZWUlCgjI0MnT55UbGys9u7dq7y8PCUkJFgxdrtdcXFx2rhxoyRp+/btOn36tEtMeHi4oqKirJhNmzbJ4XBYhTJJ6t27txwOhxVTkaKiIh07dszlBQCoGoplAAAAAFBDO3bsUPPmzWW323XXXXdp5cqV6tq1q/Ly8iRJYWFhLvFhYWHWury8PPn6+qply5bnjQkNDS2339DQUCumIqmpqdY9zhwOhyIiImrVTwDwJBTLAAAAAKCGOnXqpJycHG3evFl33323Ro0apU8++cRab7PZXOKNMeWWnevcmIriL7SdGTNmqLCw0Hrt37+/ql0CAI9HsQwAAAAAasjX11c/+clP1KtXL6Wmpqp79+7685//LKfTKUnlZn/l5+dbs82cTqeKi4tVUFBw3phDhw6V2+/hw4fLzVr7Mbvdbj2ls+wFAKgaimUAAAAAUEeMMSoqKlJkZKScTqeysrKsdcXFxcrOzlafPn0kST179pSPj49LTG5urnbu3GnFxMbGqrCwUFu3brVitmzZosLCQisGAFC3eBomAAAAANTA/fffr8GDBysiIkLHjx9XRkaG1q9fr8zMTNlsNiUnJ2vWrFnq2LGjOnbsqFmzZikgIEAjR46UJDkcDo0ZM0ZTp05VSEiIgoODNW3aNEVHR1tPx+zSpYsGDRqksWPH6vnnn5ckjRs3TomJiTwJEwAuEoplAAAAAFADhw4dUlJSknJzc+VwONStWzdlZmZqwIABkqTp06fr1KlTmjBhggoKChQTE6PVq1crKCjI2sbTTz8tb29vjRgxQqdOnVK/fv20ePFieXl5WTHLli3T5MmTradmDh06VPPmzavfzgKAB+EyTABAk/Puu+/qhhtuUHh4uGw2m1599VWX9cYYpaSkKDw8XP7+/oqPj9euXbtcYoqKijRp0iS1atVKgYGBGjp0qA4cOOASU1BQoKSkJOtJYklJSTp69OhF7h0AoKlYuHCh9u3bp6KiIuXn52vNmjVWoUz64cb8KSkpys3N1ffff6/s7GxFRUW5bMPPz09paWk6cuSIvvvuO73xxhvlnlwZHBys9PR0HTt2TMeOHVN6erouueSS+ugiAHgkimUAgCbn5MmT6t69e6Xfqs+ePVtz5szRvHnztG3bNjmdTg0YMEDHjx+3YpKTk7Vy5UplZGRow4YNOnHihBITE1VSUmLFjBw5Ujk5OcrMzFRmZqZycnKUlJR00fsHAAAAoOHUqliWmppqXYtfhm/zAQAX2+DBg/XYY49p+PDh5dYZYzR37lzNnDlTw4cPV1RUlJYsWaLvvvtOy5cvlyQVFhZq4cKF+tOf/qT+/furR48eSk9P144dO7RmzRpJ0u7du5WZmakXX3xRsbGxio2N1YIFC/Tmm29qz5499dpfAAAAAPWnxsWybdu26YUXXlC3bt1clvNtPgCgIe3du1d5eXnWfV0kyW63Ky4uThs3bpQkbd++XadPn3aJCQ8PV1RUlBWzadMmORwOxcTEWDG9e/eWw+GwYipSVFRkXSZT9gIAAADQdNSoWHbixAnddtttWrBggVq2bGkt59t8AEBDy8vLkySFhYW5LA8LC7PW5eXlydfX1yWHVRQTGhpabvuhoaFWTEVSU1OtWdEOh6PcfWcAAAAANG41Kpbdc889GjJkiPU44zIN+W0+3+QDAH7MZrO5vDfGlFt2rnNjKoq/0HZmzJihwsJC67V///5qthwAAABAQ6p2sSwjI0MffPCBUlNTy61ryG/z+SYfACBJTqdTksrli/z8fCs/OZ1OFRcXq6Cg4Lwxhw4dKrf9w4cPl8tzP2a329WiRQuXFwAAAICmo1rFsv379+vee+9Venq6/Pz8Ko1riG/z+SYfACBJkZGRcjqdysrKspYVFxcrOztbffr0kST17NlTPj4+LjG5ubnauXOnFRMbG6vCwkJt3brVitmyZYsKCwutGAAAAADux7s6wdu3b1d+fr569uxpLSspKdG7776refPmWfcTy8vLU+vWra2Yyr7N//Hssvz8fOvkoybf5tvtdtnt9up0BwDQRJ04cUKff/659X7v3r3KyclRcHCw2rZtq+TkZM2aNUsdO3ZUx44dNWvWLAUEBGjkyJGSJIfDoTFjxmjq1KkKCQlRcHCwpk2bpujoaOsWA126dNGgQYM0duxYPf/885KkcePGKTExUZ06dar/TgMAAACoF9WaWdavXz/t2LFDOTk51qtXr1667bbblJOTow4dOvBtPgDgonv//ffVo0cP9ejRQ5I0ZcoU9ejRQw899JAkafr06UpOTtaECRPUq1cvff3111q9erWCgoKsbTz99NMaNmyYRowYoWuuuUYBAQF644035OXlZcUsW7ZM0dHRSkhIUEJCgrp166alS5fWb2cBAAAA1KtqzSwLCgpSVFSUy7LAwECFhIRYy/k2HwBwscXHx8sYU+l6m82mlJQUpaSkVBrj5+entLQ0paWlVRoTHBys9PT02jQVAAAAQBNTrWJZVUyfPl2nTp3ShAkTVFBQoJiYmAq/zff29taIESN06tQp9evXT4sXLy73bf7kyZOtp2YOHTpU8+bNq+vmAgAAAAAAAJZaF8vWr1/v8p5v8wEAAAAAANBUVeueZQAAAAAAAIA7o1gGAAAAAAAAlKJYBgAAAAAAAJSiWAYAAAAAAACUolgGAAAAAAAAlKJYBgAAAAAAAJSiWAYAAAAAAACUolgGAAAAAAAAlKJYBgAAAAAAAJSiWAYAAAAAAACUolgGAAAAAAAAlKJYBgAAAAAAAJSiWAYAAAAAAACUolgGAAAAAAAAlKJYBgAAAAAAAJSiWAYAAAAAAACUolgGAAAAAAAAlKJYBgAAAAAAAJSiWAYAAAAAAACUolgGAAAAAAAAlKJYBgAAAAAAAJSiWAYAAAAAAACUolgGAAAAADWQmpqqq6++WkFBQQoNDdWwYcO0Z88elxhjjFJSUhQeHi5/f3/Fx8dr165dLjFFRUWaNGmSWrVqpcDAQA0dOlQHDhxwiSkoKFBSUpIcDoccDoeSkpJ09OjRi91FAPBIFMsAAAAAoAays7N1zz33aPPmzcrKytKZM2eUkJCgkydPWjGzZ8/WnDlzNG/ePG3btk1Op1MDBgzQ8ePHrZjk5GStXLlSGRkZ2rBhg06cOKHExESVlJRYMSNHjlROTo4yMzOVmZmpnJwcJSUl1Wt/AcBTeDd0AwAAAACgKcrMzHR5v2jRIoWGhmr79u36xS9+IWOM5s6dq5kzZ2r48OGSpCVLligsLEzLly/X+PHjVVhYqIULF2rp0qXq37+/JCk9PV0RERFas2aNBg4cqN27dyszM1ObN29WTEyMJGnBggWKjY3Vnj171KlTp/rtOAC4OWaWAQAAAEAdKCwslCQFBwdLkvbu3au8vDwlJCRYMXa7XXFxcdq4caMkafv27Tp9+rRLTHh4uKKioqyYTZs2yeFwWIUySerdu7ccDocVc66ioiIdO3bM5QUAqBqKZQAAAABQS8YYTZkyRddee62ioqIkSXl5eZKksLAwl9iwsDBrXV5ennx9fdWyZcvzxoSGhpbbZ2hoqBVzrtTUVOv+Zg6HQxEREbXrIAB4EIplAAAAAFBLEydO1Mcff6yXX3653Dqbzeby3hhTbtm5zo2pKP5825kxY4YKCwut1/79+6vSDQCAKJYBAAAAQK1MmjRJr7/+utatW6c2bdpYy51OpySVm/2Vn59vzTZzOp0qLi5WQUHBeWMOHTpUbr+HDx8uN2utjN1uV4sWLVxeAICqoVgGAAAAADVgjNHEiRO1YsUKrV27VpGRkS7rIyMj5XQ6lZWVZS0rLi5Wdna2+vTpI0nq2bOnfHx8XGJyc3O1c+dOKyY2NlaFhYXaunWrFbNlyxYVFhZaMQCAusPTMAEAAACgBu655x4tX75cr732moKCgqwZZA6HQ/7+/rLZbEpOTtasWbPUsWNHdezYUbNmzVJAQIBGjhxpxY4ZM0ZTp05VSEiIgoODNW3aNEVHR1tPx+zSpYsGDRqksWPH6vnnn5ckjRs3TomJiTwJEwAuAoplAAAAAFAD8+fPlyTFx8e7LF+0aJFGjx4tSZo+fbpOnTqlCRMmqKCgQDExMVq9erWCgoKs+Kefflre3t4aMWKETp06pX79+mnx4sXy8vKyYpYtW6bJkydbT80cOnSo5s2bd3E7CAAeimIZAAAAANSAMeaCMTabTSkpKUpJSak0xs/PT2lpaUpLS6s0Jjg4WOnp6TVpJgCgmiiWAQAAj9X+vlX1vs99Twyp9302RD89TUP8vwIAgIuDYhkAAHDhKQUkAAAAoCIUywAAQINj5hOaOorMAAC4D4plAAAAQBNEgQ4AgIujWUM3AAAAAAAAAGgsKJYBAAAAAAAApSiWAQAAAAAAAKW4ZxkAAEA94mEGAAAAjRszywAAAAAAAIBSFMsAAAAAAACAUhTLAAAAAAAAgFIUywAAAAAAAIBSFMsAAAAAAACAUhTLAAAAAAAAgFIUywAAAAAAAIBSFMsAAAAAAACAUhTLAABuKSUlRTabzeXldDqt9cYYpaSkKDw8XP7+/oqPj9euXbtctlFUVKRJkyapVatWCgwM1NChQ3XgwIH67goAAACAelStYllqaqquvvpqBQUFKTQ0VMOGDdOePXtcYurq5KOgoEBJSUlyOBxyOBxKSkrS0aNHa9ZLAIBHuvLKK5Wbm2u9duzYYa2bPXu25syZo3nz5mnbtm1yOp0aMGCAjh8/bsUkJydr5cqVysjI0IYNG3TixAklJiaqpKSkIboDAAAAoB5Uq1iWnZ2te+65R5s3b1ZWVpbOnDmjhIQEnTx50oqpq5OPkSNHKicnR5mZmcrMzFROTo6SkpLqoMsAAE/h7e0tp9NpvS699FJJP3yxM3fuXM2cOVPDhw9XVFSUlixZou+++07Lly+XJBUWFmrhwoX605/+pP79+6tHjx5KT0/Xjh07tGbNmobsFgAAAICLqFrFsszMTI0ePVpXXnmlunfvrkWLFumrr77S9u3bJdXdycfu3buVmZmpF198UbGxsYqNjdWCBQv05ptvlpvJBgBAZT777DOFh4crMjJSt9xyi7744gtJ0t69e5WXl6eEhAQr1m63Ky4uThs3bpQkbd++XadPn3aJCQ8PV1RUlBVTkaKiIh07dszlBQAAAKDpqNU9ywoLCyVJwcHBkuru5GPTpk1yOByKiYmxYnr37i2Hw1HpCQonJwCAH4uJidFLL72kt99+WwsWLFBeXp769OmjI0eOKC8vT5IUFhbm8pmwsDBrXV5ennx9fdWyZctKYyqSmppq3ULA4XAoIiKijnsGAAAA4GKqcbHMGKMpU6bo2muvVVRUlCTV2clHXl6eQkNDy+0zNDS00hMUTk4AAD82ePBg3XjjjYqOjlb//v21atUqSdKSJUusGJvN5vIZY0y5Zee6UMyMGTNUWFhovfbv31+LXgAAAACobzUulk2cOFEff/yxXn755XLr6uLko6L4822HkxMAwPkEBgYqOjpan332mfVUzHO/gMnPz7e+8HE6nSouLlZBQUGlMRWx2+1q0aKFywsAAABA01GjYtmkSZP0+uuva926dWrTpo21vK5OPpxOpw4dOlRuv4cPH670BIWTEwDA+RQVFWn37t1q3bq1IiMj5XQ6lZWVZa0vLi5Wdna2+vTpI0nq2bOnfHx8XGJyc3O1c+dOKwYAAACA+6lWscwYo4kTJ2rFihVau3atIiMjXdbX1clHbGysCgsLtXXrVitmy5YtKiws5AQFAFAl06ZNU3Z2tvbu3astW7boV7/6lY4dO6ZRo0bJZrMpOTlZs2bN0sqVK7Vz506NHj1aAQEBGjlypCTJ4XBozJgxmjp1qt555x19+OGHuv32263LOgEAAAC4J+/qBN9zzz1avny5XnvtNQUFBVkzyBwOh/z9/V1OPjp27KiOHTtq1qxZlZ58hISEKDg4WNOmTXM5+ejSpYsGDRqksWPH6vnnn5ckjRs3TomJierUqVNd9h8A4KYOHDigW2+9Vd98840uvfRS9e7dW5s3b1a7du0kSdOnT9epU6c0YcIEFRQUKCYmRqtXr1ZQUJC1jaefflre3t4aMWKETp06pX79+mnx4sXy8vJqqG4BAAAAuMiqVSybP3++JCk+Pt5l+aJFizR69GhJdXfysWzZMk2ePNl6aubQoUM1b968mvQRAOCBMjIyzrveZrMpJSVFKSkplcb4+fkpLS1NaWlpddw6AAAAAI1VtYplxpgLxtTVyUdwcLDS09Or0zwAAAAAAACgVmr8NEwAAAAAAADA3VAsAwAAAAAAAEpRLAMAAAAAAABKUSwDAAAAAAAASlEsAwAAAAAAAEpRLAMAAACAGnj33Xd1ww03KDw8XDabTa+++qrLemOMUlJSFB4eLn9/f8XHx2vXrl0uMUVFRZo0aZJatWqlwMBADR06VAcOHHCJKSgoUFJSkhwOhxwOh5KSknT06NGL3DsA8FwUywAAAACgBk6ePKnu3btr3rx5Fa6fPXu25syZo3nz5mnbtm1yOp0aMGCAjh8/bsUkJydr5cqVysjI0IYNG3TixAklJiaqpKTEihk5cqRycnKUmZmpzMxM5eTkKCkp6aL3DwA8lXdDNwAAAAAAmqLBgwdr8ODBFa4zxmju3LmaOXOmhg8fLklasmSJwsLCtHz5co0fP16FhYVauHChli5dqv79+0uS0tPTFRERoTVr1mjgwIHavXu3MjMztXnzZsXExEiSFixYoNjYWO3Zs0edOnWqn84CgAehWAYAAAAAdWzv3r3Ky8tTQkKCtcxutysuLk4bN27U+PHjtX37dp0+fdolJjw8XFFRUdq4caMGDhyoTZs2yeFwWIUySerdu7ccDoc2btxYabGsqKhIRUVF1vtjx45dhF5eXO3vW1Xv+9z3xJB63yeAxofLMAEAAACgjuXl5UmSwsLCXJaHhYVZ6/Ly8uTr66uWLVueNyY0NLTc9kNDQ62YiqSmplr3OHM4HIqIiKhVfwDAk1AsAwAAAICLxGazubw3xpRbdq5zYyqKv9B2ZsyYocLCQuu1f//+arYcADwXxTIAAAAAqGNOp1OSys3+ys/Pt2abOZ1OFRcXq6Cg4Lwxhw4dKrf9w4cPl5u19mN2u10tWrRweQEAqoZiGQAAAADUscjISDmdTmVlZVnLiouLlZ2drT59+kiSevbsKR8fH5eY3Nxc7dy504qJjY1VYWGhtm7dasVs2bJFhYWFVgwAoG5xg38AAAAAqIETJ07o888/t97v3btXOTk5Cg4OVtu2bZWcnKxZs2apY8eO6tixo2bNmqWAgACNHDlSkuRwODRmzBhNnTpVISEhCg4O1rRp0xQdHW09HbNLly4aNGiQxo4dq+eff16SNG7cOCUmJvIkTAC4SCiWAQAAAEANvP/+++rbt6/1fsqUKZKkUaNGafHixZo+fbpOnTqlCRMmqKCgQDExMVq9erWCgoKszzz99NPy9vbWiBEjdOrUKfXr10+LFy+Wl5eXFbNs2TJNnjzZemrm0KFDNW/evHrqJQB4HoplAAAAAFAD8fHxMsZUut5msyklJUUpKSmVxvj5+SktLU1paWmVxgQHBys9Pb02TQUAVAP3LAMAAAAAAABKUSwDAAAAAAAASlEsAwAAAAAAAEpRLAMAAAAAAABKUSwDAAAAAAAASlEsAwAAAAAAAEpRLAMAAAAAAABKeTd0AwAAAAAAaAza37eqQfa774khDbJfABVjZhkAAAAAAABQimIZAAAAAAAAUIpiGQAAAAAAAFCKYhkAAAAAAABQimIZAAAAAAAAUIpiGQAAAAAAAFCKYhkAAAAAAABQyruhGwAAAAAAgCdrf9+qet/nvieG1Ps+gaaCmWUAAAAAAABAKWaWAQAAAADgYZjNBlSOmWUAAAAAAABAKYplAAAAAAAAQCmKZQAAAAAAAEApimUAAAAAAABAKYplAAAAAAAAQCmehgkAAAAAAC66hngCp8RTOFF9zCwDAAAAAAAASlEsAwAAAAAAAEpxGSYAAAAAAHBbDXH5J5d+Nm3MLAMAAAAAAABKUSwDAAAAAAAASnEZJgAAAAAAQB3i0s+mjWIZPOaX2JMeU+wp/6eSZ/UVAAAAAHDxUSxrRBqqmNMQ6Kv78ZR+ShToAAAAAMCdUSwDAAAAAABo4jzpaqqLjRv8AwAAAAAAAKUa/cyyZ599Vk899ZRyc3N15ZVXau7cufr5z3/e0M0CAHgQchEAoDEgHwFojNzxNjWNembZK6+8ouTkZM2cOVMffvihfv7zn2vw4MH66quvGrppAAAPQS4CADQG5CMAqD+Nulg2Z84cjRkzRr/5zW/UpUsXzZ07VxEREZo/f35DNw0A4CHIRQCAxoB8BAD1p9FehllcXKzt27frvvvuc1mekJCgjRs3losvKipSUVGR9b6wsFCSdOzYsRrt/2zRdzX6HABcDDU9lpV9zhhTl83xGNXNRRL5CIB7Ix81jIbOR+QiAI3Nxc5HjbZY9s0336ikpERhYWEuy8PCwpSXl1cuPjU1VY888ki55RERERetjQBQXxxza/f548ePy+Fw1ElbPEl1c5FEPgLg3shHDYN8BACuLnY+arTFsjI2m83lvTGm3DJJmjFjhqZMmWK9P3v2rL799luFhIRUGH8+x44dU0REhPbv368WLVrUrOGNjDv2SXLPftGnpqMp9MsYo+PHjys8PLyhm9KkVTUXSeSjmvCUfkr01V15Sl9r00/yUd1oiHzkKT/f9YGxrBuMY93xxLGsaj5qtMWyVq1aycvLq9w3Jfn5+eW+UZEku90uu93usuySSy6pVRtatGjhdj8w7tgnyT37RZ+ajsbeL77Br7nq5iKJfFQbntJPib66K0/pa037ST6qucaQjzzl57s+MJZ1g3GsO542llXJR432Bv++vr7q2bOnsrKyXJZnZWWpT58+DdQqAIAnIRcBABoD8hEA1K9GO7NMkqZMmaKkpCT16tVLsbGxeuGFF/TVV1/prrvuauimAQA8BLkIANAYkI8AoP406mLZzTffrCNHjujRRx9Vbm6uoqKi9NZbb6ldu3YXdb92u10PP/xwuWnLTZk79klyz37Rp6bDXfsFVw2ViyTP+RnzlH5K9NVdeUpfPaWfjRXnRk0fY1k3GMe6w1hWzmZ4fjMAAAAAAAAgqRHfswwAAAAAAACobxTLAAAAAAAAgFIUywAAAAAAAIBSFMsAAAAAAACAUhTLzvHss88qMjJSfn5+6tmzp957772GblKtpKam6uqrr1ZQUJBCQ0M1bNgw7dmzp6GbVadSU1Nls9mUnJzc0E2pta+//lq33367QkJCFBAQoJ/+9Kfavn17Qzerxs6cOaMHHnhAkZGR8vf3V4cOHfToo4/q7NmzDd20Knv33Xd1ww03KDw8XDabTa+++qrLemOMUlJSFB4eLn9/f8XHx2vXrl0N01i4FXfLR5Ln/D5VJfe6S1/nz5+vbt26qUWLFmrRooViY2P1r3/9y1rvLv08V0V/e7hLX1NSUmSz2VxeTqfTWu8u/UTVuWM+qo26yGVFRUWaNGmSWrVqpcDAQA0dOlQHDhxwiSkoKFBSUpIcDoccDoeSkpJ09OjRi9y7+lFXedLTx1GqmzzMOFaMYtmPvPLKK0pOTtbMmTP14Ycf6uc//7kGDx6sr776qqGbVmPZ2dm65557tHnzZmVlZenMmTNKSEjQyZMnG7ppdWLbtm164YUX1K1bt4ZuSq0VFBTommuukY+Pj/71r3/pk08+0Z/+9CddcsklDd20GnvyySf13HPPad68edq9e7dmz56tp556SmlpaQ3dtCo7efKkunfvrnnz5lW4fvbs2ZozZ47mzZunbdu2yel0asCAATp+/Hg9txTuxB3zkeQ5v09Vyb3u0tc2bdroiSee0Pvvv6/3339f1113nf7v//7P+kPcXfr5Y5X97eFOfb3yyiuVm5trvXbs2GGtc6d+4sLcNR/VRl3ksuTkZK1cuVIZGRnasGGDTpw4ocTERJWUlFgxI0eOVE5OjjIzM5WZmamcnBwlJSVd9P7Vh7rKk54+jlLd5GHGsRIGlp/97GfmrrvuclnWuXNnc9999zVQi+pefn6+kWSys7Mbuim1dvz4cdOxY0eTlZVl4uLizL333tvQTaqV3//+9+baa69t6GbUqSFDhpg777zTZdnw4cPN7bff3kAtqh1JZuXKldb7s2fPGqfTaZ544glr2ffff28cDod57rnnGqCFcBeekI886ffp3Nzrzn01xpiWLVuaF1980S37WdnfHu7U14cffth07969wnXu1E9UjSfko9qoSS47evSo8fHxMRkZGVbM119/bZo1a2YyMzONMcZ88sknRpLZvHmzFbNp0yYjyXz66acXuVf1ryZ5knGsXHXyMONYOWaWlSouLtb27duVkJDgsjwhIUEbN25soFbVvcLCQklScHBwA7ek9u655x4NGTJE/fv3b+im1InXX39dvXr10k033aTQ0FD16NFDCxYsaOhm1cq1116rd955R//5z38kSR999JE2bNig66+/voFbVjf27t2rvLw8l+OG3W5XXFycWx03UL88JR+dy51/n87Nve7a15KSEmVkZOjkyZOKjY11y35W9reHu/X1s88+U3h4uCIjI3XLLbfoiy++kOR+/cT5eWo+qo2q/I5s375dp0+fdokJDw9XVFSUFbNp0yY5HA7FxMRYMb1795bD4XDLsa9JnmQcy6tJHmYcK+fd0A1oLL755huVlJQoLCzMZXlYWJjy8vIaqFV1yxijKVOm6Nprr1VUVFRDN6dWMjIy9MEHH2jbtm0N3ZQ688UXX2j+/PmaMmWK7r//fm3dulWTJ0+W3W7XHXfc0dDNq5Hf//73KiwsVOfOneXl5aWSkhI9/vjjuvXWWxu6aXWi7NhQ0XHjyy+/bIgmwQ14Qj6qiLv+PlWUe92trzt27FBsbKy+//57NW/eXCtXrlTXrl2tP6DdpZ/n+9vDnf5PY2Ji9NJLL+mKK67QoUOH9Nhjj6lPnz7atWuXW/UTF+ap+ag2qvI7kpeXJ19fX7Vs2bJcTNnn8/LyFBoaWm77oaGhbjf2Nc2TjOP/1CYPM46Vo1h2DpvN5vLeGFNuWVM1ceJEffzxx9qwYUNDN6VW9u/fr3vvvVerV6+Wn59fQzenzpw9e1a9evXSrFmzJEk9evTQrl27NH/+/CZbLHvllVeUnp6u5cuX68orr1ROTo6Sk5MVHh6uUaNGNXTz6ow7HzfQcDz158rd+n2+3Osufe3UqZNycnJ09OhR/fOf/9SoUaOUnZ1trXeHflb1bw936OvgwYOtf0dHRys2NlaXX365lixZot69e0tyj36i6vj/rr6ajNm5MRXFu+PY13We9MRxvBh52BPH8VxchlmqVatW8vLyKlcZzc/PL1eJbYomTZqk119/XevWrVObNm0aujm1sn37duXn56tnz57y9vaWt7e3srOz9Ze//EXe3t4uNyJsSlq3bq2uXbu6LOvSpUuTvoHq7373O91333265ZZbFB0draSkJP32t79VampqQzetTpQ9HcxdjxtoGO6ejyrjjr9PleVed+urr6+vfvKTn6hXr15KTU1V9+7d9ec//9mt+nmhvz3K+uMOfT1XYGCgoqOj9dlnn7nV/ykuzFPzUW1U5XfE6XSquLhYBQUF5405dOhQue0fPnzYrca+NnmScfyf2uRhxrFyFMtK+fr6qmfPnsrKynJZnpWVpT59+jRQq2rPGKOJEydqxYoVWrt2rSIjIxu6SbXWr18/7dixQzk5OdarV69euu2225STkyMvL6+GbmKNXHPNNeUemfyf//xH7dq1a6AW1d53332nZs1cDzNeXl46e/ZsA7WobkVGRsrpdLocN4qLi5Wdnd2kjxtoWO6ajy7EnX6fLpR73amvFTHGqKioyK36eaG/PTp06OA2fT1XUVGRdu/erdatW7vV/ykuzFPzUW1U5XekZ8+e8vHxcYnJzc3Vzp07rZjY2FgVFhZq69atVsyWLVtUWFjoFmNfF3mScaxcdfIw43ge9fUkgaYgIyPD+Pj4mIULF5pPPvnEJCcnm8DAQLNv376GblqN3X333cbhcJj169eb3Nxc6/Xdd981dNPqlDs8DXPr1q3G29vbPP744+azzz4zy5YtMwEBASY9Pb2hm1Zjo0aNMpdddpl58803zd69e82KFStMq1atzPTp0xu6aVV2/Phx8+GHH5oPP/zQSDJz5swxH374ofnyyy+NMcY88cQTxuFwmBUrVpgdO3aYW2+91bRu3docO3asgVuOpswd85ExnvP7VJXc6y59nTFjhnn33XfN3r17zccff2zuv/9+06xZM7N69WpjjPv0syLn/u3hLn2dOnWqWb9+vfniiy/M5s2bTWJiogkKCrKOP+7ST1SNu+aj2qiLXHbXXXeZNm3amDVr1pgPPvjAXHfddaZ79+7mzJkzVsygQYNMt27dzKZNm8ymTZtMdHS0SUxMrPf+Xgx1lSc9fRyNqZs8zDhWjGLZOZ555hnTrl074+vra6666irr8bVNlaQKX4sWLWroptUpdyiWGWPMG2+8YaKioozdbjedO3c2L7zwQkM3qVaOHTtm7r33XtO2bVvj5+dnOnToYGbOnGmKiooaumlVtm7dugp/h0aNGmWM+eHR1g8//LBxOp3GbrebX/ziF2bHjh0N22i4BXfLR8Z4zu9TVXKvu/T1zjvvtH5OL730UtOvXz/rD3Rj3KefFTn3bw936evNN99sWrdubXx8fEx4eLgZPny42bVrl7XeXfqJqnPHfFQbdZHLTp06ZSZOnGiCg4ONv7+/SUxMNF999ZVLzJEjR8xtt91mgoKCTFBQkLnttttMQUFBPfXy4qqrPOnp42hM3eRhxrFiNmOMubhz1wAAAAAAAICmgXuWAQAAAAAAAKUolgEAAAAAAAClKJYBAAAAAAAApSiWAQAAAAAAAKUolgEAAAAAAAClKJah3thsNk2cOPGi7yMlJeWi7qMm9u3bJ5vNpsWLF3vUvuvSJ598opSUFO3bt6/cuvj4eEVFRV1wG+4yFgBcbdy4USkpKTp69GiNPr98+XLNnTu3Vm2Ij49XfHx8rbbRGDz77LMVHiPXr18vm82mf/zjHxfcxujRo9W+ffu6b1ypBx54QG3btpW3t7cuueSSi7YfAJ7FnXPJ6NGj1bx58zrfLmrv4MGDSklJUU5OTkM3BeegWAagSfjkk0/0yCOPVFgsA+DZNm7cqEceeaRBT3DcRWXFssbitdde0+OPP6477rhD2dnZWrNmTUM3CYCbIJegIRw8eFCPPPIIxbJGyLuhGwAAAABUxc6dOyVJkydPVmhoaAO3BgDwY6dOnZK/v39DN6NJKCkp0ZkzZy7a9k+dOiU/Pz/ZbLaLtg93x8yyJuDzzz/Xr3/9a3Xs2FEBAQG67LLLdMMNN2jHjh1WzOHDh+Xr66sHH3yw3Oc//fRT2Ww2/eUvf7GWbdiwQbGxsfLz89Nll12mBx98UC+++KJsNlu1Z+689tpr6tatm+x2uzp06KA///nPSklJqfQX8/nnn9cVV1whu92url27KiMjo1r7k6Rjx45p7NixCgkJUfPmzTVo0CD95z//qTB2w4YN6tevn4KCghQQEKA+ffpo1apVLtvy9vbWU089ZS375ptv1KxZMzkcDpeD2OTJk3XppZfKGCPpf5f/bdu2TT//+c8VEBCgDh066IknntDZs2cv2I8LtU364f92woQJ6tq1q5o3b67Q0FBdd911eu+998pt7+DBgxoxYoSCgoLkcDh08803Ky8v74LtOFfZ5TbLly/X73//e7Vu3VrNmzfXDTfcoEOHDun48eMaN26cWrVqpVatWunXv/61Tpw44bKN77//XjNmzFBkZKR8fX112WWX6Z577in3bV379u2VmJiozMxMXXXVVfL391fnzp3117/+1YpZvHixbrrpJklS3759ZbPZKrycsrr/D++9955sNptefvnlcuteeukl2Ww2bdu2rZqjB6A+paSk6He/+50kKTIy0jo+rF+/XmfPntXs2bPVuXNn2e12hYaG6o477tCBAwesz8fHx2vVqlX68ssvrc/+OH898sgjiomJUXBwsFq0aKGrrrpKCxcutPJAbZTlkE2bNqlPnz7y9/dX+/bttWjRIknSqlWrdNVVVykgIEDR0dHKzMwst42q5JHFixfLZrNp3bp1uvvuu9WqVSuFhIRo+PDhOnjwoBXXvn177dq1S9nZ2dY4nHs55enTpzVz5kyFh4erRYsW6t+/v/bs2XPefvbr10+dO3cuN2bGGP3kJz/RkCFDqjRe7du31wMPPCBJCgsLK3frhVdeeUWxsbEKDAxU8+bNNXDgQH344YfltvP+++9r6NChCg4Olp+fn3r06KG//e1vVWoDAPfUlHOJJGVmZqpfv35yOBwKCAhQly5dlJqaWi7u888/1/XXX6/mzZsrIiJCU6dOVVFRkUtMVdta9jf8ihUr1KNHD/n5+emRRx6RJO3atUsJCQkKCAjQpZdeqnvuuUerVq2yxvTH1qxZo379+qlFixYKCAjQNddco3feeccl5vDhwxo3bpwiIiJkt9t16aWX6pprrqn27OLNmzfrmmuukZ+fn8LDwzVjxgwtWLCg3DlwZbf2ad++vUaPHu3Srqqcp5XdCmb27Nl67LHHFBkZKbvdrnXr1unqq6+WJP3617+2fm5+vO+q5KyyPL969WrdeeeduvTSSxUQEFDu/xbVZNDoZWdnm6lTp5p//OMfJjs726xcudIMGzbM+Pv7m08//dSK++Uvf2kiIiJMSUmJy+enT59ufH19zTfffGOMMeajjz4yfn5+plu3biYjI8O8/vrr5vrrrzft27c3kszevXur3LZ//etfplmzZiY+Pt6sXLnS/P3vfzcxMTHWtn5MkomIiDBdu3Y1L7/8snn99dfNoEGDjCTz97//vcr7PHv2rOnbt6+x2+3m8ccfN6tXrzYPP/yw6dChg5FkHn74YSt2/fr1xsfHx/Ts2dO88sor5tVXXzUJCQnGZrOZjIwMK653794mISHBep+RkWH8/PyMzWYz//73v63lXbp0MSNGjLDex8XFmZCQENOxY0fz3HPPmaysLDNhwgQjySxZssSK27t3r5FkFi1aVO22ffrpp+buu+82GRkZZv369ebNN980Y8aMMc2aNTPr1q2z4r777jvTpUsX43A4TFpamnn77bfN5MmTTdu2bcvt+0LWrVtnJJl27dqZ0aNHm8zMTPPcc8+Z5s2bm759+5oBAwaYadOmmdWrV5snn3zSeHl5mUmTJrn8Hw0cONB4e3ubBx980Kxevdr88Y9/NIGBgaZHjx7m+++/t2LbtWtn2rRpY7p27Wpeeukl8/bbb5ubbrrJSDLZ2dnGGGPy8/PNrFmzjCTzzDPPmE2bNplNmzaZ/Pz8Wv8/9OjRw1xzzTXlxuDqq682V199dZXHDEDD2L9/v5k0aZKRZFasWGEdHwoLC824ceOMJDNx4kTrOHbppZeaiIgIc/jwYWOMMbt27TLXXHONcTqd1mc3bdpkbX/06NFm4cKFJisry2RlZZk//OEPxt/f3zzyyCMu7YiLizNxcXHVanvZsatTp05m4cKF5u233zaJiYlGknnkkUdMdHS0efnll81bb71levfubex2u/n666+tz1c1jyxatMhIMh06dDCTJk0yb7/9tnnxxRdNy5YtTd++fa24Dz74wHTo0MH06NHDGocPPvjAGPO/vNC+fXtz2223mVWrVpmXX37ZtG3b1nTs2NGcOXPG2s6oUaNMu3btrPevvfaakWSysrJc+r9q1SojyaxatapK4/XBBx+YMWPGGEkmMzPTbNq0yezfv98YY8zjjz9ubDabufPOO82bb75pVqxYYWJjY01gYKDZtWuXtY21a9caX19f8/Of/9y88sorJjMz04wePbraeRKAe2nKueTFF180NpvNxMfHm+XLl5s1a9aYZ5991kyYMMGKGTVqlPH19TVdunQxf/zjH82aNWvMQw89ZGw2W7k2VLWt7dq1M61btzYdOnQwf/3rX826devM1q1bzcGDB01ISIhp27atWbx4sXnrrbdMUlKSdX744/OXpUuXGpvNZoYNG2ZWrFhh3njjDZOYmGi8vLzMmjVrrLiBAweaSy+91Lzwwgtm/fr15tVXXzUPPfSQS667kF27dpmAgADrXPS1114zAwcOtM6VfnwOfO455Y/7PGrUKOt9Vc/Tys5BLrvsMtO3b1/zj3/8w6xevdp89NFHVo5+4IEHrJ+bstxW1ZxVto3LLrvMjBs3zvzrX/8y//jHP1xyM6qPYlkTdObMGVNcXGw6duxofvvb31rLX3/9dSPJrF692iU2PDzc3Hjjjdaym266yQQGBloHd2OMKSkpMV27dq12sezqq682ERERpqioyFp2/PhxExISUmGxzN/f3+Tl5bm0r3PnzuYnP/lJlff5r3/9y0gyf/7zn12WP/744+UObL179zahoaHm+PHjLvuMiooybdq0MWfPnjXGGPPAAw8Yf39/q4jzm9/8xgwaNMh069bNSgxff/21kWReeOEFa1txcXFGktmyZYtLW7p27WoGDhxova+oSFPVtp3rzJkz5vTp06Zfv37ml7/8pbV8/vz5RpJ57bXXXOLHjh1b42LZDTfc4LI8OTnZSDKTJ092WT5s2DATHBxsvc/MzDSSzOzZs13iXnnllXJj2K5dO+Pn52e+/PJLa9mpU6dMcHCwGT9+vLXs73//e7kEW6Y2/w9lyeXDDz+0lm3durVcoQ1A4/XUU0+Vy1+7d+82klxOFowxZsuWLUaSuf/++61lQ4YMcSnuVKakpMScPn3aPProoyYkJMTlOF3TYpkk8/7771vLjhw5Yry8vIy/v79LYSwnJ8dIMn/5y1+sZVXNI2XHuXPHYvbs2UaSyc3NtZZdeeWVFfajLC9cf/31Lsv/9re/GUkuJ4XnFstKSkpMhw4dzP/93/+5fHbw4MHm8ssvrzTfVeThhx82klz+hvnqq6+Mt7e3y5c2xvzw94jT6XT5kqtz586mR48e5vTp0y6xiYmJpnXr1uW+cATgOZpiLjl+/Lhp0aKFufbaa897LB01apSRZP72t7+5LL/++utNp06datTWdu3aGS8vL7Nnzx6Xz/zud78zNpvN5YsKY34oeP34b/mTJ0+a4ODgcucbJSUlpnv37uZnP/uZtax58+YmOTm50nZWxc0331zpuWhNi2Xnquw8rewc5PLLLzfFxcUun9m2bVul52pVzVllef6OO+44zwigurgMswk4c+aMZs2apa5du8rX11fe3t7y9fXVZ599pt27d1txgwcPltPptC7fkKS3335bBw8e1J133mkty87O1nXXXadWrVpZy5o1a6YRI0ZUq10nT57U+++/r2HDhsnX19daXna5XkX69eunsLAw672Xl5duvvlmff755y5Tmc9n3bp1kqTbbrvNZfnIkSPLtW/Lli361a9+5fL0Fy8vLyUlJenAgQPWpSP9+vXTqVOntHHjRkk/TAceMGCA+vfvr6ysLGuZJPXv399lP06nUz/72c9clnXr1k1ffvllpX2oTtsk6bnnntNVV10lPz8/eXt7y8fHR++8847L//+6desUFBSkoUOHnndcqiMxMdHlfZcuXSSp3CUzXbp00bfffmtdirl27VpJcpmmLEk33XSTAgMDy02t/ulPf6q2bdta7/38/HTFFVecdwzPVZP/B0m69dZbFRoaqmeeecZalpaWpksvvVQ333xzlfcPoHEpyxXnHod+9rOfqUuXLuWOQ5VZu3at+vfvL4fDIS8vL/n4+Oihhx7SkSNHlJ+fX+t2tm7dWj179rTeBwcHKzQ0VD/96U8VHh5uLS87/pYd06qbRySVyw/dunVz2WZV1GQbzZo108SJE/Xmm2/qq6++kiT997//VWZmpiZMmFDr+6m8/fbbOnPmjO644w6dOXPGevn5+SkuLs665Ofzzz/Xp59+av398OPY66+/Xrm5uRe8pBSAZ2nsuWTjxo06duxYlY6lNput3DlaRX8rV6et3bp10xVXXOGyLDs7W1FRUeratavL8ltvvbVc27/99luNGjXK5Xh89uxZDRo0SNu2bdPJkycl/TDeixcv1mOPPabNmzfr9OnTFx6cc6xbt67Sc9HaqMp5WpmhQ4fKx8enStutSc668cYba9UXuKJY1gRMmTJFDz74oIYNG6Y33nhDW7Zs0bZt29S9e3edOnXKivP29lZSUpJWrlxp3Rdq8eLFat26tQYOHGjFHTlyxOUgUaaiZedTUFAgY0y1tuV0OitdduTIkSrt98iRI/L29lZISMh5t13WvtatW5fbRtkJSNk++/Tpo4CAAK1Zs0aff/659u3bZxXLtmzZohMnTmjNmjXq0KGDIiMjXbZ1bjskyW63u/zfnKs6bZszZ47uvvtuxcTE6J///Kc2b96sbdu2adCgQS77qOz/taIxr6rg4GCX92VF0cqWf//991ZbvL29demll7rE2Ww2OZ3Ocv/XNRnDc9V0G3a7XePHj9fy5ct19OhRHT58WH/729/0m9/8Rna7vcr7B9C4lB1nKjvOViXnbN26VQkJCZKkBQsW6N///re2bdummTNnSlK1jlGVOfd4Kv1wTL3QcbY6eaTMucfJsmNcbY61Vd3GnXfeKX9/fz333HOSpGeeeUb+/v4uX+bV1KFDhyRJV199tXx8fFxer7zyir755huXuGnTppWLmzBhgiRZsQAgNf5ccvjwYUlSmzZtLhgbEBAgPz8/l2V2u93KKzVpa0XjUtVzzbJj8q9+9atyx+Qnn3xSxhh9++23kn64J+WoUaP04osvKjY2VsHBwbrjjjuqdW/mI0eOnPdctCaqep5WpqLxqkxNclZ1to8L42mYTUB6erruuOMOzZo1y2X5N998o0suucRl2a9//Ws99dRTysjI0M0336zXX39dycnJ8vLysmJCQkKsX74fq+6N4Fu2bCmbzVatbVW0vGxZRcWOioSEhOjMmTM6cuSIy2fO3XbLli3VrFkz5ebmlttG2Q2Ny2bX+fr66tprr9WaNWvUpk0bOZ1ORUdHq0OHDpJ+uOH9O++8U26mVU1Vp23p6emKj4/X/PnzXeKOHz/u8j4kJERbt24tt72a3OC/tsr+jw4fPuxSMDPGKC8vz7qRZWNx991364knntBf//pXff/99zpz5ozuuuuuhm4WgFooyw+5ubnlTiIOHjzoMru6MhkZGfLx8dGbb77pcoLx6quv1mlba6I6eaQxcDgc1onOtGnTtGjRIo0cObLc3zE1UdbPf/zjH2rXrt0F42bMmKHhw4dXGNOpU6datweA+2jsuaTs7+yqXqFzIdVta0Wz2ap6rlk2dmlpaerdu3eF2y8rsLVq1Upz587V3Llz9dVXX+n111/Xfffdp/z8/AofflORkJCQ856L/pjdbq/w5vjnFkerep5WpjozqWuSs3jyZd1iZlkTYLPZys1wWbVqlb7++utysV26dFFMTIwWLVqk5cuXq6ioSL/+9a9dYuLi4rR27VqXSvTZs2f197//vVrtCgwMVK9evfTqq6+quLjYWn7ixAm9+eabFX7mnXfecTl4lpSU6JVXXtHll19epW9EpB+ehihJy5Ytc1m+fPnycu2LiYnRihUrXCr7Z8+eVXp6utq0aeMybbh///7avn27/vnPf1qXWgYGBqp3795KS0vTwYMHy12CWVPVaVtF//8ff/yxNm3a5LKsb9++On78uF5//XWX5eeOS33o16+fpB8SyI/985//1MmTJ6311VGTGRBV1bp1a91000169tln9dxzz+mGG25wuSwUQONW0fHhuuuuk1T+OLRt2zbt3r3b5ThU2SxUm80mb29vly+cTp06paVLl9Zp+2uiujmuqqo7q7c6Jk+erG+++Ua/+tWvdPToUU2cOLFOtjtw4EB5e3vrv//9r3r16lXhS/rhpKJjx4766KOPKo0LCgqqkzYBaHqaYi7p06ePHA6HnnvuuTp5smZdtDUuLk47d+7UJ5984rI8IyPD5f0111yjSy65RJ988kmlx+Qf3+qnTNu2bTVx4kQNGDBAH3zwQZXb1bdv30rPRc/Vvn17ffzxxy7L1q5da91ypkxVz9POp7JzHHJWw2NmWROQmJioxYsXq3PnzurWrZu2b9+up556qtLi0p133qnx48fr4MGD6tOnT7mK88yZM/XGG2+oX79+mjlzpnVZRNk14c2aVb2G+uijj2rIkCEaOHCg7r33XpWUlOipp55S8+bNrWmzP9aqVStdd911evDBBxUYGKhnn31Wn376abmD5/kkJCToF7/4haZPn66TJ0+qV69e+ve//13hQTw1NVUDBgxQ3759NW3aNPn6+urZZ5/Vzp079fLLL7tU3/v166eSkhK98847WrJkibW8f//+evjhh2Wz2ayEWReq2rbExET94Q9/0MMPP6y4uDjt2bNHjz76qCIjI3XmzBlre3fccYeefvpp3XHHHXr88cfVsWNHvfXWW3r77bfrrM1VNWDAAA0cOFC///3vdezYMV1zzTX6+OOP9fDDD6tHjx5KSkqq9jajoqIkSS+88IKCgoLk5+enyMjIKs9IvJB7771XMTExkuRy3z8AjV90dLQk6c9//rNGjRolHx8fderUSePGjVNaWpqaNWumwYMHa9++fXrwwQcVERGh3/72ty6fX7FihebPn6+ePXuqWbNm6tWrl4YMGaI5c+Zo5MiRGjdunI4cOaI//vGPjeYS7erkuKqKjo5WRkaGXnnlFXXo0EF+fn7W+NbWFVdcoUGDBulf//qXrr32WnXv3r1Ottu+fXs9+uijmjlzpr744gsNGjRILVu21KFDh7R161YFBgbqkUcekSQ9//zzGjx4sAYOHKjRo0frsssu07fffqvdu3frgw8+qPYXhwDcR1PMJc2bN9ef/vQn/eY3v1H//v01duxYhYWF6fPPP9dHH32kefPmVWt7ddHW5ORk/fWvf9XgwYP16KOPKiwsTMuXL9enn34q6X/nms2bN1daWppGjRqlb7/9Vr/61a8UGhqqw4cP66OPPtLhw4c1f/58FRYWqm/fvho5cqQ6d+6soKAgbdu2TZmZmZXOuKrIAw88oNdff13XXXedHnroIQUEBOiZZ56xzoF/LCkpSQ8++KAeeughxcXF6ZNPPtG8efPkcDhc4qp6nnY+l19+ufz9/bVs2TJ16dJFzZs3V3h4uMLDw8lZDa0BHy6AKiooKDBjxowxoaGhJiAgwFx77bXmvffeq/RpKYWFhcbf399IMgsWLKhwm++9956JiYkxdrvdOJ1O87vf/c48+eSTRpI5evRotdq3cuVKEx0dbXx9fU3btm3NE088YSZPnmxatmzpEifJ3HPPPebZZ581l19+ufHx8TGdO3c2y5Ytq9b+jDHm6NGj5s477zSXXHKJCQgIMAMGDDCffvpphU8uee+998x1111nAgMDjb+/v+ndu7d54403ym3z7NmzplWrVkaSyxPI/v3vfxtJ5qqrrir3mbi4OHPllVeWW37uk8AqegpjVdtWVFRkpk2bZi677DLj5+dnrrrqKvPqq6+W24cxxhw4cMDceOONpnnz5iYoKMjceOONZuPGjTV+Gubf//53l+VlT1rZtm2by/KKnk526tQp8/vf/960a9fO+Pj4mNatW5u7777bFBQUuHy2Xbt2ZsiQIeXaUNHP99y5c01kZKTx8vJy6VNt/x/KtG/f3nTp0qXCdQAatxkzZpjw8HDTrFkz62lbJSUl5sknnzRXXHGF8fHxMa1atTK333679Uj2Mt9++6351a9+ZS655BJjs9lcnub817/+1XTq1MnY7XbToUMHk5qaahYuXFjuyVk1fRpmRceuyo6LZXn0x6qSRyo7dpcd63/8lOF9+/aZhIQEExQUZCRZx9DK8kJFx9WK8lOZxYsXG0kmIyOjwvUXUlG+KfPqq6+avn37mhYtWhi73W7atWtnfvWrX5k1a9a4xH300UdmxIgRJjQ01Pj4+Bin02muu+4689xzz9WoTQDcR1PMJcYY89Zbb5m4uDgTGBhoAgICTNeuXc2TTz5prR81apQJDAws97myY+qPVbWtleUqY4zZuXOn6d+/v/Hz8zPBwcFmzJgxZsmSJUaS+eijj1xis7OzzZAhQ0xwcLDx8fExl112mRkyZIiVb77//ntz1113mW7dupkWLVoYf39/06lTJ/Pwww+bkydPVmuc/v3vf5vevXu7nAO/8MIL5fpWVFRkpk+fbiIiIoy/v7+Ji4szOTk55Z6GWdXztLJc+dRTT1XYrpdfftl07tzZ+Pj4lDufrUrOqizPo3ZsxtTBfE24hYSEBO3bt0//+c9/arWd06dP66c//akuu+wyrV69uo5aB1xcH3/8sbp3765nnnnGumkmAKBu3Xjjjdq8ebP27dtX5SeCAQCavnHjxunll1/WkSNHKry8sqEsXrxYv/71r7V37161b9++oZuDRoTLMD3UlClT1KNHD0VEROjbb7/VsmXLlJWVpYULF1Z7W2PGjNGAAQPUunVr5eXl6bnnntPu3bv15z//+SK0HKhb//3vf/Xll1/q/vvvV+vWrcs9GhwAUDtFRUX64IMPtHXrVq1cuVJz5syhUAYAbuzRRx9VeHi4OnToYN3P+sUXX9QDDzzQqAplwPlQLPNQJSUleuihh5SXlyebzaauXbtq6dKluv322yX9cIPgs2fPnncb3t4//PgcP35c06ZN0+HDh+Xj46OrrrpKb731Vo1uhn+ha7ubNWtWrXuqwZUxRiUlJeeN8fLy8qgnqfzhD3/Q0qVL1aVLF/39739XQEBAQzcJQBNXUlJy3hst22w2l5snu7vc3Fz16dNHLVq00Pjx4zVp0qRyMYwZALhqysdFHx8fPfXUUzpw4IDOnDmjjh07as6cObr33nvrfF+c3+Bi4TJMVCglJcW6GW5lLsZU1QsdxEaNGqXFixfX6T49yfr1662niVZm0aJFzK4CgFqIj49XdnZ2pevbtWunffv21V+DmgDGDABccVysmrLLKM9n3bp1io+Pr58GwW1QLEOFDh48qIMHD543plu3bnU+jfb9998/7/pWrVpxLXktHD9+XHv27DlvTF0+YRIAPNGePXt0/PjxStfb7fY6e8Kku2DMAMAVx8WqOXLkiPbu3XvemE6dOikoKKieWgR3QbEMAAAAAAAAKMXNnwAAAAAAAIBSbnuD/7Nnz+rgwYMKCgriZn4APJYxRsePH1d4eDgPx2gg5CMAIB81BuQjAKh6PnLbYtnBgwcVERHR0M0AgEZh//79atOmTUM3wyORjwDgf8hHDYd8BAD/c6F85LbFsrIb+O3fv18tWrRo4NYAQMM4duyYIiIiuKlpAyIfAQD5qDEgHwFA1fOR2xbLyqYWt2jRgmQAwONxuUXDIR8BwP+QjxoO+QgA/udC+YgbBgAAAAAAAAClKJYBAAAAAAAApSiWAQAAAAAAAKUolgEAAAAAAACl3PYG/7XV/r5V9b7PfU8Mqfd9AgAaN/IRAKChNUQukshHABoOM8sAAAAAAACAUhTLAAAAAAAAgFIUywAAAAAAAIBSFMsAAAAAAACAUhTLAAAAAAAAgFIUywAATc67776rG264QeHh4bLZbHr11Vdd1o8ePVo2m83l1bt3b5eYoqIiTZo0Sa1atVJgYKCGDh2qAwcOuMQUFBQoKSlJDodDDodDSUlJOnr06EXuHQAAAICGRLEMANDknDx5Ut27d9e8efMqjRk0aJByc3Ot11tvveWyPjk5WStXrlRGRoY2bNigEydOKDExUSUlJVbMyJEjlZOTo8zMTGVmZionJ0dJSUkXrV8AAAAAGp53QzcAAIDqGjx4sAYPHnzeGLvdLqfTWeG6wsJCLVy4UEuXLlX//v0lSenp6YqIiNCaNWs0cOBA7d69W5mZmdq8ebNiYmIkSQsWLFBsbKz27NmjTp061W2nAAAAADQKzCwDALil9evXKzQ0VFdccYXGjh2r/Px8a9327dt1+vRpJSQkWMvCw8MVFRWljRs3SpI2bdokh8NhFcokqXfv3nI4HFZMRYqKinTs2DGXFwAAAICmg2IZAMDtDB48WMuWLdPatWv1pz/9Sdu2bdN1112noqIiSVJeXp58fX3VsmVLl8+FhYUpLy/PigkNDS237dDQUCumIqmpqdY9zhwOhyIiIuqwZwAAAAAuNi7DBAC4nZtvvtn6d1RUlHr16qV27dpp1apVGj58eKWfM8bIZrNZ73/878pizjVjxgxNmTLFen/s2DEKZgAAAEATwswyAIDba926tdq1a6fPPvtMkuR0OlVcXKyCggKXuPz8fIWFhVkxhw4dKretw4cPWzEVsdvtatGihcsLAAAAQNNBsQwA4PaOHDmi/fv3q3Xr1pKknj17ysfHR1lZWVZMbm6udu7cqT59+kiSYmNjVVhYqK1bt1oxW7ZsUWFhoRUDAAAAwP1wGSYAoMk5ceKEPv/8c+v93r17lZOTo+DgYAUHByslJUU33nijWrdurX379un+++9Xq1at9Mtf/lKS5HA4NGbMGE2dOlUhISEKDg7WtGnTFB0dbT0ds0uXLho0aJDGjh2r559/XpI0btw4JSYm8iRMAAAAwI1RLAMANDnvv/+++vbta70vu0fYqFGjNH/+fO3YsUMvvfSSjh49qtatW6tv37565ZVXFBQUZH3m6aeflre3t0aMGKFTp06pX79+Wrx4sby8vKyYZcuWafLkydZTM4cOHap58+bVUy8BAAAANASKZQCAJic+Pl7GmErXv/322xfchp+fn9LS0pSWllZpTHBwsNLT02vURgAAAABNE/csAwAAAAAAAEpRLAMAAAAAAABKUSwDAAAAAAAASlWrWJaamqqrr75aQUFBCg0N1bBhw7Rnzx6XGGOMUlJSFB4eLn9/f8XHx2vXrl0uMUVFRZo0aZJatWqlwMBADR06VAcOHHCJKSgoUFJSkhwOhxwOh5KSknT06NGa9RIAAAAAAACogmoVy7Kzs3XPPfdo8+bNysrK0pkzZ5SQkKCTJ09aMbNnz9acOXM0b948bdu2TU6nUwMGDNDx48etmOTkZK1cuVIZGRnasGGDTpw4ocTERJWUlFgxI0eOVE5OjjIzM5WZmamcnBwlJSXVQZcBAAAAAACAilXraZiZmZku7xctWqTQ0FBt375dv/jFL2SM0dy5czVz5kwNHz5ckrRkyRKFhYVp+fLlGj9+vAoLC7Vw4UItXbpU/fv3lySlp6crIiJCa9as0cCBA7V7925lZmZq8+bNiomJkSQtWLBAsbGx2rNnjzp16lSubUVFRSoqKrLeHzt2rHojAQAAAAAAAI9Xq3uWFRYWSpKCg4MlSXv37lVeXp4SEhKsGLvdrri4OG3cuFGStH37dp0+fdolJjw8XFFRUVbMpk2b5HA4rEKZJPXu3VsOh8OKOVdqaqp1yabD4VBERERtugYAAAAAAAAPVONimTFGU6ZM0bXXXquoqChJUl5eniQpLCzMJTYsLMxal5eXJ19fX7Vs2fK8MaGhoeX2GRoaasWca8aMGSosLLRe+/fvr2nXAAAAAAAA4KGqdRnmj02cOFEff/yxNmzYUG6dzWZzeW+MKbfsXOfGVBR/vu3Y7XbZ7faqNB0AAAAAAACoUI1mlk2aNEmvv/661q1bpzZt2ljLnU6nJJWb/ZWfn2/NNnM6nSouLlZBQcF5Yw4dOlRuv4cPHy43aw0AAAAAAACoK9UqlhljNHHiRK1YsUJr165VZGSky/rIyEg5nU5lZWVZy4qLi5Wdna0+ffpIknr27CkfHx+XmNzcXO3cudOKiY2NVWFhobZu3WrFbNmyRYWFhVYMAAAAAAAAUNeqdRnmPffco+XLl+u1115TUFCQNYPM4XDI399fNptNycnJmjVrljp27KiOHTtq1qxZCggI0MiRI63YMWPGaOrUqQoJCVFwcLCmTZum6Oho6+mYXbp00aBBgzR27Fg9//zzkqRx48YpMTGxwidhAgAAAAAAAHWhWsWy+fPnS5Li4+Ndli9atEijR4+WJE2fPl2nTp3ShAkTVFBQoJiYGK1evVpBQUFW/NNPPy1vb2+NGDFCp06dUr9+/bR48WJ5eXlZMcuWLdPkyZOtp2YOHTpU8+bNq0kfAQAAAAAAgCqpVrHMGHPBGJvNppSUFKWkpFQa4+fnp7S0NKWlpVUaExwcrPT09Oo0DwAAAAAAAKiVGt3gHwAAAAAAAHBHFMsAAAAAAACAUhTLAAAAAAAAgFIUywAAAAAAAIBSFMsAAAAAAACAUhTLAAAAAKAGUlNTdfXVVysoKEihoaEaNmyY9uzZ4xJjjFFKSorCw8Pl7++v+Ph47dq1yyWmqKhIkyZNUqtWrRQYGKihQ4fqwIEDLjEFBQVKSkqSw+GQw+FQUlKSjh49erG7CAAeiWIZAAAAANRAdna27rnnHm3evFlZWVk6c+aMEhISdPLkSStm9uzZmjNnjubNm6dt27bJ6XRqwIABOn78uBWTnJyslStXKiMjQxs2bNCJEyeUmJiokpISK2bkyJHKyclRZmamMjMzlZOTo6SkpHrtLwB4Cu+GbgAAAAAANEWZmZku7xctWqTQ0FBt375dv/jFL2SM0dy5czVz5kwNHz5ckrRkyRKFhYVp+fLlGj9+vAoLC7Vw4UItXbpU/fv3lySlp6crIiJCa9as0cCBA7V7925lZmZq8+bNiomJkSQtWLBAsbGx2rNnjzp16lS/HQcAN8fMMgAAAACoA4WFhZKk4OBgSdLevXuVl5enhIQEK8ZutysuLk4bN26UJG3fvl2nT592iQkPD1dUVJQVs2nTJjkcDqtQJkm9e/eWw+GwYs5VVFSkY8eOubwAAFVDsQwAAAAAaskYoylTpujaa69VVFSUJCkvL0+SFBYW5hIbFhZmrcvLy5Ovr69atmx53pjQ0NBy+wwNDbVizpWammrd38zhcCgiIqJ2HQQAD0KxDAAAAABqaeLEifr444/18ssvl1tns9lc3htjyi0717kxFcWfbzszZsxQYWGh9dq/f39VugEAEMUyAAAAAKiVSZMm6fXXX9e6devUpk0ba7nT6ZSkcrO/8vPzrdlmTqdTxcXFKigoOG/MoUOHyu338OHD5WatlbHb7WrRooXLCwBQNRTLAAAAAKAGjDGaOHGiVqxYobVr1yoyMtJlfWRkpJxOp7KysqxlxcXFys7OVp8+fSRJPXv2lI+Pj0tMbm6udu7cacXExsaqsLBQW7dutWK2bNmiwsJCKwYAUHd4GiYAAAAA1MA999yj5cuX67XXXlNQUJA1g8zhcMjf3182m03JycmaNWuWOnbsqI4dO2rWrFkKCAjQyJEjrdgxY8Zo6tSpCgkJUXBwsKZNm6bo6Gjr6ZhdunTRoEGDNHbsWD3//POSpHHjxikxMZEnYQLARUCxDAAAAABqYP78+ZKk+Ph4l+WLFi3S6NGjJUnTp0/XqVOnNGHCBBUUFCgmJkarV69WUFCQFf/000/L29tbI0aM0KlTp9SvXz8tXrxYXl5eVsyyZcs0efJk66mZQ4cO1bx58y5uBwHAQ1EsAwAAAIAaMMZcMMZmsyklJUUpKSmVxvj5+SktLU1paWmVxgQHBys9Pb0mzQQAVBP3LAMAAAAAAABKUSwDAAAAAAAASlEsAwAAAAAAAEpRLAMAAAAAAABKVbtY9u677+qGG25QeHi4bDabXn31VZf1o0ePls1mc3n17t3bJaaoqEiTJk1Sq1atFBgYqKFDh+rAgQMuMQUFBUpKSpLD4ZDD4VBSUpKOHj1a7Q4CAAAAAAAAVVXtYtnJkyfVvXv38z6meNCgQcrNzbVeb731lsv65ORkrVy5UhkZGdqwYYNOnDihxMRElZSUWDEjR45UTk6OMjMzlZmZqZycHCUlJVW3uQAAAAAAAECVVbtYNnjwYD322GMaPnx4pTF2u11Op9N6BQcHW+sKCwu1cOFC/elPf1L//v3Vo0cPpaena8eOHVqzZo0kaffu3crMzNSLL76o2NhYxcbGasGCBXrzzTe1Z8+eGnQTAOBOLjTL2RijlJQUhYeHy9/fX/Hx8dq1a5dLDLOcAQAAAFTkotyzbP369QoNDdUVV1yhsWPHKj8/31q3fft2nT59WgkJCday8PBwRUVFaePGjZKkTZs2yeFwKCYmxorp3bu3HA6HFXOuoqIiHTt2zOUFAHBPF5rlPHv2bM2ZM0fz5s3Ttm3b5HQ6NWDAAB0/ftyKYZYzAAAAgIp41/UGBw8erJtuuknt2rXT3r179eCDD+q6667T9u3bZbfblZeXJ19fX7Vs2dLlc2FhYcrLy5Mk5eXlKTQ0tNy2Q0NDrZhzpaam6pFHHqnr7gAAGqHBgwdr8ODBFa4zxmju3LmaOXOmNQt6yZIlCgsL0/LlyzV+/HhrlvPSpUvVv39/SVJ6eroiIiK0Zs0aDRw40JrlvHnzZuvLmwULFig2NlZ79uxRp06d6qezAAAAAOpVnRfLbr75ZuvfUVFR6tWrl9q1a6dVq1ad99JNY4xsNpv1/sf/rizmx2bMmKEpU6ZY748dO6aIiIiadMHjtL9vVb3vc98TQ+p9nwA8w969e5WXl+cyg9lutysuLk4bN27U+PHjLzjLeeDAgRec5VxZsayoqEhFRUXWe2Y6AwAAAE3LRbkM88dat26tdu3a6bPPPpMkOZ1OFRcXq6CgwCUuPz9fYWFhVsyhQ4fKbevw4cNWzLnsdrtatGjh8gIAeJ6yGcjn5otzZzBfjFnO0g8zncvuceZwOPjiBgAAAGhiLnqx7MiRI9q/f79at24tSerZs6d8fHyUlZVlxeTm5mrnzp3q06ePJCk2NlaFhYXaunWrFbNlyxYVFhZaMQAAnM+5M5HPNzu5spjqznKWfpjpXFhYaL32799fzZYDAAAAaEjVvgzzxIkT+vzzz633e/fuVU5OjoKDgxUcHKyUlBTdeOONat26tfbt26f7779frVq10i9/+UtJksPh0JgxYzR16lSFhIQoODhY06ZNU3R0tHXfmC5dumjQoEEaO3asnn/+eUnSuHHjlJiYyD1iAADn5XQ6Jf0wM6zsixqp/AzmslnOP55dlp+fb30pU5NZztIPM53tdnud9AUAAABA/av2zLL3339fPXr0UI8ePSRJU6ZMUY8ePfTQQw/Jy8tLO3bs0P/93//piiuu0KhRo3TFFVdo06ZNCgoKsrbx9NNPa9iwYRoxYoSuueYaBQQE6I033pCXl5cVs2zZMkVHRyshIUEJCQnq1q2bli5dWgddBgC4s8jISDmdTpcZzMXFxcrOzrYKYcxyBgAAAFCZas8si4+PlzGm0vVvv/32Bbfh5+entLQ0paWlVRoTHBys9PT06jYPAOABzjfLuW3btkpOTtasWbPUsWNHdezYUbNmzVJAQIBGjhwpiVnOQH3hIUIAAKApqvOnYQIAcLG9//776tu3r/W+7GnIo0aN0uLFizV9+nSdOnVKEyZMUEFBgWJiYrR69epys5y9vb01YsQInTp1Sv369dPixYvLzXKePHmy9dTMoUOHat68efXUSwAAAAANgWIZAKDJudAsZ5vNppSUFKWkpFQawyxnAAAAABW56E/DBAAAAAAAAJoKimUAAAAAAABAKYplAAAAAAAAQCnuWQYAAODmGuKplAAAAE0VM8sAAAAAAACAUswsAwAAgNtoqFl0+54Y0iD7BQAAdY+ZZQAAAAAAAEApimUAAAAAAABAKYplAAAAAAAAQCmKZQAAAAAAAEApimUAAAAAAABAKYplAAAAAAAAQCmKZQAAAAAAAEApimUAAAAAAABAKYplAAAAAAAAQCnvhm4A/qf9fasaugkAAACogYb4O27fE0PqfZ8AAHgCZpYBAAAAAAAApSiWAQAAAAAAAKW4DBMAAAAA0OhweTOAhlLtmWXvvvuubrjhBoWHh8tms+nVV191WW+MUUpKisLDw+Xv76/4+Hjt2rXLJaaoqEiTJk1Sq1atFBgYqKFDh+rAgQMuMQUFBUpKSpLD4ZDD4VBSUpKOHj1a7Q4CAAAAAAAAVVXtYtnJkyfVvXt3zZs3r8L1s2fP1pw5czRv3jxt27ZNTqdTAwYM0PHjx62Y5ORkrVy5UhkZGdqwYYNOnDihxMRElZSUWDEjR45UTk6OMjMzlZmZqZycHCUlJdWgiwAAAAAAAEDVVPsyzMGDB2vw4MEVrjPGaO7cuZo5c6aGDx8uSVqyZInCwsK0fPlyjR8/XoWFhVq4cKGWLl2q/v37S5LS09MVERGhNWvWaODAgdq9e7cyMzO1efNmxcTESJIWLFig2NhY7dmzR506dappfwEAAAAAAIBK1ekN/vfu3au8vDwlJCRYy+x2u+Li4rRx40ZJ0vbt23X69GmXmPDwcEVFRVkxmzZtksPhsAplktS7d285HA4r5lxFRUU6duyYywsAAAAAAACojjotluXl5UmSwsLCXJaHhYVZ6/Ly8uTr66uWLVueNyY0NLTc9kNDQ62Yc6Wmplr3N3M4HIqIiKh1fwAAAAAAAOBZ6rRYVsZms7m8N8aUW3auc2Mqij/fdmbMmKHCwkLrtX///hq0HAAAAAAAAJ6sTotlTqdTksrN/srPz7dmmzmdThUXF6ugoOC8MYcOHSq3/cOHD5ebtVbGbrerRYsWLi8AAAAAAACgOqp9g//ziYyMlNPpVFZWlnr06CFJKi4uVnZ2tp588klJUs+ePeXj46OsrCyNGDFCkpSbm6udO3dq9uzZkqTY2FgVFhZq69at+tnPfiZJ2rJliwoLC9WnT5+6bDIAAPBg7e9bVe/73PfEkHrfJwAAAKqu2jPLTpw4oZycHOXk5Ej64ab+OTk5+uqrr2Sz2ZScnKxZs2Zp5cqV2rlzp0aPHq2AgACNHDlSkuRwODRmzBhNnTpV77zzjj788EPdfvvtio6Otp6O2aVLFw0aNEhjx47V5s2btXnzZo0dO1aJiYk8CRMAAABAo/Duu+/qhhtuUHh4uGw2m1599VWX9cYYpaSkKDw8XP7+/oqPj9euXbtcYoqKijRp0iS1atVKgYGBGjp0qA4cOOASU1BQoKSkJOv+zElJSTp69OhF7h0AeK5qF8vef/999ejRw5o5NmXKFPXo0UMPPfSQJGn69OlKTk7WhAkT1KtXL3399ddavXq1goKCrG08/fTTGjZsmEaMGKFrrrlGAQEBeuONN+Tl5WXFLFu2TNHR0UpISFBCQoK6deumpUuX1ra/AAAAAFAnTp48qe7du2vevHkVrp89e7bmzJmjefPmadu2bXI6nRowYICOHz9uxSQnJ2vlypXKyMjQhg0bdOLECSUmJqqkpMSKGTlypHJycpSZmanMzEzl5OQoKSnpovcPADxVtS/DjI+PlzGm0vU2m00pKSlKSUmpNMbPz09paWlKS0urNCY4OFjp6enVbR4AAAAA1IvBgwdr8ODBFa4zxmju3LmaOXOmhg8fLklasmSJwsLCtHz5co0fP16FhYVauHChli5dal1lk56eroiICK1Zs0YDBw7U7t27lZmZqc2bNysmJkaStGDBAsXGxmrPnj1ceQMAF8FFeRomAAAAAHiyvXv3Ki8vTwkJCdYyu92uuLg4bdy4UZK0fft2nT592iUmPDxcUVFRVsymTZvkcDisQpkk9e7dWw6Hw4qpSFFRkY4dO+byAgBUTZ3e4B+oKm6oDAAAAHeWl5cnSQoLC3NZHhYWpi+//NKK8fX1VcuWLcvFlH0+Ly9PoaGh5bYfGhpqxVQkNTVVjzzySK36AACeipllAAAAAHCR2Gw2l/fGmHLLznVuTEXxF9rOjBkzVFhYaL32799fzZYDgOeiWAYAAAAAdczpdEpSudlf+fn51mwzp9Op4uJiFRQUnDfm0KFD5bZ/+PDhcrPWfsxut6tFixYuLwBA1VAsAwAAAIA6FhkZKafTqaysLGtZcXGxsrOz1adPH0lSz5495ePj4xKTm5urnTt3WjGxsbEqLCzU1q1brZgtW7aosLDQigEA1C3uWQYAAAAANXDixAl9/vnn1vu9e/cqJydHwcHBatu2rZKTkzVr1ix17NhRHTt21KxZsxQQEKCRI0dKkhwOh8aMGaOpU6cqJCREwcHBmjZtmqKjo62nY3bp0kWDBg3S2LFj9fzzz0uSxo0bp8TERJ6ECQAXCTPLAABuKSUlRTabzeVVdkmM9MO9XlJSUhQeHi5/f3/Fx8dr165dLtsoKirSpEmT1KpVKwUGBmro0KE6cOBAfXcFANBIvf/+++rRo4d69OghSZoyZYp69Oihhx56SJI0ffp0JScna8KECerVq5e+/vprrV69WkFBQdY2nn76aQ0bNkwjRozQNddco4CAAL3xxhvy8vKyYpYtW6bo6GglJCQoISFB3bp109KlS+u3swDgQZhZBgBwW1deeaXWrFljvf/xicfs2bM1Z84cLV68WFdccYUee+wxDRgwQHv27LFOYpKTk/XGG28oIyNDISEhmjp1qhITE7V9+3aXbQEAPFN8fLyMMZWut9lsSklJUUpKSqUxfn5+SktLU1paWqUxwcHBSk9Pr01TAQDVQLEMAOC2vL29XWaTlTHGaO7cuZo5c6aGDx8uSVqyZInCwsK0fPlyjR8/XoWFhVq4cKGWLl1qXQqTnp6uiIgIrVmzRgMHDqxwn0VFRSoqKrLeHzt27CL0DAAAAMDFwmWYAAC39dlnnyk8PFyRkZG65ZZb9MUXX0j64Z4yeXl5SkhIsGLtdrvi4uK0ceNGSdL27dt1+vRpl5jw8HBFRUVZMRVJTU2Vw+GwXhERERepdwAAAAAuBoplAAC3FBMTo5deeklvv/22FixYoLy8PPXp00dHjhxRXl6eJCksLMzlM2FhYda6vLw8+fr6qmXLlpXGVGTGjBkqLCy0Xvv376/jngEAAAC4mLgMEwDglgYPHmz9Ozo6WrGxsbr88su1ZMkS9e7dW9IP95L5MWNMuWXnulCM3W6X3W6vRcsBAAAANCSKZQAAjxAYGKjo6Gh99tlnGjZsmKQfZo+1bt3aisnPz7dmmzmdThUXF6ugoMBldll+fr769OlTr22He2l/36qGbgIAAADOg8swAQAeoaioSLt371br1q0VGRkpp9OprKwsa31xcbGys7OtQljPnj3l4+PjEpObm6udO3dSLAMAAADcGDPLAABuadq0abrhhhvUtm1b5efn67HHHtOxY8c0atQo2Ww2JScna9asWerYsaM6duyoWbNmKSAgQCNHjpQkORwOjRkzRlOnTlVISIiCg4M1bdo0RUdHW0/HBAAAAOB+KJYBANzSgQMHdOutt+qbb77RpZdeqt69e2vz5s1q166dJGn69Ok6deqUJkyYoIKCAsXExGj16tUKCgqytvH000/L29tbI0aM0KlTp9SvXz8tXrxYXl5eDdUtAAAAABcZxTIAgFvKyMg473qbzaaUlBSlpKRUGuPn56e0tDSlpaXVcesAAAAANFbcswwAAAAAAAAoRbEMAAAAAAAAKMVlmIAban/fqnrf574nhtT7PgEAAIC61BB/R0v8LQ00NswsAwAAAAAAAErVebEsJSVFNpvN5eV0Oq31xhilpKQoPDxc/v7+io+P165du1y2UVRUpEmTJqlVq1YKDAzU0KFDdeDAgbpuKgAAAAAAAODiolyGeeWVV2rNmjXWey8vL+vfs2fP1pw5c7R48WJdccUVeuyxxzRgwADt2bNHQUFBkqTk5GS98cYbysjIUEhIiKZOnarExERt377dZVsAAMA9NNRlLwAAAMC5LkqxzNvb22U2WRljjObOnauZM2dq+PDhkqQlS5YoLCxMy5cv1/jx41VYWKiFCxdq6dKl6t+/vyQpPT1dERERWrNmjQYOHFjhPouKilRUVGS9P3bs2EXoGQAAAAAAANzZRbln2Weffabw8HBFRkbqlltu0RdffCFJ2rt3r/Ly8pSQkGDF2u12xcXFaePGjZKk7du36/Tp0y4x4eHhioqKsmIqkpqaKofDYb0iIiIuRtcAAAAAAADgxuq8WBYTE6OXXnpJb7/9thYsWKC8vDz16dNHR44cUV5eniQpLCzM5TNhYWHWury8PPn6+qply5aVxlRkxowZKiwstF779++v454BAAAAAADA3dX5ZZiDBw+2/h0dHa3Y2FhdfvnlWrJkiXr37i1JstlsLp8xxpRbdq4Lxdjtdtnt9lq0HAAAAAAAAJ7uolyG+WOBgYGKjo7WZ599Zt3H7NwZYvn5+dZsM6fTqeLiYhUUFFQaAwAAAAAAAFwMF+UG/z9WVFSk3bt36+c//7kiIyPldDqVlZWlHj16SJKKi4uVnZ2tJ598UpLUs2dP+fj4KCsrSyNGjJAk5ebmaufOnZo9e/bFbi4AAAAAAPWqIZ4Kve+JIfW+T6CpqPNi2bRp03TDDTeobdu2ys/P12OPPaZjx45p1KhRstlsSk5O1qxZs9SxY0d17NhRs2bNUkBAgEaOHClJcjgcGjNmjKZOnaqQkBAFBwdr2rRpio6Otp6OCQAAAHg6Tq4BALg46rxYduDAAd1666365ptvdOmll6p3797avHmz2rVrJ0maPn26Tp06pQkTJqigoEAxMTFavXq1goKCrG08/fTT8vb21ogRI3Tq1Cn169dPixcvlpeXV103FwAAAAAAALDUebEsIyPjvOttNptSUlKUkpJSaYyfn5/S0tKUlpZWx60DAAAAAADMTgUqd9Fv8A8AAAAAAAA0FRf9Bv8AAAAAAAANMZtNYkYbqo9iGQAAAAAAcFtccorq4jJMAAAAAAAAoBTFMgAAAAAAAKAUxTIAAAAAAACgFPcsAwAALhrq5rsAAABAY0CxDECTxs06AQAAADQ2nKc0bVyGCQAAAAAAAJSiWAYAAAAAAACUolgGAAAAAAAAlKJYBgAAAAAAAJTiBv8AAAAAAABNXEM90dwdHyzAzDIAAAAAAACgFMUyAAAAAAAAoBSXYQIXWUNNhQUAAAAAANXHzDIAAAAAAACgFDPL4DGY4QUAAAAAQN1qiHPti/1QAYplAFBN7pgMAAAAAAA/4DJMAAAAAAAAoFSjL5Y9++yzioyMlJ+fn3r27Kn33nuvoZsEAPAw5CIAQGNAPgKA+tGoi2WvvPKKkpOTNXPmTH344Yf6+c9/rsGDB+urr75q6KYBADwEuQgA0BiQjwCg/jTqYtmcOXM0ZswY/eY3v1GXLl00d+5cRUREaP78+Q3dNACAhyAXAQAaA/IRANSfRnuD/+LiYm3fvl333Xefy/KEhARt3LixXHxRUZGKioqs94WFhZKkY8eO1Wj/Z4u+q9HnAE9V09+12vKU39Wajm/Z54wxddkcj1HdXCSRjwC4N/JRw2jofEQuAtDYXOx81GiLZd98841KSkoUFhbmsjwsLEx5eXnl4lNTU/XII4+UWx4REXHR2gjgfxxzG7oF7q2243v8+HE5HI46aYsnqW4ukshHANwb+ahhkI8AwNXFzkeNtlhWxmazubw3xpRbJkkzZszQlClTrPdnz57Vt99+q5CQkArjz+fYsWOKiIjQ/v371aJFi5o13M0wJuUxJuUxJuU19JgYY3T8+HGFh4fX+77dSVVzkVS3+agiDf0zVV88pZ+S5/SVfrqf6vSVfFQ3GiIfedLPdFUwHv/DWLhiPFw11vGoaj5qtMWyVq1aycvLq9w3Jfn5+eW+UZEku90uu93usuySSy6pVRtatGjRqP5TGwPGpDzGpDzGpLyGHBO+wa+56uYi6eLko4p4yu+Zp/RT8py+0k/3U9W+ko9qrjHkI0/6ma4KxuN/GAtXjIerxjgeVclHjfYG/76+vurZs6eysrJclmdlZalPnz4N1CoAgCchFwEAGgPyEQDUr0Y7s0ySpkyZoqSkJPXq1UuxsbF64YUX9NVXX+muu+5q6KYBADwEuQgA0BiQjwCg/jTqYtnNN9+sI0eO6NFHH1Vubq6ioqL01ltvqV27dhd1v3a7XQ8//HC5acuejDEpjzEpjzEpjzFp+hoqF1XGU36mPKWfkuf0lX66H0/qa2PAuVHjwHj8D2PhivFw1dTHw2Z4fjMAAAAAAAAgqRHfswwAAAAAAACobxTLAAAAAAAAgFIUywAAAAAAAIBSFMsAAAAAAACAUhTLzvHss88qMjJSfn5+6tmzp957772GblK9SU1N1dVXX62goCCFhoZq2LBh2rNnj0uMMUYpKSkKDw+Xv7+/4uPjtWvXrgZqcf1LTU2VzWZTcnKytcwTx+Trr7/W7bffrpCQEAUEBOinP/2ptm/fbq33tDE5c+aMHnjgAUVGRsrf318dOnTQo48+qrNnz1oxnjYmqB1PPR67+zHWE46d7nw8fPfdd3XDDTcoPDxcNptNr776qsv6qvSrqKhIkyZNUqtWrRQYGKihQ4fqwIED9diLCztfP0+fPq3f//73io6OVmBgoMLDw3XHHXfo4MGDLttoCv1E1bjjuVFd5diq/JwXFBQoKSlJDodDDodDSUlJOnr06MXuYo3VNA+701jURa52l/Goq5zeZMfDwJKRkWF8fHzMggULzCeffGLuvfdeExgYaL788suGblq9GDhwoFm0aJHZuXOnycnJMUOGDDFt27Y1J06csGKeeOIJExQUZP75z3+aHTt2mJtvvtm0bt3aHDt2rAFbXj+2bt1q2rdvb7p162buvfdea7mnjcm3335r2rVrZ0aPHm22bNli9u7da9asWWM+//xzK8bTxuSxxx4zISEh5s033zR79+41f//7303z5s3N3LlzrRhPGxPUjicej939GOspx053Ph6+9dZbZubMmeaf//ynkWRWrlzpsr4q/brrrrvMZZddZrKysswHH3xg+vbta7p3727OnDlTz72p3Pn6efToUdO/f3/zyiuvmE8//dRs2rTJxMTEmJ49e7psoyn0ExfmrudGdZVjq/JzPmjQIBMVFWU2btxoNm7caKKiokxiYmK99reqapOH3WUs6ipXu8t41FVOb6rjQbHsR372s5+Zu+66y2VZ586dzX333ddALWpY+fn5RpLJzs42xhhz9uxZ43Q6zRNPPGHFfP/998bhcJjnnnuuoZpZL44fP246duxosrKyTFxcnJVAPHFMfv/735trr7220vWeOCZDhgwxd955p8uy4cOHm9tvv90Y45ljgrrl7sdjTzjGesqx01OOh+cWkarSr6NHjxofHx+TkZFhxXz99demWbNmJjMzs97aXh0VFQXPtXXrViPJKqA0xX6iYp5yblSTHFuVn/NPPvnESDKbN2+2YjZt2mQkmU8//bQ+ulZltcnD7jQWdZGr3Wk86iKnN+Xx4DLMUsXFxdq+fbsSEhJclickJGjjxo0N1KqGVVhYKEkKDg6WJO3du1d5eXkuY2S32xUXF+f2Y3TPPfdoyJAh6t+/v8tyTxyT119/Xb169dJNN92k0NBQ9ejRQwsWLLDWe+KYXHvttXrnnXf0n//8R5L00UcfacOGDbr++usleeaYoG65+/HYE46xnnLs9NTjYVX6tX37dp0+fdolJjw8XFFRUU2674WFhbLZbLrkkkskuW8/PY0nnRvVJMdW5ed806ZNcjgciomJsWJ69+4th8PR6MawNnnYncaiLnK1O41HXeT0pjwe3g2250bmm2++UUlJicLCwlyWh4WFKS8vr4Fa1XCMMZoyZYquvfZaRUVFSZI1DhWN0ZdfflnvbawvGRkZ+uCDD7Rt27Zy6zxxTL744gvNnz9fU6ZM0f3336+tW7dq8uTJstvtuuOOOzxyTH7/+9+rsLBQnTt3lpeXl0pKSvT444/r1ltvleSZPyeoO+5+PPaUY6ynHDs99XhYlX7l5eXJ19dXLVu2LBfTVP/W/P7773Xfffdp5MiRatGihST37Kcn8pRzo5rm2Kr8nOfl5Sk0NLTcPkNDQxvVGNY2D7vTWNRFrnan8aiLnN6Ux4Ni2TlsNpvLe2NMuWWeYOLEifr444+1YcOGcus8aYz279+ve++9V6tXr5afn1+lcZ40JmfPnlWvXr00a9YsSVKPHj20a9cuzZ8/X3fccYcV50lj8sorryg9PV3Lly/XlVdeqZycHCUnJys8PFyjRo2y4jxpTFB33Pl47EnHWE85dnr68bAm/WqqfT99+rRuueUWnT17Vs8+++wF45tqPz2du/6ulqnrHHtuTEXxjWkML2YebmpjIV3cXN0Ux+Ni5vSmMB5chlmqVatW8vLyKle5zM/PL1cpdXeTJk3S66+/rnXr1qlNmzbWcqfTKUkeNUbbt29Xfn6+evbsKW9vb3l7eys7O1t/+ctf5O3tbfXbk8akdevW6tq1q8uyLl266KuvvpLkmT8nv/vd73TffffplltuUXR0tJKSkvTb3/5WqampkjxzTFA33P147EnHWE85dnrq8bAq/XI6nSouLlZBQUGlMU3F6dOnNWLECO3du1dZWVnWrDLJvfrpyTzh3Kg2ObYqP+dOp1OHDh0qt9/Dhw83mjGsizzsLmMh1U2udqfxqIuc3pTHg2JZKV9fX/Xs2VNZWVkuy7OystSnT58GalX9MsZo4sSJWrFihdauXavIyEiX9ZGRkXI6nS5jVFxcrOzsbLcdo379+mnHjh3KycmxXr169dJtt92mnJwcdejQwePG5Jprrin3eO3//Oc/ateunSTP/Dn57rvv1KyZ6+HUy8vLeqyyJ44JasdTjseedIz1lGOnpx4Pq9Kvnj17ysfHxyUmNzdXO3fubFJ9LyuUffbZZ1qzZo1CQkJc1rtLPz2dO58b1UWOrcrPeWxsrAoLC7V161YrZsuWLSosLGw0Y1gXedhdxkKqm1ztTuNRFzm9SY9HPTxEoMkoezzywoULzSeffGKSk5NNYGCg2bdvX0M3rV7cfffdxuFwmPXr15vc3Fzr9d1331kxTzzxhHE4HGbFihVmx44d5tZbb20Sj3uvSz9+QowxnjcmW7duNd7e3ubxxx83n332mVm2bJkJCAgw6enpVoynjcmoUaPMZZddZj1WecWKFaZVq1Zm+vTpVoynjQlqx5OPx+56jPWUY6c7Hw+PHz9uPvzwQ/Phhx8aSWbOnDnmww8/tJ4CWZV+3XXXXaZNmzZmzZo15oMPPjDXXXed6d69uzlz5kxDdauc8/Xz9OnTZujQoaZNmzYmJyfH5fhUVFRkbaMp9BMX5q7nRnWVY6vycz5o0CDTrVs3s2nTJrNp0yYTHR1tEhMT67W/1VWTPOwuY1FXudpdxqOucnpTHQ+KZed45plnTLt27Yyvr6+56qqrrEcIewJJFb4WLVpkxZw9e9Y8/PDDxul0Grvdbn7xi1+YHTt2NFyjG8C5CcQTx+SNN94wUVFRxm63m86dO5sXXnjBZb2njcmxY8fMvffea9q2bWv8/PxMhw4dzMyZM11OHDxtTFA7nnw8dudjrCccO935eLhu3boKfy9HjRpljKlav06dOmUmTpxogoODjb+/v0lMTDRfffVVA/Smcufr5969eys9Pq1bt87aRlPoJ6rGHc+N6irHVuXn/MiRI+a2224zQUFBJigoyNx2222moKCgHnpZczXJw+40FnWRq91lPOoqpzfV8bAZY8zFnbsGAAAAAAAANA3cswwAAAAAAAAoRbEMAAAAAAAAKEWxDAAAAAAAAChFsQwAAAAAAAAoRbEMAAAAAAAAKEWxDAAAAAAAAChFsQyNxsaNG5WSkqKjR4/W6PPLly/X3Llza9WG+Ph4xcfH12obNfHtt9/qlltuUWhoqGw2m4YNG1av+9+3b59sNpsWL15cr/sFAFx8KSkpstlsNfrsW2+9pZSUlArXtW/fXqNHj655wwAAABopimVoNDZu3KhHHnmkQYtlDeUPf/iDVq5cqaefflqbNm3S7NmzG7pJAADorbfe0iOPPFLhupUrV+rBBx+s5xYBAABcfN4N3QDAHX333XcKCAiocvzOnTt1+eWX67bbbruIrQIAoO706NGjoZsAAABwUTCzDI1CSkqKfve730mSIiMjZbPZZLPZtH79ep09e1azZ89W586dZbfbFRoaqjvuuEMHDhywPh8fH69Vq1bpyy+/tD7740tOHnnkEcXExCg4OFgtWrTQVVddpYULF8oYU+u2x8fHKyoqSu+++6769OmjgIAA3XnnnZKkY8eOadq0aYqMjJSvr68uu+wyJScn6+TJk5L+d/njmjVrtHv3bpd+r1+/3vr3j1V0yeTo0aPVvHlzff7557r++uvVvHlzRUREaOrUqSoqKnL5/MGDBzVixAgFBQXJ4XDo5ptvVl5eXrl+ffHFF7rlllsUHh4uu92usLAw9evXTzk5ObUeMwBwF2WXOH744YcaPny4WrRoIYfDodtvv12HDx+24qqSy6T/5ZT33ntPvXv3lr+/vy677DI9+OCDKikpseKqkyMq8sorryghIUGtW7eWv7+/unTpovvuu8/KT9IPueWZZ56RJJfcum/fPkkVX4b51Vdf6fbbb1doaKjsdru6dOmiP/3pTzp79my5Nv7xj3/UnDlzFBkZqebNmys2NlabN2++0JADAABcdMwsQ6Pwm9/8Rt9++63S0tK0YsUKtW7dWpLUtWtX3X333XrhhRc0ceJEJSYmat++fXrwwQe1fv16ffDBB2rVqpWeffZZjRs3Tv/973+1cuXKctvft2+fxo8fr7Zt20qSNm/erEmTJunrr7/WQw89VOv25+bm6vbbb9f06dM1a9YsNWvWTN99953i4uJ04MAB3X///erWrZt27dqlhx56SDt27NCaNWvUunVrbdq0SRMmTFBhYaGWLVtm9fuDDz6oVhtOnz6toUOHasyYMZo6dareffdd/eEPf5DD4bD6eOrUKfXv318HDx5UamqqrrjiCq1atUo333xzue1df/31Kikp0ezZs9W2bVt988032rhxY40vkwUAd/bLX/5SI0aM0F133aVdu3bpwQcf1CeffKItW7bIx8enSrmsTF5enm655Rbdd999evTRR7Vq1So99thjKigo0Lx58+qkvZ999pmuv/56JScnKzAwUJ9++qmefPJJbd26VWvXrpUkPfjggzp58qT+8Y9/aNOmTdZny3L0uQ4fPqw+ffqouLhYf/jDH9S+fXu9+eabmjZtmv773//q2WefdYl/5pln1LlzZ+sWCg8++KCuv/567d27Vw6Ho076CQAAUCMGaCSeeuopI8ns3bvXWrZ7924jyUyYMMEldsuWLUaSuf/++61lQ4YMMe3atbvgfkpKSszp06fNo48+akJCQszZs2etdXFxcSYuLq5a7Y6LizOSzDvvvOOyPDU11TRr1sxs27bNZfk//vEPI8m89dZbLtu48sorXeLWrVtnJJl169a5LN+7d6+RZBYtWmQtGzVqlJFk/va3v7nEXn/99aZTp07W+/nz5xtJ5rXXXnOJGzt2rMs2v/nmGyPJzJ07t0pjAACe6uGHHzaSzG9/+1uX5cuWLTOSTHp6erVyWVlOqeg43axZM/Pll18aY6qXI8raWJmzZ8+a06dPm+zsbCPJfPTRR9a6e+65p9LPtmvXzowaNcp6f9999xlJZsuWLS5xd999t7HZbGbPnj0ubYyOjjZnzpyx4rZu3WokmZdffrnStgIAANQHLsNEo7Zu3TpJKneZx89+9jN16dJF77zzTpW2s3btWvXv318Oh0NeXl7y8fHRQw89pCNHjig/P7/W7WzZsqWuu+46l2VvvvmmoqKi9NOf/lRnzpyxXgMHDqzw0pnastlsuuGGG1yWdevWTV9++aX1ft26dQoKCtLQoUNd4kaOHOnyPjg4WJdffrmeeuopzZkzRx9++KHLJTQAAFfn3nNyxIgR8vb21rp166qdyyo7Tp89e1bvvvtunbT3iy++0MiRI+V0Oq28GBcXJ0navXt3jba5du1ade3aVT/72c9clo8ePVrGGGvGWpkhQ4bIy8vLet+tWzdJcslbAAAADYFiGRq1I0eOSKr4ko/w8HBr/fls3bpVCQkJkqQFCxbo3//+t7Zt26aZM2dK+uHSxNqqqH2HDh3Sxx9/LB8fH5dXUFCQjDH65ptvar3fHwsICJCfn5/LMrvdru+//956f+TIEYWFhZX7rNPpdHlvs9n0zjvvaODAgZo9e7auuuoqXXrppZo8ebKOHz9ep+0GAHdw7nHU29tbISEhOnLkSLVz2fmO01XJexdy4sQJ/fznP9eWLVv02GOPaf369dq2bZtWrFghqeZ58ciRI5X2sWz9j4WEhLi8t9vttdo/AABAXeGeZWjUyv6Qzs3NVZs2bVzWHTx40OUeL5XJyMiQj4+P3nzzTZdi0quvvlpn7fzxwwTKtGrVSv7+/vrrX/9a4Wcu1Paytp57g/7aFNlCQkK0devWcssrusF/u3bttHDhQknSf/7zH/3tb39TSkqKiouL9dxzz9W4DQDgjvLy8nTZZZdZ78+cOaMjR44oJCSk2rns0KFDFW5f+l9erE2OWLt2rQ4ePKj169dbs8kk1fqelCEhIcrNzS23/ODBg5IunPcAAAAaC2aWodGo6Bvlsksb09PTXWK3bdum3bt3q1+/fi6fr+jbaJvNJm9vb5dLPU6dOqWlS5fWafvPlZiYqP/+978KCQlRr169yr3at29/3s+Xrf/4449dlr/++us1blPfvn11/PjxcttYvnz5eT93xRVX6IEHHlB0dHS1HzwAAJ6g7AEtZf72t7/pzJkzio+Pr1Yuk1TpcbpZs2b6xS9+Ial2OaLsC56yvFvm+eefLxdbndle/fr10yeffFIuT7z00kuy2Wzq27fvBbcBAADQGDCzDI1GdHS0JOnPf/6zRo0aJR8fH3Xq1Enjxo1TWlqamjVrpsGDB1tPEIuIiNBvf/tbl8+vWLFC8+fPV8+ePdWsWTP16tVLQ4YM0Zw5czRy5EiNGzdOR44c0R//+MdyJwl1LTk5Wf/85z/1i1/8Qr/97W/VrVs3nT17Vl999ZVWr16tqVOnKiYmptLPO51O9e/fX6mpqWrZsqXatWund955x7pMpibuuOMOPf3007rjjjv0+OOPq2PHjnrrrbf09ttvu8R9/PHHmjhxom666SZ17NhRvr6+Wrt2rT7++GPdd999Nd4/ALir4YaX4AAAK2hJREFU/2/v/qOqrvL9j7+IH0ck+CQQHE+S0o3IAh0Huog16VwV9UqsljNjpVGtmszfkXZNc9YVWwX9mElnxps3Ha9aasyalc7YL0a8FY0Lfw3GhOZQs0LF4ojd8IBFoLi/f8zx8+2ImoAC5/h8rPVZy7P3+5zPfoPrsw/vs89nb9y4USEhIRo9erS9G+bgwYM1ceJEhYWFXfBcJv1zhda0adN06NAh3XDDDXr77be1cuVKTZs2zd7VuTNzxLBhw9SnTx9NnTpVixYtUmhoqNavX6+//e1vbWJPz83PPfecxo0bp+DgYA0aNEhhYWFtYh977DG98sorGj9+vJ566in1799fb731ll566SVNmzZNN9xwQ0d+tAAAAF2vu3cYAL5rwYIFxuVymSuuuMLe5au1tdU899xz5oYbbjChoaEmNjbW3HvvvaampsbnuV999ZX56U9/aq666ioTFBTks3vX//zP/5jk5GTjcDjMddddZwoLC82qVava7L7Z0d0wz9zJ8rTjx4+bX/ziFyY5OdmEhYUZy7JMamqqeeyxx4zb7f7e16itrTU//elPTXR0tLEsy9x7773mr3/961l3w4yIiGjz/LPtgHb48GHzk5/8xFx55ZUmMjLS/OQnPzFlZWU+r3nkyBHzwAMPmBtvvNFERESYK6+80gwaNMgsWbLEZ+cyALjcnb7OlpeXmzvuuMO+tt5zzz3myJEjdtyFzmWn54P333/fpKenG4fDYfr27WuefPJJc+LECZ/YC50jzjYXlJWVmczMTNO7d29z9dVXm5///Odmz549bZ7b3Nxsfv7zn5urr77anltPz5tn7oZpjDEHDx40kyZNMjExMSY0NNQkJyebF154wbS2ttoxp3fDfOGFF9r8PCWZRYsWXcBPHgAA4NIJMsaYbqnSAQAA+Ln8/HwtXrxYR48evSj35BoxYoS+/PJL7d279yKMDgAAAB3BPcsAAAAAAAAAL+5ZBpxDa2urzrfwMigoyGfTAAAAAAAA4P/4GiZwDiNGjFBpaek5+/v3768DBw503YAAAAAAAMAlR7EMOIeqqio1Njaes9/hcNi7hAEAAAAAgMDAPcuAc0hOTlZ6evo5DwplQPdZvny5Bg0apKioKEVFRSkzM1PvvPOO3W+MUX5+vlwul8LDwzVixAjt27fP5zWam5s1a9YsxcbGKiIiQjk5OTp8+LBPTH19vXJzc2VZlizLUm5uro4dO9YVKQIAAADoJgG7suzUqVP64osvFBkZqaCgoO4eDgB0C2OMGhsb5XK5dMUVgfP5yBtvvKHg4GBdf/31kqS1a9fqhRde0Icffqibb75Zzz33nJ555hmtWbNGN9xwg55++ml98MEHqqqqUmRkpCRp2rRpeuONN7RmzRrFxMRo7ty5+uqrr1ReXm7fj3DcuHE6fPiwVqxYIUmaMmWKBgwYoDfeeOOCx8p8BACBOx8BAAJTwBbLDh8+rISEhO4eBgD0CDU1NerXr193D+OSio6O1gsvvKAHH3xQLpdLeXl5euKJJyT9cxVZfHy8nnvuOT3yyCPyeDy6+uqr9eqrr+quu+6SJH3xxRdKSEjQ22+/rTFjxmj//v266aabtGPHDmVkZEiSduzYoczMTP39739XcnLyBY2L+QgA/r/LYT4CAPi/gN0N8/TKgZqaGkVFRXXzaACgezQ0NCghIcG+Jgai1tZW/eEPf9DXX3+tzMxMVVdXy+12Kysry45xOBwaPny4ysrK9Mgjj6i8vFwnTpzwiXG5XEpJSVFZWZnGjBmj7du3y7Isu1AmSUOHDpVlWSorKztnsay5uVnNzc3249OfSTEfAbicXQ7zEQAgcARssez0V11O388GAC5ngfj1v8rKSmVmZurbb7/VlVdeqU2bNummm25SWVmZJCk+Pt4nPj4+XgcPHpQkud1uhYWFqU+fPm1i3G63HRMXF9fmvHFxcXbM2RQWFmrx4sVt2pmPACAw5yMAQODhhgEAAL+UnJysiooK7dixQ9OmTdP999+vjz/+2O4/8w8yY8z3/pF2ZszZ4r/vdRYsWCCPx2MfNTU1F5oSAAAAgB6AYhkAwC+FhYXp+uuvV3p6ugoLCzV48GD9+te/ltPplKQ2q7/q6urs1WZOp1MtLS2qr68/b8yRI0fanPfo0aNtVq19l8PhsFeRsZoMAAAA8D8UywAAAcEYo+bmZiUmJsrpdKqkpMTua2lpUWlpqYYNGyZJSktLU2hoqE9MbW2t9u7da8dkZmbK4/Fo165ddszOnTvl8XjsGAAAAACBJ2DvWQYACFxPPvmkxo0bp4SEBDU2NqqoqEjvv/++iouLFRQUpLy8PBUUFCgpKUlJSUkqKChQ7969NWnSJEmSZVl66KGHNHfuXMXExCg6OlqPP/64UlNTNWrUKEnSwIEDNXbsWD388MN6+eWXJUlTpkxRdnb2Be+ECQAAAMD/UCwDAPidI0eOKDc3V7W1tbIsS4MGDVJxcbFGjx4tSZo3b56ampo0ffp01dfXKyMjQ1u2bPHZhW3JkiUKCQnRxIkT1dTUpJEjR2rNmjUKDg62Y9avX6/Zs2fbu2bm5ORo2bJlXZssAAAAgC4VZE7vaR9gGhoaZFmWPB5Ph+4XM2D+W5dgVOd34NnxXX5OAIGts9dCdB7zEQAwHwEA/Av3LAMAAAAAAAC8KJYBAAAAAAAAXhTLAAAAAAAAAC+KZQAAAAAAAIAXxTIAAAAAAADAi2IZAAAAAAAA4EWxDAAAAAAAAPCiWAYAAAAAAAB4tbtY9vnnn+vee+9VTEyMevfurR/84AcqLy+3+40xys/Pl8vlUnh4uEaMGKF9+/b5vEZzc7NmzZql2NhYRUREKCcnR4cPH/aJqa+vV25urizLkmVZys3N1bFjxzqWJQAAAAAAAHAB2lUsq6+v16233qrQ0FC98847+vjjj/WrX/1KV111lR3z/PPP68UXX9SyZcu0e/duOZ1OjR49Wo2NjXZMXl6eNm3apKKiIm3btk3Hjx9Xdna2Wltb7ZhJkyapoqJCxcXFKi4uVkVFhXJzczufMQAAAAAAAHAOIe0Jfu6555SQkKDVq1fbbQMGDLD/bYzR0qVLtXDhQk2YMEGStHbtWsXHx2vDhg165JFH5PF4tGrVKr366qsaNWqUJGndunVKSEjQ1q1bNWbMGO3fv1/FxcXasWOHMjIyJEkrV65UZmamqqqqlJyc3GZszc3Nam5uth83NDS0JzUAAAAAAACgfSvLNm/erPT0dP3sZz9TXFychgwZopUrV9r91dXVcrvdysrKstscDoeGDx+usrIySVJ5eblOnDjhE+NyuZSSkmLHbN++XZZl2YUySRo6dKgsy7JjzlRYWGh/ZdOyLCUkJLQnNQAAAAAAAKB9xbLPPvtMy5cvV1JSkv785z9r6tSpmj17tl555RVJktvtliTFx8f7PC8+Pt7uc7vdCgsLU58+fc4bExcX1+b8cXFxdsyZFixYII/HYx81NTXtSQ0AAAAAAABo39cwT506pfT0dBUUFEiShgwZon379mn58uW677777LigoCCf5xlj2rSd6cyYs8Wf73UcDoccDscF5wIAAAAAAACcqV0ry/r27aubbrrJp23gwIE6dOiQJMnpdEpSm9VfdXV19mozp9OplpYW1dfXnzfmyJEjbc5/9OjRNqvWAAAAAAAAgIulXcWyW2+9VVVVVT5tn3zyifr37y9JSkxMlNPpVElJid3f0tKi0tJSDRs2TJKUlpam0NBQn5ja2lrt3bvXjsnMzJTH49GuXbvsmJ07d8rj8dgxAAAAAAAAwMXWrq9hPvbYYxo2bJgKCgo0ceJE7dq1SytWrNCKFSsk/fOrk3l5eSooKFBSUpKSkpJUUFCg3r17a9KkSZIky7L00EMPae7cuYqJiVF0dLQef/xxpaam2rtjDhw4UGPHjtXDDz+sl19+WZI0ZcoUZWdnn3UnTAAAAAAAAOBiaFex7JZbbtGmTZu0YMECPfXUU0pMTNTSpUs1efJkO2bevHlqamrS9OnTVV9fr4yMDG3ZskWRkZF2zJIlSxQSEqKJEyeqqalJI0eO1Jo1axQcHGzHrF+/XrNnz7Z3zczJydGyZcs6my8AAAAAAABwTkHGGNPdg7gUGhoaZFmWPB6PoqKi2v38AfPfugSjOr8Dz47v8nMCCGydvRai85iPAID5CADgX9p1zzIAAAAAAAAgkFEsAwAAAAAAALwolgEAAAAAAABeFMsAAAAAAAAAL4plAAAAAAAAgBfFMgAAAAAAAMCLYhkAAAAAAADgRbEMAAAAAAAA8KJYBgAAAAAAAHhRLAMAAAAAAAC8KJYBAAAAAAAAXhTLAAAAAAAAAC+KZQAAAAAAAIAXxTIAAAAAAADAi2IZAAAAAAAA4EWxDAAAAAAAAPCiWAYAAAAAAAB4USwDAAAAAAAAvCiWAQAAAAAAAF4UywAAAAAAAAAvimUAAAAAAACAF8UyAAAAAAAAwItiGQDA7xQWFuqWW25RZGSk4uLidOedd6qqqsonxhij/Px8uVwuhYeHa8SIEdq3b59PTHNzs2bNmqXY2FhFREQoJydHhw8f9ompr69Xbm6uLMuSZVnKzc3VsWPHLnWKAAAAALoJxTIAgN8pLS3VjBkztGPHDpWUlOjkyZPKysrS119/bcc8//zzevHFF7Vs2TLt3r1bTqdTo0ePVmNjox2Tl5enTZs2qaioSNu2bdPx48eVnZ2t1tZWO2bSpEmqqKhQcXGxiouLVVFRodzc3C7NFwAAAEDXCenuAQAA0F7FxcU+j1evXq24uDiVl5fr9ttvlzFGS5cu1cKFCzVhwgRJ0tq1axUfH68NGzbokUcekcfj0apVq/Tqq69q1KhRkqR169YpISFBW7du1ZgxY7R//34VFxdrx44dysjIkCStXLlSmZmZqqqqUnJyctcmDgAAAOCSY2UZAMDveTweSVJ0dLQkqbq6Wm63W1lZWXaMw+HQ8OHDVVZWJkkqLy/XiRMnfGJcLpdSUlLsmO3bt8uyLLtQJklDhw6VZVl2zJmam5vV0NDgcwAAAADwHxTLAAB+zRijOXPm6LbbblNKSookye12S5Li4+N9YuPj4+0+t9utsLAw9enT57wxcXFxbc4ZFxdnx5ypsLDQvr+ZZVlKSEjoXIIAAAAAuhTFMgCAX5s5c6Y++ugjvfbaa236goKCfB4bY9q0nenMmLPFn+91FixYII/HYx81NTUXkgYAAACAHoJiGQDAb82aNUubN2/We++9p379+tntTqdTktqs/qqrq7NXmzmdTrW0tKi+vv68MUeOHGlz3qNHj7ZZtXaaw+FQVFSUzwEAAADAf1AsAwD4HWOMZs6cqY0bN+rdd99VYmKiT39iYqKcTqdKSkrstpaWFpWWlmrYsGGSpLS0NIWGhvrE1NbWau/evXZMZmamPB6Pdu3aZcfs3LlTHo/HjgEAAAAQWNgNEwDgd2bMmKENGzboT3/6kyIjI+0VZJZlKTw8XEFBQcrLy1NBQYGSkpKUlJSkgoIC9e7dW5MmTbJjH3roIc2dO1cxMTGKjo7W448/rtTUVHt3zIEDB2rs2LF6+OGH9fLLL0uSpkyZouzsbHbCBAAAAAIUxTIAgN9Zvny5JGnEiBE+7atXr9YDDzwgSZo3b56ampo0ffp01dfXKyMjQ1u2bFFkZKQdv2TJEoWEhGjixIlqamrSyJEjtWbNGgUHB9sx69ev1+zZs+1dM3NycrRs2bJLmyAAAACAbhNkjDHdPYhLoaGhQZZlyePxdOh+MQPmv3UJRnV+B54d3+XnBBDYOnstROcxHwEA8xEAwL9wzzIAAAAAAADAq1PFssLCQvu+MKcZY5Sfny+Xy6Xw8HCNGDFC+/bt83lec3OzZs2apdjYWEVERCgnJ0eHDx/2iamvr1dubq4sy5JlWcrNzdWxY8c6M1wAAAAAAADgvDpcLNu9e7dWrFihQYMG+bQ///zzevHFF7Vs2TLt3r1bTqdTo0ePVmNjox2Tl5enTZs2qaioSNu2bdPx48eVnZ2t1tZWO2bSpEmqqKhQcXGxiouLVVFRodzc3I4OFwAAAAAAAPheHSqWHT9+XJMnT9bKlSvVp08fu90Yo6VLl2rhwoWaMGGCUlJStHbtWn3zzTfasGGDJMnj8WjVqlX61a9+pVGjRmnIkCFat26dKisrtXXrVknS/v37VVxcrN/97nfKzMxUZmamVq5cqTfffFNVVVUXIW0AAAAAAACgrQ4Vy2bMmKHx48dr1KhRPu3V1dVyu932jmGS5HA4NHz4cJWVlUmSysvLdeLECZ8Yl8ullJQUO2b79u2yLEsZGRl2zNChQ2VZlh1zpubmZjU0NPgcAAAAAAAAQHuEtPcJRUVF2rNnj3bv3t2mz+12S5Li4+N92uPj43Xw4EE7JiwszGdF2umY0893u92Ki4tr8/pxcXF2zJkKCwu1ePHi9qYDAAAAAAAA2Nq1sqympkaPPvqo1q1bp169ep0zLigoyOexMaZN25nOjDlb/PleZ8GCBfJ4PPZRU1Nz3vMBAAAAAAAAZ2pXsay8vFx1dXVKS0tTSEiIQkJCVFpaqt/85jcKCQmxV5Sdufqrrq7O7nM6nWppaVF9ff15Y44cOdLm/EePHm2zau00h8OhqKgonwMAAAAAAABoj3YVy0aOHKnKykpVVFTYR3p6uiZPnqyKigpdd911cjqdKikpsZ/T0tKi0tJSDRs2TJKUlpam0NBQn5ja2lrt3bvXjsnMzJTH49GuXbvsmJ07d8rj8dgxAAAAAAAAwMXWrnuWRUZGKiUlxactIiJCMTExdnteXp4KCgqUlJSkpKQkFRQUqHfv3po0aZIkybIsPfTQQ5o7d65iYmIUHR2txx9/XKmpqfaGAQMHDtTYsWP18MMP6+WXX5YkTZkyRdnZ2UpOTu500gAAAAAAAMDZtPsG/99n3rx5ampq0vTp01VfX6+MjAxt2bJFkZGRdsySJUsUEhKiiRMnqqmpSSNHjtSaNWsUHBxsx6xfv16zZ8+2d83MycnRsmXLLvZwAQAAAAAAAFuQMcZ09yAuhYaGBlmWJY/H06H7lw2Y/9YlGNX5HXh2fJefE0Bg6+y1EJ3HfAQAzEcAAP/SrnuWAQAAAAAAAIGMYhkAAAAAAADgRbEMAAAAAAAA8KJYBgAAAAAAAHhRLAMAAAAAAAC8KJYBAAAAAAAAXhTLAAAAAAAAAC+KZQAAAAAAAIAXxTIAAAAAAADAi2IZAAAAAAAA4EWxDAAAAAAAAPCiWAYAAAAAAAB4USwDAAAAAAAAvCiWAQAAAAAAAF4UywAAAAAAAAAvimUAAAAAAACAF8UyAAAAAAAAwItiGQAAAAAAAOBFsQwAAAAAAADwolgGAAAAAAAAeFEsAwAAAAAAALwolgEAAAAAAABeFMsAAAAAAAAAL4plAAAAAAAAgBfFMgAAAAAAAMCLYhkAAAAAAADgRbEMAAAAAAAA8KJYBgAAAAAAAHiFdPcAAAAABsx/q1vOe+DZ8d1yXgAAAPRcrCwDAPidDz74QHfccYdcLpeCgoL0xz/+0affGKP8/Hy5XC6Fh4drxIgR2rdvn09Mc3OzZs2apdjYWEVERCgnJ0eHDx/2iamvr1dubq4sy5JlWcrNzdWxY8cucXYAAAAAuhPFMgCA3/n66681ePBgLVu27Kz9zz//vF588UUtW7ZMu3fvltPp1OjRo9XY2GjH5OXladOmTSoqKtK2bdt0/PhxZWdnq7W11Y6ZNGmSKioqVFxcrOLiYlVUVCg3N/eS5wcAAACg+/A1TACA3xk3bpzGjRt31j5jjJYuXaqFCxdqwoQJkqS1a9cqPj5eGzZs0COPPCKPx6NVq1bp1Vdf1ahRoyRJ69atU0JCgrZu3aoxY8Zo//79Ki4u1o4dO5SRkSFJWrlypTIzM1VVVaXk5OSuSRYAAABAl2JlGQAgoFRXV8vtdisrK8tuczgcGj58uMrKyiRJ5eXlOnHihE+My+VSSkqKHbN9+3ZZlmUXyiRp6NChsizLjjmb5uZmNTQ0+BwAAAAA/AfFMgBAQHG73ZKk+Ph4n/b4+Hi7z+12KywsTH369DlvTFxcXJvXj4uLs2POprCw0L7HmWVZSkhI6FQ+AAAAALoWxTIAQEAKCgryeWyMadN2pjNjzhb/fa+zYMECeTwe+6ipqWnnyAEAAAB0p3YVywoLC3XLLbcoMjJScXFxuvPOO1VVVeUTww5kAIDu5HQ6JanN6q+6ujp7tZnT6VRLS4vq6+vPG3PkyJE2r3/06NE2q9a+y+FwKCoqyucAAAAA4D/aVSwrLS3VjBkztGPHDpWUlOjkyZPKysrS119/bcewAxkAoDslJibK6XSqpKTEbmtpaVFpaamGDRsmSUpLS1NoaKhPTG1trfbu3WvHZGZmyuPxaNeuXXbMzp075fF47BgAAAAAgaddu2EWFxf7PF69erXi4uJUXl6u22+/nR3IAABd4vjx4/rHP/5hP66urlZFRYWio6N17bXXKi8vTwUFBUpKSlJSUpIKCgrUu3dvTZo0SZJkWZYeeughzZ07VzExMYqOjtbjjz+u1NRUe24aOHCgxo4dq4cfflgvv/yyJGnKlCnKzs4O+HlowPy3unsIAAAAQLfp1D3LPB6PJCk6OlpS9+5Axu5jAHD5+Otf/6ohQ4ZoyJAhkqQ5c+ZoyJAh+s///E9J0rx585SXl6fp06crPT1dn3/+ubZs2aLIyEj7NZYsWaI777xTEydO1K233qrevXvrjTfeUHBwsB2zfv16paamKisrS1lZWRo0aJBeffXVrk0WAAAAQJdq18qy7zLGaM6cObrtttuUkpIi6fw7kB08eNCOuRQ7kBUWFmrx4sUdTQcA4EdGjBghY8w5+4OCgpSfn6/8/PxzxvTq1Uu//e1v9dvf/vacMdHR0Vq3bl1nhgoAAADAz3R4ZdnMmTP10Ucf6bXXXmvT1x07kLH7GAAAAAAAADqrQ8WyWbNmafPmzXrvvffUr18/u707dyBj9zEAAAAAAAB0VruKZcYYzZw5Uxs3btS7776rxMREn352IAMAAAAAAIA/a9c9y2bMmKENGzboT3/6kyIjI+0VZJZlKTw8XEFBQexABgAAAAAAAL/VrmLZ8uXLJf3zxsrftXr1aj3wwAOS/rkDWVNTk6ZPn676+nplZGScdQeykJAQTZw4UU1NTRo5cqTWrFnTZgey2bNn27tm5uTkaNmyZR3JEQAA4KwGzH+ry8954NnxXX5OAAAAXLggc77txPxYQ0ODLMuSx+Pp0P3LePMMIBB09lqIzvPH+QiXFvM9LkfMRwAAf9KulWUAAADoHD6QAwAA6Nk6tBsmAAAAAAAAEIgolgEAAAAAAABeFMsAAAAAAAAAL4plAAAAAAAAgBfFMgAAAAAAAMCLYhkAAAAAAADgRbEMAAAAAAAA8KJYBgAAAAAAAHhRLAMAAAAAAAC8KJYBAAAAAAAAXhTLAAAAAAAAAC+KZQAAAAAAAIAXxTIAAAAAAADAi2IZAAAAAAAA4EWxDAAAAAAAAPCiWAYAAAAAAAB4USwDAAAAAAAAvCiWAQAAAAAAAF4UywAAAAAAAAAvimUAAAAAAACAV0h3DwAAAACX1oD5b3XLeQ88O75bzgsAANAZrCwDAAAAAAAAvCiWAQAAAAAAAF4UywAAAAAAAAAvimUAAAAAAACAF8UyAAAAAAAAwItiGQAAAAAAAOBFsQwAAAAAAADwolgGAAAAAAAAeIV09wAAAAAQmAbMf6vLz3ng2fFdfk4AABBYWFkGAAAAAAAAeFEsAwAAAAAAALz4GiYAAAACRnd89VPi658AAAQSVpYBAAAAAAAAXj2+WPbSSy8pMTFRvXr1Ulpamv7yl79095AAAJcZ5iIAAADg8tGji2W///3vlZeXp4ULF+rDDz/Uj370I40bN06HDh3q7qEBAC4TzEUAAADA5SXIGGO6exDnkpGRoR/+8Idavny53TZw4EDdeeedKiws9Iltbm5Wc3Oz/djj8ejaa69VTU2NoqKi2n3ulEV/7vjAAeAi27t4TIee19DQoISEBB07dkyWZV3kUV0e2jMXScxHAAIb8xEA4HLQY2/w39LSovLycs2fP9+nPSsrS2VlZW3iCwsLtXjx4jbtCQkJl2yMANBVrKWde35jYyN/nHRAe+ciifkIQGBjPgIAXA56bLHsyy+/VGtrq+Lj433a4+Pj5Xa728QvWLBAc+bMsR+fOnVKX331lWJiYhQUFNSuc5/+5KujqwD8CbkGJnINTB3J1RijxsZGuVyuSzy6wNTeuUhiPrpQgZybRH7+jvwuPuYjAIA/6bHFstPO/MPCGHPWPzYcDoccDodP21VXXdWpc0dFRQXkG6SzIdfARK6Bqb258gl+513oXCQxH7VXIOcmkZ+/I7+Li/kIAOAveuwN/mNjYxUcHNzmk/u6uro2n/ADAHApMBcBAAAAl58eWywLCwtTWlqaSkpKfNpLSko0bNiwbhoVAOBywlwEAAAAXH569Ncw58yZo9zcXKWnpyszM1MrVqzQoUOHNHXq1Et6XofDoUWLFrX5Gk0gItfARK6B6XLKtSfprrlICuzfeSDnJpGfvyM/AAAub0HGGNPdgzifl156Sc8//7xqa2uVkpKiJUuW6Pbbb+/uYQEALiPMRQAAAMDlo8cXywAAAAAAAICu0mPvWQYAAAAAAAB0NYplAAAAAAAAgBfFMgAAAAAAAMCLYhkAAAAAAADgRbHsDC+99JISExPVq1cvpaWl6S9/+Ut3D+mi+OCDD3THHXfI5XIpKChIf/zjH336jTHKz8+Xy+VSeHi4RowYoX379nXPYDuhsLBQt9xyiyIjIxUXF6c777xTVVVVPjGBkuvy5cs1aNAgRUVFKSoqSpmZmXrnnXfs/kDJ82wKCwsVFBSkvLw8uy1Q8s3Pz1dQUJDP4XQ67f5AyRPfr6fNRxfr+trc3KxZs2YpNjZWERERysnJ0eHDh31i6uvrlZubK8uyZFmWcnNzdezYMZ+YQ4cO6Y477lBERIRiY2M1e/ZstbS0XNR8O3Kd6cn5ff7557r33nsVExOj3r176wc/+IHKy8sDIr+TJ0/qF7/4hRITExUeHq7rrrtOTz31lE6dOuV3+V2M92xdmUdlZaWGDx+u8PBwXXPNNXrqqafE/mEAAL9nYCsqKjKhoaFm5cqV5uOPPzaPPvqoiYiIMAcPHuzuoXXa22+/bRYuXGhef/11I8ls2rTJp//ZZ581kZGR5vXXXzeVlZXmrrvuMn379jUNDQ3dM+AOGjNmjFm9erXZu3evqaioMOPHjzfXXnutOX78uB0TKLlu3rzZvPXWW6aqqspUVVWZJ5980oSGhpq9e/caYwInzzPt2rXLDBgwwAwaNMg8+uijdnug5Lto0SJz8803m9raWvuoq6uz+wMlT5xfT5yPLtb1derUqeaaa64xJSUlZs+ePebHP/6xGTx4sDl58qQdM3bsWJOSkmLKyspMWVmZSUlJMdnZ2Xb/yZMnTUpKivnxj39s9uzZY0pKSozL5TIzZ868KLl25jrTU/P76quvTP/+/c0DDzxgdu7caaqrq83WrVvNP/7xj4DI7+mnnzYxMTHmzTffNNXV1eYPf/iDufLKK83SpUv9Lr+L8Z6tq/LweDwmPj7e3H333aaystK8/vrrJjIy0vzyl7+8oFwBAOipKJZ9x7/+67+aqVOn+rTdeOONZv78+d00okvjzDdep06dMk6n0zz77LN227fffmssyzL//d//3Q0jvHjq6uqMJFNaWmqMCexcjTGmT58+5ne/+13A5tnY2GiSkpJMSUmJGT58uP1HbCDlu2jRIjN48OCz9gVSnjg/f5iPOnJ9PXbsmAkNDTVFRUV2zOeff26uuOIKU1xcbIwx5uOPPzaSzI4dO+yY7du3G0nm73//uzHmn8WEK664wnz++ed2zGuvvWYcDofxeDydyqsz15menN8TTzxhbrvttnP2+3t+48ePNw8++KBP24QJE8y9997r1/l15D1bV+bx0ksvGcuyzLfffmvHFBYWGpfLZU6dOtWuXAEA6En4GqZXS0uLysvLlZWV5dOelZWlsrKybhpV16iurpbb7fbJ3eFwaPjw4X6fu8fjkSRFR0dLCtxcW1tbVVRUpK+//lqZmZkBm+eMGTM0fvx4jRo1yqc90PL99NNP5XK5lJiYqLvvvlufffaZpMDLE2fnL/NRR66v5eXlOnHihE+My+VSSkqKHbN9+3ZZlqWMjAw7ZujQobIsyycmJSVFLpfLjhkzZoyam5t9vlbYEZ25zvTk/DZv3qz09HT97Gc/U1xcnIYMGaKVK1cGTH633Xab/vd//1effPKJJOlvf/ubtm3bpn//938PiPxO62l5bN++XcOHD5fD4fCJ+eKLL3TgwIFO5QoAQHcK6e4B9BRffvmlWltbFR8f79MeHx8vt9vdTaPqGqfzO1vuBw8e7I4hXRTGGM2ZM0e33XabUlJSJAVerpWVlcrMzNS3336rK6+8Ups2bdJNN91kv9ENlDwlqaioSHv27NHu3bvb9AXS7zUjI0OvvPKKbrjhBh05ckRPP/20hg0bpn379gVUnjg3f5iPOnp9dbvdCgsLU58+fdrEnH6+2+1WXFxcm3PGxcX5xJx5nj59+igsLKxTP6POXmd6cn6fffaZli9frjlz5ujJJ5/Url27NHv2bDkcDt13331+n98TTzwhj8ejG2+8UcHBwWptbdUzzzyje+65xz6nP+d3Wk/Lw+12a8CAAW3Oc7ovMTGxI2kCANDtKJadISgoyOexMaZNW6AKtNxnzpypjz76SNu2bWvTFyi5Jicnq6KiQseOHdPrr7+u+++/X6WlpXZ/oORZU1OjRx99VFu2bFGvXr3OGRcI+Y4bN87+d2pqqjIzM/Uv//IvWrt2rYYOHSopMPLE9+vJv+eLfX09M+Zs8R2JaY9LeZ3pCfmdOnVK6enpKigokCQNGTJE+/bt0/Lly3Xfffed87z+kt/vf/97rVu3Ths2bNDNN9+siooK5eXlyeVy6f777z/nef0lvzP1pDzONpZzPRcAAH/B1zC9YmNjFRwc3OYTv7q6ujafqgWa0zvtBVLus2bN0ubNm/Xee++pX79+dnug5RoWFqbrr79e6enpKiws1ODBg/XrX/864PIsLy9XXV2d0tLSFBISopCQEJWWluo3v/mNQkJCfD7F/i5/zfe7IiIilJqaqk8//TTgfq84u54+H3Xm+up0OtXS0qL6+vrzxhw5cqTNeY8ePeoTc+Z56uvrdeLEiQ7/jC7GdaYn59e3b1/ddNNNPm0DBw7UoUOH7HP6c37/8R//ofnz5+vuu+9WamqqcnNz9dhjj6mwsDAg8jutp+Vxtpi6ujpJbVe/AQDgTyiWeYWFhSktLU0lJSU+7SUlJRo2bFg3japrJCYmyul0+uTe0tKi0tJSv8vdGKOZM2dq48aNevfdd9ss/w+kXM/GGKPm5uaAy3PkyJGqrKxURUWFfaSnp2vy5MmqqKjQddddF1D5fldzc7P279+vvn37BtzvFWfXU+eji3F9TUtLU2hoqE9MbW2t9u7da8dkZmbK4/Fo165ddszOnTvl8Xh8Yvbu3ava2lo7ZsuWLXI4HEpLS+tQfhfjOtOT87v11ltVVVXl0/bJJ5+of//+kvz/9/fNN9/oiit839YGBwfr1KlTAZHfaT0tj8zMTH3wwQdqaWnxiXG5XG2+ngkAgF+55FsI+JGioiITGhpqVq1aZT7++GOTl5dnIiIizIEDB7p7aJ3W2NhoPvzwQ/Phhx8aSebFF180H374oTl48KAx5p/bkFuWZTZu3GgqKyvNPffc02Ybcn8wbdo0Y1mWef/9901tba19fPPNN3ZMoOS6YMEC88EHH5jq6mrz0UcfmSeffNJcccUVZsuWLcaYwMnzXL67S50xgZPv3Llzzfvvv28+++wzs2PHDpOdnW0iIyPt61Cg5Inz64nz0cW6vk6dOtX069fPbN261ezZs8f827/9mxk8eLA5efKkHTN27FgzaNAgs337drN9+3aTmppqsrOz7f6TJ0+alJQUM3LkSLNnzx6zdetW069fPzNz5syLmnNHrjM9Nb9du3aZkJAQ88wzz5hPP/3UrF+/3vTu3dusW7cuIPK7//77zTXXXGPefPNNU11dbTZu3GhiY2PNvHnz/C6/i/GeravyOHbsmImPjzf33HOPqaysNBs3bjRRUVHml7/85QXlCgBAT0Wx7Az/9V//Zfr372/CwsLMD3/4Q1NaWtrdQ7oo3nvvPSOpzXH//fcbY/65FfmiRYuM0+k0DofD3H777aaysrJ7B90BZ8tRklm9erUdEyi5Pvjgg/b/1auvvtqMHDnSLpQZEzh5nsuZf8QGSr533XWX6du3rwkNDTUul8tMmDDB7Nu3z+4PlDzx/XrafHSxrq9NTU1m5syZJjo62oSHh5vs7Gxz6NAhn5j/+7//M5MnTzaRkZEmMjLSTJ482dTX1/vEHDx40IwfP96Eh4eb6OhoM3PmTPPtt99e1Jw7cp3pyfm98cYbJiUlxTgcDnPjjTeaFStW+PT7c34NDQ3m0UcfNddee63p1auXue6668zChQtNc3Oz3+V3Md6zdWUeH330kfnRj35kHA6HcTqdJj8/35w6deqCcgUAoKcKMsZ7F04AAAAAAADgMsc9ywAAAAAAAAAvimUAAAAAAACAF8UyAAAAAAAAwItiGQAAAAAAAOBFsQwAAAAAAADwolgGAAAAAAAAeFEsAwAAAAAAALwolgEAAAAAAABeFMsAAAAAAAAAL4plAAAAAAAAgBfFMgAAAAAAAMDr/wEJc4qZVYMN9wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1500x1000 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_num = df.select_dtypes(exclude = 'object')\n",
    "\n",
    "df_num.fillna(\"\").hist(bins = 10, figsize=(15,10), grid = False)\n",
    "plt.suptitle('Distribution of Numeric Variables', y=1, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<b>Highly skewed features:</b> `num_dependent`, `num_referrals`, `total_refunds` & `total_long_distance_fee`\n",
    "- Consider to bin `num_dependent` & `num_referrals` as they are discrete data and majority of data falls under 0 -- as a way to handle zero or outlier\n",
    "- `total_long_distance_fee` will need to undergo transformation to bring down the skewness\n",
    "\n",
    "<b>Slightly skewed features:</b> `tenure_months`, `population` and `avg_gb_download_monthly`\n",
    "-  Consider to undergo scaling methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Determine Correlation of Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>",
         "name": "0",
         "texttemplate": "%{z}",
         "type": "heatmap",
         "x": [
          "age",
          "num_dependents",
          "tenure_months",
          "num_referrals",
          "avg_long_distance_fee_monthly",
          "total_long_distance_fee",
          "avg_gb_download_monthly",
          "total_monthly_fee",
          "total_charges_quarter",
          "total_refunds",
          "population"
         ],
         "xaxis": "x",
         "y": [
          "age",
          "num_dependents",
          "tenure_months",
          "num_referrals",
          "avg_long_distance_fee_monthly",
          "total_long_distance_fee",
          "avg_gb_download_monthly",
          "total_monthly_fee",
          "total_charges_quarter",
          "total_refunds",
          "population"
         ],
         "yaxis": "y",
         "z": [
          [
           1,
           -0.12,
           0.01,
           -0.03,
           -0.01,
           0,
           -0.38,
           0.14,
           0.06,
           0.02,
           -0.02
          ],
          [
           -0.12,
           1,
           0.11,
           0.28,
           -0.01,
           0.07,
           0.13,
           -0.13,
           0.02,
           0.01,
           -0.02
          ],
          [
           0.01,
           0.11,
           1,
           0.33,
           0.01,
           0.67,
           0.05,
           0.25,
           0.83,
           0.06,
           -0.01
          ],
          [
           -0.03,
           0.28,
           0.33,
           1,
           0.01,
           0.22,
           0.04,
           0.03,
           0.25,
           0.02,
           -0.01
          ],
          [
           -0.01,
           -0.01,
           0.01,
           0.01,
           1,
           0.6,
           -0.03,
           0.14,
           0.07,
           -0.02,
           -0.02
          ],
          [
           0,
           0.07,
           0.67,
           0.22,
           0.6,
           1,
           0.01,
           0.25,
           0.61,
           0.03,
           -0.02
          ],
          [
           -0.38,
           0.13,
           0.05,
           0.04,
           -0.03,
           0.01,
           1,
           0.39,
           0.22,
           0,
           0.02
          ],
          [
           0.14,
           -0.13,
           0.25,
           0.03,
           0.14,
           0.25,
           0.39,
           1,
           0.65,
           0.03,
           0.01
          ],
          [
           0.06,
           0.02,
           0.83,
           0.25,
           0.07,
           0.61,
           0.22,
           0.65,
           1,
           0.04,
           -0.01
          ],
          [
           0.02,
           0.01,
           0.06,
           0.02,
           -0.02,
           0.03,
           0,
           0.03,
           0.04,
           1,
           0.02
          ],
          [
           -0.02,
           -0.02,
           -0.01,
           -0.01,
           -0.02,
           -0.02,
           0.02,
           0.01,
           -0.01,
           0.02,
           1
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "colorscale": [
          [
           0,
           "rgb(103,0,31)"
          ],
          [
           0.1,
           "rgb(178,24,43)"
          ],
          [
           0.2,
           "rgb(214,96,77)"
          ],
          [
           0.3,
           "rgb(244,165,130)"
          ],
          [
           0.4,
           "rgb(253,219,199)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(209,229,240)"
          ],
          [
           0.7,
           "rgb(146,197,222)"
          ],
          [
           0.8,
           "rgb(67,147,195)"
          ],
          [
           0.9,
           "rgb(33,102,172)"
          ],
          [
           1,
           "rgb(5,48,97)"
          ]
         ]
        },
        "height": 750,
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "side": "top"
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "domain": [
          0,
          1
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df_num = df.select_dtypes(exclude = 'object')\n",
    "\n",
    "corr = df_num.corr().round(2)\n",
    "\n",
    "#Creating a heat map of all the numerical features.\n",
    "fig = px.imshow(corr, aspect = 'auto', color_continuous_scale= 'RdBu', text_auto= True)\n",
    "fig.update_xaxes(side=\"top\")\n",
    "\n",
    "fig.layout.height = 750\n",
    "# fig.layout.width = 1000\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "From the heat map, we can deduce some positive correlations: \n",
    "- tenure_months ↔ total_charges_quarter\n",
    "- tenure_months ↔ total_long_distance_fee\n",
    "- avg_long_distance_fee_monthly ↔ total_long_distance_fee_monthly\n",
    "- total_long_distance_fee_monthly ↔ total_charges_quarter\n",
    "\n",
    "Additionally, we can deduce some negative correlations:\n",
    "- age ↔ avg_gb_download_monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Explore Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id                 0\n",
       "zip_code                    0\n",
       "account_id                  0\n",
       "gender                      0\n",
       "senior_citizen              0\n",
       "married                     0\n",
       "has_internet_service        0\n",
       "internet_type               0\n",
       "has_unlimited_data          0\n",
       "has_phone_service           0\n",
       "has_multiple_lines          0\n",
       "has_premium_tech_support    0\n",
       "has_online_security         0\n",
       "has_online_backup           0\n",
       "has_device_protection       0\n",
       "contract_type               0\n",
       "paperless_billing           0\n",
       "payment_method              0\n",
       "stream_tv                   0\n",
       "stream_movie                0\n",
       "stream_music                0\n",
       "area_id                     0\n",
       "city                        0\n",
       "latitude                    0\n",
       "longitude                   0\n",
       "status                      0\n",
       "churn_label                 0\n",
       "churn_category              0\n",
       "churn_reason                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(include = ['object']).isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "churn_label\n",
       "No     5174\n",
       "Yes    1817\n",
       "         52\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find out the num of instances for each churn status\n",
    "df['churn_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Other'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Deceased', 'Moved'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Understand records without churn label\n",
    "display(df[df['churn_label']=='']['churn_category'].unique())\n",
    "display(df[df['churn_label']=='']['churn_reason'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>churn_label</th>\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_premium_tech_support</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "churn_label                       No   Yes\n",
       "has_premium_tech_support                  \n",
       "No                        0.01  0.69  0.30\n",
       "Yes                       0.01  0.85  0.15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>churn_label</th>\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_multiple_lines</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "churn_label                 No   Yes\n",
       "has_multiple_lines                  \n",
       "No                  0.01  0.75  0.24\n",
       "Yes                 0.01  0.71  0.28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>churn_label</th>\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stream_tv</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "churn_label          No   Yes\n",
       "stream_tv                    \n",
       "No           0.01  0.76  0.24\n",
       "Yes          0.01  0.70  0.29"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>churn_label</th>\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_phone_service</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "churn_label                No   Yes\n",
       "has_phone_service                  \n",
       "No                 0.01  0.75  0.24\n",
       "Yes                0.01  0.73  0.26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>churn_label</th>\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paperless_billing</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "churn_label                No   Yes\n",
       "paperless_billing                  \n",
       "No                 0.01  0.84  0.16\n",
       "Yes                0.01  0.66  0.33"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>churn_label</th>\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contract_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Month-to-Month</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>One Year</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Two Year</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "churn_label             No   Yes\n",
       "contract_type                   \n",
       "Month-to-Month  0.01  0.54  0.45\n",
       "One Year        0.01  0.89  0.10\n",
       "Two Year         NaN  0.97  0.03"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>churn_label</th>\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senior_citizen</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "churn_label             No   Yes\n",
       "senior_citizen                  \n",
       "No              0.01  0.76  0.23\n",
       "Yes             0.01  0.58  0.41"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>churn_label</th>\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_internet_service</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "churn_label                   No   Yes\n",
       "has_internet_service                  \n",
       "No                    0.00  0.93  0.07\n",
       "Yes                   0.01  0.68  0.31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>churn_label</th>\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stream_music</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "churn_label           No   Yes\n",
       "stream_music                  \n",
       "No            0.01  0.75  0.24\n",
       "Yes           0.01  0.71  0.28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>churn_label</th>\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "churn_label          No   Yes\n",
       "gender                       \n",
       "Female       0.01  0.73  0.26\n",
       "Male         0.01  0.74  0.25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>churn_label</th>\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zip_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "churn_label          No   Yes\n",
       "zip_code                     \n",
       "90           0.01  0.76  0.23\n",
       "91           0.01  0.75  0.24\n",
       "92           0.01  0.68  0.31\n",
       "93           0.01  0.76  0.23\n",
       "94           0.01  0.73  0.26\n",
       "95           0.01  0.74  0.25\n",
       "96           0.00  0.77  0.22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>churn_label</th>\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_unlimited_data</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "churn_label                 No   Yes\n",
       "has_unlimited_data                  \n",
       "No                  0.00  0.84  0.16\n",
       "Yes                 0.01  0.68  0.31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>churn_label</th>\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>married</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "churn_label          No   Yes\n",
       "married                      \n",
       "No           0.01  0.67  0.32\n",
       "Yes          0.01  0.80  0.19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>churn_label</th>\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_online_security</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "churn_label                  No   Yes\n",
       "has_online_security                  \n",
       "No                   0.01  0.69  0.30\n",
       "Yes                  0.00  0.85  0.14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>churn_label</th>\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>internet_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cable</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DSL</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fiber Optic</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>None</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "churn_label            No   Yes\n",
       "internet_type                  \n",
       "Cable          0.01  0.74  0.25\n",
       "DSL            0.00  0.81  0.18\n",
       "Fiber Optic    0.01  0.59  0.40\n",
       "None           0.00  0.93  0.07"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>churn_label</th>\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stream_movie</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "churn_label           No   Yes\n",
       "stream_movie                  \n",
       "No            0.01  0.76  0.24\n",
       "Yes           0.01  0.70  0.29"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>churn_label</th>\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_online_backup</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "churn_label                No   Yes\n",
       "has_online_backup                  \n",
       "No                 0.01  0.71  0.28\n",
       "Yes                0.01  0.78  0.21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>churn_label</th>\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_device_protection</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "churn_label                    No   Yes\n",
       "has_device_protection                  \n",
       "No                     0.01  0.71  0.28\n",
       "Yes                    0.00  0.77  0.22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>churn_label</th>\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>payment_method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bank Withdrawal</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Credit Card</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mailed Check</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "churn_label              No   Yes\n",
       "payment_method                   \n",
       "Bank Withdrawal  0.01  0.66  0.33\n",
       "Credit Card      0.00  0.86  0.14\n",
       "Mailed Check     0.01  0.63  0.36"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all the columns with data type as string\n",
    "cat_cols = df.select_dtypes('object').columns\n",
    "cat_cols_eda = list(set(cat_cols) - set(['customer_id', 'account_id', 'city', 'status', 'churn_label', 'churn_category', 'churn_reason', 'area_id', 'latitude', 'longitude']))\n",
    "\n",
    "for col in cat_cols_eda:\n",
    "    churn_grp = df.groupby(col)['churn_label'].value_counts(normalize= True).round(2).unstack()\n",
    "    display(churn_grp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Collinearity between Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('contract_type', 'contract_type'),\n",
       " ('contract_type', 'gender'),\n",
       " ('contract_type', 'has_device_protection'),\n",
       " ('contract_type', 'has_internet_service'),\n",
       " ('contract_type', 'has_multiple_lines'),\n",
       " ('contract_type', 'has_online_backup'),\n",
       " ('contract_type', 'has_online_security'),\n",
       " ('contract_type', 'has_phone_service'),\n",
       " ('contract_type', 'has_premium_tech_support'),\n",
       " ('contract_type', 'has_unlimited_data'),\n",
       " ('contract_type', 'internet_type'),\n",
       " ('contract_type', 'married'),\n",
       " ('contract_type', 'paperless_billing'),\n",
       " ('contract_type', 'payment_method'),\n",
       " ('contract_type', 'senior_citizen'),\n",
       " ('contract_type', 'stream_movie'),\n",
       " ('contract_type', 'stream_music'),\n",
       " ('contract_type', 'stream_tv'),\n",
       " ('contract_type', 'zip_code'),\n",
       " ('gender', 'contract_type'),\n",
       " ('gender', 'gender'),\n",
       " ('gender', 'has_device_protection'),\n",
       " ('gender', 'has_internet_service'),\n",
       " ('gender', 'has_multiple_lines'),\n",
       " ('gender', 'has_online_backup'),\n",
       " ('gender', 'has_online_security'),\n",
       " ('gender', 'has_phone_service'),\n",
       " ('gender', 'has_premium_tech_support'),\n",
       " ('gender', 'has_unlimited_data'),\n",
       " ('gender', 'internet_type'),\n",
       " ('gender', 'married'),\n",
       " ('gender', 'paperless_billing'),\n",
       " ('gender', 'payment_method'),\n",
       " ('gender', 'senior_citizen'),\n",
       " ('gender', 'stream_movie'),\n",
       " ('gender', 'stream_music'),\n",
       " ('gender', 'stream_tv'),\n",
       " ('gender', 'zip_code'),\n",
       " ('has_device_protection', 'contract_type'),\n",
       " ('has_device_protection', 'gender'),\n",
       " ('has_device_protection', 'has_device_protection'),\n",
       " ('has_device_protection', 'has_internet_service'),\n",
       " ('has_device_protection', 'has_multiple_lines'),\n",
       " ('has_device_protection', 'has_online_backup'),\n",
       " ('has_device_protection', 'has_online_security'),\n",
       " ('has_device_protection', 'has_phone_service'),\n",
       " ('has_device_protection', 'has_premium_tech_support'),\n",
       " ('has_device_protection', 'has_unlimited_data'),\n",
       " ('has_device_protection', 'internet_type'),\n",
       " ('has_device_protection', 'married'),\n",
       " ('has_device_protection', 'paperless_billing'),\n",
       " ('has_device_protection', 'payment_method'),\n",
       " ('has_device_protection', 'senior_citizen'),\n",
       " ('has_device_protection', 'stream_movie'),\n",
       " ('has_device_protection', 'stream_music'),\n",
       " ('has_device_protection', 'stream_tv'),\n",
       " ('has_device_protection', 'zip_code'),\n",
       " ('has_internet_service', 'contract_type'),\n",
       " ('has_internet_service', 'gender'),\n",
       " ('has_internet_service', 'has_device_protection'),\n",
       " ('has_internet_service', 'has_internet_service'),\n",
       " ('has_internet_service', 'has_multiple_lines'),\n",
       " ('has_internet_service', 'has_online_backup'),\n",
       " ('has_internet_service', 'has_online_security'),\n",
       " ('has_internet_service', 'has_phone_service'),\n",
       " ('has_internet_service', 'has_premium_tech_support'),\n",
       " ('has_internet_service', 'has_unlimited_data'),\n",
       " ('has_internet_service', 'internet_type'),\n",
       " ('has_internet_service', 'married'),\n",
       " ('has_internet_service', 'paperless_billing'),\n",
       " ('has_internet_service', 'payment_method'),\n",
       " ('has_internet_service', 'senior_citizen'),\n",
       " ('has_internet_service', 'stream_movie'),\n",
       " ('has_internet_service', 'stream_music'),\n",
       " ('has_internet_service', 'stream_tv'),\n",
       " ('has_internet_service', 'zip_code'),\n",
       " ('has_multiple_lines', 'contract_type'),\n",
       " ('has_multiple_lines', 'gender'),\n",
       " ('has_multiple_lines', 'has_device_protection'),\n",
       " ('has_multiple_lines', 'has_internet_service'),\n",
       " ('has_multiple_lines', 'has_multiple_lines'),\n",
       " ('has_multiple_lines', 'has_online_backup'),\n",
       " ('has_multiple_lines', 'has_online_security'),\n",
       " ('has_multiple_lines', 'has_phone_service'),\n",
       " ('has_multiple_lines', 'has_premium_tech_support'),\n",
       " ('has_multiple_lines', 'has_unlimited_data'),\n",
       " ('has_multiple_lines', 'internet_type'),\n",
       " ('has_multiple_lines', 'married'),\n",
       " ('has_multiple_lines', 'paperless_billing'),\n",
       " ('has_multiple_lines', 'payment_method'),\n",
       " ('has_multiple_lines', 'senior_citizen'),\n",
       " ('has_multiple_lines', 'stream_movie'),\n",
       " ('has_multiple_lines', 'stream_music'),\n",
       " ('has_multiple_lines', 'stream_tv'),\n",
       " ('has_multiple_lines', 'zip_code'),\n",
       " ('has_online_backup', 'contract_type'),\n",
       " ('has_online_backup', 'gender'),\n",
       " ('has_online_backup', 'has_device_protection'),\n",
       " ('has_online_backup', 'has_internet_service'),\n",
       " ('has_online_backup', 'has_multiple_lines'),\n",
       " ('has_online_backup', 'has_online_backup'),\n",
       " ('has_online_backup', 'has_online_security'),\n",
       " ('has_online_backup', 'has_phone_service'),\n",
       " ('has_online_backup', 'has_premium_tech_support'),\n",
       " ('has_online_backup', 'has_unlimited_data'),\n",
       " ('has_online_backup', 'internet_type'),\n",
       " ('has_online_backup', 'married'),\n",
       " ('has_online_backup', 'paperless_billing'),\n",
       " ('has_online_backup', 'payment_method'),\n",
       " ('has_online_backup', 'senior_citizen'),\n",
       " ('has_online_backup', 'stream_movie'),\n",
       " ('has_online_backup', 'stream_music'),\n",
       " ('has_online_backup', 'stream_tv'),\n",
       " ('has_online_backup', 'zip_code'),\n",
       " ('has_online_security', 'contract_type'),\n",
       " ('has_online_security', 'gender'),\n",
       " ('has_online_security', 'has_device_protection'),\n",
       " ('has_online_security', 'has_internet_service'),\n",
       " ('has_online_security', 'has_multiple_lines'),\n",
       " ('has_online_security', 'has_online_backup'),\n",
       " ('has_online_security', 'has_online_security'),\n",
       " ('has_online_security', 'has_phone_service'),\n",
       " ('has_online_security', 'has_premium_tech_support'),\n",
       " ('has_online_security', 'has_unlimited_data'),\n",
       " ('has_online_security', 'internet_type'),\n",
       " ('has_online_security', 'married'),\n",
       " ('has_online_security', 'paperless_billing'),\n",
       " ('has_online_security', 'payment_method'),\n",
       " ('has_online_security', 'senior_citizen'),\n",
       " ('has_online_security', 'stream_movie'),\n",
       " ('has_online_security', 'stream_music'),\n",
       " ('has_online_security', 'stream_tv'),\n",
       " ('has_online_security', 'zip_code'),\n",
       " ('has_phone_service', 'contract_type'),\n",
       " ('has_phone_service', 'gender'),\n",
       " ('has_phone_service', 'has_device_protection'),\n",
       " ('has_phone_service', 'has_internet_service'),\n",
       " ('has_phone_service', 'has_multiple_lines'),\n",
       " ('has_phone_service', 'has_online_backup'),\n",
       " ('has_phone_service', 'has_online_security'),\n",
       " ('has_phone_service', 'has_phone_service'),\n",
       " ('has_phone_service', 'has_premium_tech_support'),\n",
       " ('has_phone_service', 'has_unlimited_data'),\n",
       " ('has_phone_service', 'internet_type'),\n",
       " ('has_phone_service', 'married'),\n",
       " ('has_phone_service', 'paperless_billing'),\n",
       " ('has_phone_service', 'payment_method'),\n",
       " ('has_phone_service', 'senior_citizen'),\n",
       " ('has_phone_service', 'stream_movie'),\n",
       " ('has_phone_service', 'stream_music'),\n",
       " ('has_phone_service', 'stream_tv'),\n",
       " ('has_phone_service', 'zip_code'),\n",
       " ('has_premium_tech_support', 'contract_type'),\n",
       " ('has_premium_tech_support', 'gender'),\n",
       " ('has_premium_tech_support', 'has_device_protection'),\n",
       " ('has_premium_tech_support', 'has_internet_service'),\n",
       " ('has_premium_tech_support', 'has_multiple_lines'),\n",
       " ('has_premium_tech_support', 'has_online_backup'),\n",
       " ('has_premium_tech_support', 'has_online_security'),\n",
       " ('has_premium_tech_support', 'has_phone_service'),\n",
       " ('has_premium_tech_support', 'has_premium_tech_support'),\n",
       " ('has_premium_tech_support', 'has_unlimited_data'),\n",
       " ('has_premium_tech_support', 'internet_type'),\n",
       " ('has_premium_tech_support', 'married'),\n",
       " ('has_premium_tech_support', 'paperless_billing'),\n",
       " ('has_premium_tech_support', 'payment_method'),\n",
       " ('has_premium_tech_support', 'senior_citizen'),\n",
       " ('has_premium_tech_support', 'stream_movie'),\n",
       " ('has_premium_tech_support', 'stream_music'),\n",
       " ('has_premium_tech_support', 'stream_tv'),\n",
       " ('has_premium_tech_support', 'zip_code'),\n",
       " ('has_unlimited_data', 'contract_type'),\n",
       " ('has_unlimited_data', 'gender'),\n",
       " ('has_unlimited_data', 'has_device_protection'),\n",
       " ('has_unlimited_data', 'has_internet_service'),\n",
       " ('has_unlimited_data', 'has_multiple_lines'),\n",
       " ('has_unlimited_data', 'has_online_backup'),\n",
       " ('has_unlimited_data', 'has_online_security'),\n",
       " ('has_unlimited_data', 'has_phone_service'),\n",
       " ('has_unlimited_data', 'has_premium_tech_support'),\n",
       " ('has_unlimited_data', 'has_unlimited_data'),\n",
       " ('has_unlimited_data', 'internet_type'),\n",
       " ('has_unlimited_data', 'married'),\n",
       " ('has_unlimited_data', 'paperless_billing'),\n",
       " ('has_unlimited_data', 'payment_method'),\n",
       " ('has_unlimited_data', 'senior_citizen'),\n",
       " ('has_unlimited_data', 'stream_movie'),\n",
       " ('has_unlimited_data', 'stream_music'),\n",
       " ('has_unlimited_data', 'stream_tv'),\n",
       " ('has_unlimited_data', 'zip_code'),\n",
       " ('internet_type', 'contract_type'),\n",
       " ('internet_type', 'gender'),\n",
       " ('internet_type', 'has_device_protection'),\n",
       " ('internet_type', 'has_internet_service'),\n",
       " ('internet_type', 'has_multiple_lines'),\n",
       " ('internet_type', 'has_online_backup'),\n",
       " ('internet_type', 'has_online_security'),\n",
       " ('internet_type', 'has_phone_service'),\n",
       " ('internet_type', 'has_premium_tech_support'),\n",
       " ('internet_type', 'has_unlimited_data'),\n",
       " ('internet_type', 'internet_type'),\n",
       " ('internet_type', 'married'),\n",
       " ('internet_type', 'paperless_billing'),\n",
       " ('internet_type', 'payment_method'),\n",
       " ('internet_type', 'senior_citizen'),\n",
       " ('internet_type', 'stream_movie'),\n",
       " ('internet_type', 'stream_music'),\n",
       " ('internet_type', 'stream_tv'),\n",
       " ('internet_type', 'zip_code'),\n",
       " ('married', 'contract_type'),\n",
       " ('married', 'gender'),\n",
       " ('married', 'has_device_protection'),\n",
       " ('married', 'has_internet_service'),\n",
       " ('married', 'has_multiple_lines'),\n",
       " ('married', 'has_online_backup'),\n",
       " ('married', 'has_online_security'),\n",
       " ('married', 'has_phone_service'),\n",
       " ('married', 'has_premium_tech_support'),\n",
       " ('married', 'has_unlimited_data'),\n",
       " ('married', 'internet_type'),\n",
       " ('married', 'married'),\n",
       " ('married', 'paperless_billing'),\n",
       " ('married', 'payment_method'),\n",
       " ('married', 'senior_citizen'),\n",
       " ('married', 'stream_movie'),\n",
       " ('married', 'stream_music'),\n",
       " ('married', 'stream_tv'),\n",
       " ('married', 'zip_code'),\n",
       " ('paperless_billing', 'contract_type'),\n",
       " ('paperless_billing', 'gender'),\n",
       " ('paperless_billing', 'has_device_protection'),\n",
       " ('paperless_billing', 'has_internet_service'),\n",
       " ('paperless_billing', 'has_multiple_lines'),\n",
       " ('paperless_billing', 'has_online_backup'),\n",
       " ('paperless_billing', 'has_online_security'),\n",
       " ('paperless_billing', 'has_phone_service'),\n",
       " ('paperless_billing', 'has_premium_tech_support'),\n",
       " ('paperless_billing', 'has_unlimited_data'),\n",
       " ('paperless_billing', 'internet_type'),\n",
       " ('paperless_billing', 'married'),\n",
       " ('paperless_billing', 'paperless_billing'),\n",
       " ('paperless_billing', 'payment_method'),\n",
       " ('paperless_billing', 'senior_citizen'),\n",
       " ('paperless_billing', 'stream_movie'),\n",
       " ('paperless_billing', 'stream_music'),\n",
       " ('paperless_billing', 'stream_tv'),\n",
       " ('paperless_billing', 'zip_code'),\n",
       " ('payment_method', 'contract_type'),\n",
       " ('payment_method', 'gender'),\n",
       " ('payment_method', 'has_device_protection'),\n",
       " ('payment_method', 'has_internet_service'),\n",
       " ('payment_method', 'has_multiple_lines'),\n",
       " ('payment_method', 'has_online_backup'),\n",
       " ('payment_method', 'has_online_security'),\n",
       " ('payment_method', 'has_phone_service'),\n",
       " ('payment_method', 'has_premium_tech_support'),\n",
       " ('payment_method', 'has_unlimited_data'),\n",
       " ('payment_method', 'internet_type'),\n",
       " ('payment_method', 'married'),\n",
       " ('payment_method', 'paperless_billing'),\n",
       " ('payment_method', 'payment_method'),\n",
       " ('payment_method', 'senior_citizen'),\n",
       " ('payment_method', 'stream_movie'),\n",
       " ('payment_method', 'stream_music'),\n",
       " ('payment_method', 'stream_tv'),\n",
       " ('payment_method', 'zip_code'),\n",
       " ('senior_citizen', 'contract_type'),\n",
       " ('senior_citizen', 'gender'),\n",
       " ('senior_citizen', 'has_device_protection'),\n",
       " ('senior_citizen', 'has_internet_service'),\n",
       " ('senior_citizen', 'has_multiple_lines'),\n",
       " ('senior_citizen', 'has_online_backup'),\n",
       " ('senior_citizen', 'has_online_security'),\n",
       " ('senior_citizen', 'has_phone_service'),\n",
       " ('senior_citizen', 'has_premium_tech_support'),\n",
       " ('senior_citizen', 'has_unlimited_data'),\n",
       " ('senior_citizen', 'internet_type'),\n",
       " ('senior_citizen', 'married'),\n",
       " ('senior_citizen', 'paperless_billing'),\n",
       " ('senior_citizen', 'payment_method'),\n",
       " ('senior_citizen', 'senior_citizen'),\n",
       " ('senior_citizen', 'stream_movie'),\n",
       " ('senior_citizen', 'stream_music'),\n",
       " ('senior_citizen', 'stream_tv'),\n",
       " ('senior_citizen', 'zip_code'),\n",
       " ('stream_movie', 'contract_type'),\n",
       " ('stream_movie', 'gender'),\n",
       " ('stream_movie', 'has_device_protection'),\n",
       " ('stream_movie', 'has_internet_service'),\n",
       " ('stream_movie', 'has_multiple_lines'),\n",
       " ('stream_movie', 'has_online_backup'),\n",
       " ('stream_movie', 'has_online_security'),\n",
       " ('stream_movie', 'has_phone_service'),\n",
       " ('stream_movie', 'has_premium_tech_support'),\n",
       " ('stream_movie', 'has_unlimited_data'),\n",
       " ('stream_movie', 'internet_type'),\n",
       " ('stream_movie', 'married'),\n",
       " ('stream_movie', 'paperless_billing'),\n",
       " ('stream_movie', 'payment_method'),\n",
       " ('stream_movie', 'senior_citizen'),\n",
       " ('stream_movie', 'stream_movie'),\n",
       " ('stream_movie', 'stream_music'),\n",
       " ('stream_movie', 'stream_tv'),\n",
       " ('stream_movie', 'zip_code'),\n",
       " ('stream_music', 'contract_type'),\n",
       " ('stream_music', 'gender'),\n",
       " ('stream_music', 'has_device_protection'),\n",
       " ('stream_music', 'has_internet_service'),\n",
       " ('stream_music', 'has_multiple_lines'),\n",
       " ('stream_music', 'has_online_backup'),\n",
       " ('stream_music', 'has_online_security'),\n",
       " ('stream_music', 'has_phone_service'),\n",
       " ('stream_music', 'has_premium_tech_support'),\n",
       " ('stream_music', 'has_unlimited_data'),\n",
       " ('stream_music', 'internet_type'),\n",
       " ('stream_music', 'married'),\n",
       " ('stream_music', 'paperless_billing'),\n",
       " ('stream_music', 'payment_method'),\n",
       " ('stream_music', 'senior_citizen'),\n",
       " ('stream_music', 'stream_movie'),\n",
       " ('stream_music', 'stream_music'),\n",
       " ('stream_music', 'stream_tv'),\n",
       " ('stream_music', 'zip_code'),\n",
       " ('stream_tv', 'contract_type'),\n",
       " ('stream_tv', 'gender'),\n",
       " ('stream_tv', 'has_device_protection'),\n",
       " ('stream_tv', 'has_internet_service'),\n",
       " ('stream_tv', 'has_multiple_lines'),\n",
       " ('stream_tv', 'has_online_backup'),\n",
       " ('stream_tv', 'has_online_security'),\n",
       " ('stream_tv', 'has_phone_service'),\n",
       " ('stream_tv', 'has_premium_tech_support'),\n",
       " ('stream_tv', 'has_unlimited_data'),\n",
       " ('stream_tv', 'internet_type'),\n",
       " ('stream_tv', 'married'),\n",
       " ('stream_tv', 'paperless_billing'),\n",
       " ('stream_tv', 'payment_method'),\n",
       " ('stream_tv', 'senior_citizen'),\n",
       " ('stream_tv', 'stream_movie'),\n",
       " ('stream_tv', 'stream_music'),\n",
       " ('stream_tv', 'stream_tv'),\n",
       " ('stream_tv', 'zip_code'),\n",
       " ('zip_code', 'contract_type'),\n",
       " ('zip_code', 'gender'),\n",
       " ('zip_code', 'has_device_protection'),\n",
       " ('zip_code', 'has_internet_service'),\n",
       " ('zip_code', 'has_multiple_lines'),\n",
       " ('zip_code', 'has_online_backup'),\n",
       " ('zip_code', 'has_online_security'),\n",
       " ('zip_code', 'has_phone_service'),\n",
       " ('zip_code', 'has_premium_tech_support'),\n",
       " ('zip_code', 'has_unlimited_data'),\n",
       " ('zip_code', 'internet_type'),\n",
       " ('zip_code', 'married'),\n",
       " ('zip_code', 'paperless_billing'),\n",
       " ('zip_code', 'payment_method'),\n",
       " ('zip_code', 'senior_citizen'),\n",
       " ('zip_code', 'stream_movie'),\n",
       " ('zip_code', 'stream_music'),\n",
       " ('zip_code', 'stream_tv'),\n",
       " ('zip_code', 'zip_code')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create 2 list of cat variables\n",
    "col_cat1 = df[cat_cols_eda].columns\n",
    "col_cat2 = df[cat_cols_eda].columns.sort_values()\n",
    "\n",
    "cat_combi = list(product(col_cat1, col_cat2, repeat = 1))\n",
    "cat_combi_1 = set(cat_combi) \n",
    "cat_combi_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "married has_multiple_lines\n",
      "payment_method married\n",
      "zip_code paperless_billing\n",
      "stream_music payment_method\n",
      "has_multiple_lines has_device_protection\n",
      "married stream_tv\n",
      "contract_type has_internet_service\n",
      "zip_code has_premium_tech_support\n",
      "stream_music married\n",
      "has_online_backup internet_type\n",
      "stream_music stream_movie\n",
      "stream_tv contract_type\n",
      "has_internet_service internet_type\n",
      "married has_unlimited_data\n",
      "has_phone_service stream_music\n",
      "paperless_billing has_online_backup\n",
      "has_device_protection internet_type\n",
      "senior_citizen married\n",
      "zip_code gender\n",
      "stream_movie stream_tv\n",
      "senior_citizen stream_movie\n",
      "has_phone_service contract_type\n",
      "gender senior_citizen\n",
      "has_online_backup senior_citizen\n",
      "stream_movie has_online_security\n",
      "gender has_phone_service\n",
      "stream_tv payment_method\n",
      "stream_movie has_unlimited_data\n",
      "has_online_backup zip_code\n",
      "has_online_backup has_phone_service\n",
      "has_internet_service senior_citizen\n",
      "gender has_premium_tech_support\n",
      "has_online_backup has_multiple_lines\n",
      "has_internet_service zip_code\n",
      "stream_tv married\n",
      "has_internet_service has_phone_service\n",
      "has_unlimited_data stream_tv\n",
      "stream_movie stream_music\n",
      "has_internet_service has_multiple_lines\n",
      "stream_music has_internet_service\n",
      "stream_tv stream_movie\n",
      "has_premium_tech_support payment_method\n",
      "has_device_protection senior_citizen\n",
      "zip_code stream_music\n",
      "has_multiple_lines internet_type\n",
      "has_device_protection zip_code\n",
      "has_online_security paperless_billing\n",
      "has_unlimited_data has_online_security\n",
      "contract_type stream_tv\n",
      "has_premium_tech_support married\n",
      "has_phone_service payment_method\n",
      "has_device_protection has_multiple_lines\n",
      "has_online_security has_premium_tech_support\n",
      "payment_method paperless_billing\n",
      "has_premium_tech_support stream_movie\n",
      "internet_type zip_code\n",
      "has_device_protection stream_tv\n",
      "payment_method has_premium_tech_support\n",
      "has_internet_service has_unlimited_data\n",
      "has_unlimited_data stream_music\n",
      "internet_type has_multiple_lines\n",
      "contract_type has_online_security\n",
      "senior_citizen has_internet_service\n",
      "has_phone_service stream_movie\n",
      "has_phone_service has_device_protection\n",
      "zip_code contract_type\n",
      "internet_type stream_tv\n",
      "has_device_protection has_unlimited_data\n",
      "contract_type stream_music\n",
      "gender has_online_backup\n",
      "paperless_billing married\n",
      "has_online_security gender\n",
      "internet_type has_unlimited_data\n",
      "paperless_billing stream_movie\n",
      "has_multiple_lines zip_code\n",
      "payment_method gender\n",
      "stream_tv has_internet_service\n",
      "has_multiple_lines stream_tv\n",
      "stream_music gender\n",
      "zip_code payment_method\n",
      "has_premium_tech_support has_internet_service\n",
      "stream_movie has_device_protection\n",
      "has_multiple_lines has_unlimited_data\n",
      "zip_code stream_movie\n",
      "has_phone_service has_internet_service\n",
      "has_online_security stream_music\n",
      "senior_citizen gender\n",
      "stream_music has_online_security\n",
      "payment_method stream_music\n",
      "has_unlimited_data has_device_protection\n",
      "has_phone_service internet_type\n",
      "paperless_billing has_internet_service\n",
      "has_online_security contract_type\n",
      "contract_type has_device_protection\n",
      "payment_method contract_type\n",
      "senior_citizen has_online_security\n",
      "stream_tv gender\n",
      "stream_music contract_type\n",
      "gender married\n",
      "senior_citizen stream_music\n",
      "has_online_backup married\n",
      "zip_code has_internet_service\n",
      "has_premium_tech_support gender\n",
      "has_online_security payment_method\n",
      "stream_tv has_online_security\n",
      "has_phone_service zip_code\n",
      "married senior_citizen\n",
      "senior_citizen contract_type\n",
      "has_phone_service has_multiple_lines\n",
      "married paperless_billing\n",
      "zip_code internet_type\n",
      "married has_phone_service\n",
      "has_phone_service stream_tv\n",
      "stream_tv stream_music\n",
      "has_online_security stream_movie\n",
      "married has_premium_tech_support\n",
      "has_premium_tech_support has_online_security\n",
      "payment_method stream_movie\n",
      "payment_method has_device_protection\n",
      "paperless_billing gender\n",
      "has_phone_service has_online_security\n",
      "has_unlimited_data internet_type\n",
      "has_phone_service has_unlimited_data\n",
      "has_premium_tech_support stream_music\n",
      "stream_music has_device_protection\n",
      "stream_movie senior_citizen\n",
      "contract_type internet_type\n",
      "senior_citizen payment_method\n",
      "stream_movie zip_code\n",
      "stream_movie has_phone_service\n",
      "paperless_billing has_online_security\n",
      "stream_movie has_multiple_lines\n",
      "has_premium_tech_support contract_type\n",
      "senior_citizen has_device_protection\n",
      "zip_code has_multiple_lines\n",
      "paperless_billing stream_music\n",
      "married has_online_backup\n",
      "zip_code stream_tv\n",
      "has_unlimited_data senior_citizen\n",
      "has_online_security has_internet_service\n",
      "gender paperless_billing\n",
      "has_unlimited_data zip_code\n",
      "has_unlimited_data has_phone_service\n",
      "has_online_backup paperless_billing\n",
      "payment_method has_internet_service\n",
      "has_unlimited_data has_multiple_lines\n",
      "has_online_backup has_premium_tech_support\n",
      "zip_code has_online_security\n",
      "has_internet_service paperless_billing\n",
      "contract_type senior_citizen\n",
      "zip_code has_unlimited_data\n",
      "paperless_billing contract_type\n",
      "contract_type zip_code\n",
      "has_internet_service has_premium_tech_support\n",
      "contract_type has_phone_service\n",
      "stream_tv has_device_protection\n",
      "has_online_security internet_type\n",
      "contract_type has_multiple_lines\n",
      "has_device_protection paperless_billing\n",
      "has_device_protection has_phone_service\n",
      "payment_method internet_type\n",
      "has_device_protection has_premium_tech_support\n",
      "internet_type senior_citizen\n",
      "stream_movie has_online_backup\n",
      "internet_type paperless_billing\n",
      "has_premium_tech_support has_device_protection\n",
      "internet_type has_phone_service\n",
      "stream_music internet_type\n",
      "internet_type has_premium_tech_support\n",
      "has_online_backup gender\n",
      "contract_type has_unlimited_data\n",
      "paperless_billing payment_method\n",
      "senior_citizen internet_type\n",
      "has_multiple_lines senior_citizen\n",
      "gender has_online_security\n",
      "has_internet_service has_online_backup\n",
      "has_multiple_lines paperless_billing\n",
      "paperless_billing has_device_protection\n",
      "has_multiple_lines has_phone_service\n",
      "payment_method zip_code\n",
      "has_multiple_lines has_premium_tech_support\n",
      "married payment_method\n",
      "has_online_security stream_tv\n",
      "payment_method has_multiple_lines\n",
      "gender stream_music\n",
      "has_device_protection has_online_backup\n",
      "payment_method stream_tv\n",
      "stream_music zip_code\n",
      "stream_music has_multiple_lines\n",
      "married stream_movie\n",
      "internet_type has_online_backup\n",
      "has_online_security has_unlimited_data\n",
      "stream_tv internet_type\n",
      "stream_music stream_tv\n",
      "payment_method has_online_security\n",
      "zip_code has_device_protection\n",
      "payment_method has_unlimited_data\n",
      "gender contract_type\n",
      "has_online_backup contract_type\n",
      "senior_citizen zip_code\n",
      "has_internet_service contract_type\n",
      "has_premium_tech_support internet_type\n",
      "stream_music has_unlimited_data\n",
      "senior_citizen has_multiple_lines\n",
      "senior_citizen stream_tv\n",
      "stream_movie married\n",
      "has_multiple_lines has_online_backup\n",
      "stream_tv senior_citizen\n",
      "senior_citizen has_unlimited_data\n",
      "gender payment_method\n",
      "stream_tv zip_code\n",
      "stream_tv has_phone_service\n",
      "has_online_backup payment_method\n",
      "stream_tv has_multiple_lines\n",
      "paperless_billing internet_type\n",
      "married has_internet_service\n",
      "has_internet_service payment_method\n",
      "has_unlimited_data married\n",
      "has_premium_tech_support senior_citizen\n",
      "gender stream_movie\n",
      "gender has_device_protection\n",
      "has_premium_tech_support zip_code\n",
      "has_online_backup stream_movie\n",
      "has_internet_service married\n",
      "has_phone_service senior_citizen\n",
      "has_premium_tech_support has_multiple_lines\n",
      "has_internet_service stream_movie\n",
      "has_device_protection payment_method\n",
      "has_premium_tech_support stream_tv\n",
      "stream_tv has_unlimited_data\n",
      "has_device_protection married\n",
      "has_device_protection stream_movie\n",
      "has_online_security has_device_protection\n",
      "internet_type married\n",
      "has_premium_tech_support has_unlimited_data\n",
      "internet_type stream_movie\n",
      "paperless_billing zip_code\n",
      "paperless_billing has_multiple_lines\n",
      "paperless_billing stream_tv\n",
      "gender has_internet_service\n",
      "stream_movie paperless_billing\n",
      "married gender\n",
      "has_online_backup has_internet_service\n",
      "zip_code senior_citizen\n",
      "has_multiple_lines married\n",
      "stream_movie has_premium_tech_support\n",
      "paperless_billing has_unlimited_data\n",
      "zip_code has_phone_service\n",
      "has_multiple_lines stream_movie\n",
      "has_phone_service has_online_backup\n",
      "gender internet_type\n",
      "has_device_protection has_internet_service\n",
      "has_unlimited_data paperless_billing\n",
      "married has_online_security\n",
      "has_unlimited_data has_premium_tech_support\n",
      "internet_type has_internet_service\n",
      "stream_movie gender\n",
      "contract_type paperless_billing\n",
      "married stream_music\n",
      "contract_type has_premium_tech_support\n",
      "married contract_type\n",
      "has_unlimited_data gender\n",
      "gender zip_code\n",
      "zip_code has_online_backup\n",
      "has_multiple_lines has_internet_service\n",
      "gender has_multiple_lines\n",
      "has_internet_service gender\n",
      "gender stream_tv\n",
      "contract_type gender\n",
      "has_online_backup stream_tv\n",
      "has_unlimited_data has_online_backup\n",
      "has_internet_service stream_tv\n",
      "has_online_security senior_citizen\n",
      "has_device_protection gender\n",
      "has_online_security zip_code\n",
      "has_online_security has_phone_service\n",
      "payment_method senior_citizen\n",
      "gender has_unlimited_data\n",
      "has_online_backup has_online_security\n",
      "has_online_security has_multiple_lines\n",
      "has_online_backup has_unlimited_data\n",
      "internet_type gender\n",
      "contract_type has_online_backup\n",
      "payment_method has_phone_service\n",
      "has_internet_service has_online_security\n",
      "stream_movie contract_type\n",
      "has_phone_service married\n",
      "stream_music senior_citizen\n",
      "has_online_backup stream_music\n",
      "stream_music paperless_billing\n",
      "stream_music has_phone_service\n",
      "has_internet_service stream_music\n",
      "stream_music has_premium_tech_support\n",
      "has_device_protection has_online_security\n",
      "married has_device_protection\n",
      "internet_type has_online_security\n",
      "has_unlimited_data contract_type\n",
      "has_device_protection stream_music\n",
      "has_multiple_lines gender\n",
      "senior_citizen paperless_billing\n",
      "senior_citizen has_phone_service\n",
      "internet_type stream_music\n",
      "stream_movie payment_method\n",
      "senior_citizen has_premium_tech_support\n",
      "has_online_security has_online_backup\n",
      "has_device_protection contract_type\n",
      "payment_method has_online_backup\n",
      "zip_code married\n",
      "has_multiple_lines has_online_security\n",
      "internet_type contract_type\n",
      "stream_tv paperless_billing\n",
      "has_unlimited_data payment_method\n",
      "stream_music has_online_backup\n",
      "stream_tv has_premium_tech_support\n",
      "has_multiple_lines stream_music\n",
      "contract_type payment_method\n",
      "has_unlimited_data stream_movie\n",
      "has_premium_tech_support paperless_billing\n",
      "has_online_backup has_device_protection\n",
      "has_premium_tech_support has_phone_service\n",
      "contract_type married\n",
      "has_internet_service has_device_protection\n",
      "has_phone_service paperless_billing\n",
      "senior_citizen has_online_backup\n",
      "married internet_type\n",
      "contract_type stream_movie\n",
      "has_multiple_lines contract_type\n",
      "has_phone_service has_premium_tech_support\n",
      "internet_type payment_method\n",
      "stream_movie has_internet_service\n",
      "paperless_billing senior_citizen\n",
      "internet_type has_device_protection\n",
      "paperless_billing has_phone_service\n",
      "paperless_billing has_premium_tech_support\n",
      "stream_tv has_online_backup\n",
      "has_phone_service gender\n",
      "stream_movie internet_type\n",
      "has_multiple_lines payment_method\n",
      "has_unlimited_data has_internet_service\n",
      "has_online_security married\n",
      "married zip_code\n",
      "has_premium_tech_support has_online_backup\n"
     ]
    }
   ],
   "source": [
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x,y)\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r-((r-1)**2)/(n-1)\n",
    "    kcorr = k-((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in cat_combi_1:\n",
    "    if i[0] != i[1]:\n",
    "        print(i[0], i[1])\n",
    "        result.append((i[0],i[1], cramers_v(df[i[0]], df[i[1]])))\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "var2: %{x}<br>var1: %{y}<br>color: %{z}<extra></extra>",
         "name": "0",
         "texttemplate": "%{z}",
         "type": "heatmap",
         "x": [
          "contract_type",
          "gender",
          "has_device_protection",
          "has_internet_service",
          "has_multiple_lines",
          "has_online_backup",
          "has_online_security",
          "has_phone_service",
          "has_premium_tech_support",
          "has_unlimited_data",
          "internet_type",
          "married",
          "paperless_billing",
          "payment_method",
          "senior_citizen",
          "stream_movie",
          "stream_music",
          "stream_tv",
          "zip_code"
         ],
         "xaxis": "x",
         "y": [
          "contract_type",
          "gender",
          "has_device_protection",
          "has_internet_service",
          "has_multiple_lines",
          "has_online_backup",
          "has_online_security",
          "has_phone_service",
          "has_premium_tech_support",
          "has_unlimited_data",
          "internet_type",
          "married",
          "paperless_billing",
          "payment_method",
          "senior_citizen",
          "stream_movie",
          "stream_music",
          "stream_tv",
          "zip_code"
         ],
         "yaxis": "y",
         "z": [
          [
           null,
           0,
           0.23,
           0.2,
           0.12,
           0.17,
           0.24,
           0,
           0.27,
           0.14,
           0.16,
           0.28,
           0.15,
           0.12,
           0.03,
           0.13,
           0.09,
           0.12,
           0.01
          ],
          [
           0,
           null,
           0,
           0,
           0,
           0.01,
           0.01,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.03
          ],
          [
           0.23,
           0,
           null,
           0.38,
           0.2,
           0.3,
           0.27,
           0.07,
           0.33,
           0.3,
           0.38,
           0.15,
           0.1,
           0.08,
           0.06,
           0.4,
           0.35,
           0.39,
           0
          ],
          [
           0.2,
           0,
           0.38,
           null,
           0.21,
           0.38,
           0.33,
           0.17,
           0.34,
           0.76,
           1,
           0,
           0.32,
           0.27,
           0.18,
           0.42,
           0.39,
           0.42,
           0.03
          ],
          [
           0.12,
           0,
           0.2,
           0.21,
           null,
           0.2,
           0.1,
           0.28,
           0.1,
           0.16,
           0.36,
           0.14,
           0.16,
           0.15,
           0.14,
           0.26,
           0.19,
           0.26,
           0.02
          ],
          [
           0.17,
           0.01,
           0.3,
           0.38,
           0.2,
           null,
           0.28,
           0.05,
           0.29,
           0.28,
           0.38,
           0.14,
           0.13,
           0.1,
           0.07,
           0.27,
           0.24,
           0.28,
           0.04
          ],
          [
           0.24,
           0.01,
           0.27,
           0.33,
           0.1,
           0.28,
           null,
           0.09,
           0.35,
           0.26,
           0.39,
           0.14,
           0,
           0.04,
           0.04,
           0.19,
           0.19,
           0.18,
           0
          ],
          [
           0,
           0,
           0.07,
           0.17,
           0.28,
           0.05,
           0.09,
           null,
           0.1,
           0.12,
           0.44,
           0.01,
           0.01,
           0.02,
           0,
           0.03,
           0.04,
           0.02,
           0
          ],
          [
           0.27,
           0,
           0.33,
           0.34,
           0.1,
           0.29,
           0.35,
           0.1,
           null,
           0.25,
           0.39,
           0.12,
           0.04,
           0.05,
           0.06,
           0.28,
           0.28,
           0.28,
           0
          ],
          [
           0.14,
           0,
           0.3,
           0.76,
           0.16,
           0.28,
           0.26,
           0.12,
           0.25,
           null,
           0.76,
           0.01,
           0.24,
           0.2,
           0.14,
           0.32,
           0.3,
           0.32,
           0.02
          ],
          [
           0.16,
           0,
           0.38,
           1,
           0.36,
           0.38,
           0.39,
           0.44,
           0.39,
           0.76,
           null,
           0,
           0.38,
           0.23,
           0.26,
           0.44,
           0.4,
           0.44,
           0.03
          ],
          [
           0.28,
           0,
           0.15,
           0,
           0.14,
           0.14,
           0.14,
           0.01,
           0.12,
           0.01,
           0,
           null,
           0.01,
           0.06,
           0.01,
           0.12,
           0.09,
           0.12,
           0.03
          ],
          [
           0.15,
           0,
           0.1,
           0.32,
           0.16,
           0.13,
           0,
           0.01,
           0.04,
           0.24,
           0.38,
           0.01,
           null,
           0.18,
           0.16,
           0.21,
           0.17,
           0.22,
           0.02
          ],
          [
           0.12,
           0,
           0.08,
           0.27,
           0.15,
           0.1,
           0.04,
           0.02,
           0.05,
           0.2,
           0.23,
           0.06,
           0.18,
           null,
           0.15,
           0.18,
           0.13,
           0.18,
           0.05
          ],
          [
           0.03,
           0,
           0.06,
           0.18,
           0.14,
           0.07,
           0.04,
           0,
           0.06,
           0.14,
           0.26,
           0.01,
           0.16,
           0.15,
           null,
           0.12,
           0.15,
           0.1,
           0
          ],
          [
           0.13,
           0,
           0.4,
           0.42,
           0.26,
           0.27,
           0.19,
           0.03,
           0.28,
           0.32,
           0.44,
           0.12,
           0.21,
           0.18,
           0.12,
           null,
           0.85,
           0.53,
           0
          ],
          [
           0.09,
           0,
           0.35,
           0.39,
           0.19,
           0.24,
           0.19,
           0.04,
           0.28,
           0.3,
           0.4,
           0.09,
           0.17,
           0.13,
           0.15,
           0.85,
           null,
           0.45,
           0
          ],
          [
           0.12,
           0,
           0.39,
           0.42,
           0.26,
           0.28,
           0.18,
           0.02,
           0.28,
           0.32,
           0.44,
           0.12,
           0.22,
           0.18,
           0.1,
           0.53,
           0.45,
           null,
           0.01
          ],
          [
           0.01,
           0.03,
           0,
           0.03,
           0.02,
           0.04,
           0,
           0,
           0,
           0.02,
           0.03,
           0.03,
           0.02,
           0.05,
           0,
           0,
           0,
           0.01,
           null
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "height": 750,
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "side": "top",
         "title": {
          "text": "var2"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "var1"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chitest = pd.DataFrame(result, columns = ['var1', 'var2', 'coeff'] )\n",
    "chitest_pvt = chitest.pivot(index='var1', columns='var2', values='coeff').round(2)\n",
    "fig_2 = px.imshow(chitest_pvt, color_continuous_scale = 'plasma', text_auto = True, aspect = 'auto')\n",
    "\n",
    "fig_2.update_xaxes(side=\"top\")\n",
    "fig_2.layout.height = 750\n",
    "fig.layout.width = 1000\n",
    "fig_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "From the heat map, we can deduce some collinearity: \n",
    "- has_internet_service ↔ internet_type\n",
    "- stream_movie ↔ stream_music\n",
    "- stream_movie ↔ stream_tv\n",
    "- has_internet_service ↔ has_unlimited_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "## a. Treatment to Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "churn_label\n",
       "No     5174\n",
       "Yes    1817\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(6991, 40)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove records with no churn label as customers deceased or moved should be out of the sample scope\n",
    "df_clean1 = df[df['churn_label'] != \"\"]\n",
    "\n",
    "display(df_clean1['churn_label'].value_counts())\n",
    "display(df_clean1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "churn_label\n",
       "1    5174\n",
       "0    5174\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As there are more \"No\" under churn_label, resulting in imbalanced data, increase the size of \"yes\"\n",
    "\n",
    "df_clean1_no = df_clean1[df_clean1['churn_label'] == 'No']\n",
    "df_clean1_yes = df_clean1[df_clean1['churn_label'] == 'Yes']\n",
    "\n",
    "df_upsampled = resample(df_clean1_yes, replace = True, n_samples = len(df_clean1_no), random_state= 1)\n",
    "\n",
    "df_clean2 = pd.concat([df_clean1_no, df_upsampled], axis = 0, ignore_index= True).sort_values(['customer_id', 'account_id'])\n",
    "df_clean3 = df_clean2.sample(frac = 1, random_state= 88).reset_index(drop = True)  \n",
    "\n",
    "df_clean3['churn_label'] = np.where(df_clean3['churn_label'] == \"Yes\", 1, 0)\n",
    "\n",
    "df_clean3['churn_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Treatment to Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "\n",
    "As some of the column values are \"Yes\" and \"No\", it will be cleaner to apply label encoding than one-hot encoding as it keeps the dataset's dimension the same as compared to having many more columns for model to study which slows down the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>senior_citizen</th>\n",
       "      <th>married</th>\n",
       "      <th>has_premium_tech_support</th>\n",
       "      <th>has_online_backup</th>\n",
       "      <th>has_device_protection</th>\n",
       "      <th>has_online_security</th>\n",
       "      <th>has_multiple_lines</th>\n",
       "      <th>stream_music</th>\n",
       "      <th>has_phone_service</th>\n",
       "      <th>has_internet_service</th>\n",
       "      <th>stream_movie</th>\n",
       "      <th>has_unlimited_data</th>\n",
       "      <th>paperless_billing</th>\n",
       "      <th>stream_tv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10343</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10344</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10345</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10346</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10347</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10348 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       senior_citizen  married  has_premium_tech_support  has_online_backup  \\\n",
       "0                   0        1                         0                  0   \n",
       "1                   0        0                         0                  0   \n",
       "2                   0        1                         1                  1   \n",
       "3                   1        0                         0                  0   \n",
       "4                   1        0                         1                  1   \n",
       "...               ...      ...                       ...                ...   \n",
       "10343               1        1                         0                  1   \n",
       "10344               0        1                         1                  0   \n",
       "10345               1        1                         0                  0   \n",
       "10346               0        0                         0                  0   \n",
       "10347               1        0                         0                  1   \n",
       "\n",
       "       has_device_protection  has_online_security  has_multiple_lines  \\\n",
       "0                          0                    0                   1   \n",
       "1                          1                    1                   0   \n",
       "2                          1                    1                   1   \n",
       "3                          0                    1                   0   \n",
       "4                          1                    0                   0   \n",
       "...                      ...                  ...                 ...   \n",
       "10343                      1                    0                   1   \n",
       "10344                      0                    0                   1   \n",
       "10345                      0                    0                   0   \n",
       "10346                      0                    1                   1   \n",
       "10347                      0                    0                   1   \n",
       "\n",
       "       stream_music  has_phone_service  has_internet_service  stream_movie  \\\n",
       "0                 1                  1                     1             1   \n",
       "1                 0                  1                     1             0   \n",
       "2                 1                  1                     1             1   \n",
       "3                 0                  1                     1             1   \n",
       "4                 0                  1                     1             1   \n",
       "...             ...                ...                   ...           ...   \n",
       "10343             1                  1                     1             1   \n",
       "10344             1                  1                     1             1   \n",
       "10345             0                  1                     1             0   \n",
       "10346             0                  1                     1             0   \n",
       "10347             0                  1                     1             0   \n",
       "\n",
       "       has_unlimited_data  paperless_billing  stream_tv  \n",
       "0                       1                  1          0  \n",
       "1                       1                  1          0  \n",
       "2                       1                  1          1  \n",
       "3                       1                  0          1  \n",
       "4                       1                  1          0  \n",
       "...                   ...                ...        ...  \n",
       "10343                   1                  1          1  \n",
       "10344                   1                  1          1  \n",
       "10345                   1                  1          0  \n",
       "10346                   1                  1          0  \n",
       "10347                   1                  1          0  \n",
       "\n",
       "[10348 rows x 14 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encode_cols = ['senior_citizen', 'married', 'has_premium_tech_support', 'has_online_backup', 'has_device_protection', 'has_online_security', 'has_multiple_lines', 'stream_music', 'has_phone_service', 'has_internet_service',\n",
    "                   'stream_movie', 'has_unlimited_data', 'paperless_billing', 'stream_tv']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for col in label_encode_cols:\n",
    "    df_clean3[col] = label_encoder.fit_transform(df_clean3[col])\n",
    "\n",
    "df_clean3[label_encode_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Treatment to Numeric Columns\n",
    "\n",
    "<b> Recap: </b><br>\n",
    "Based on the observations from EDA\n",
    "-  `age`, `num_dependent` & `num_referrals` to be created as bin features\n",
    "- `tenure_mth` to be switched to year to reduce unnecessary grannularity\n",
    "- `total_refund` to be switched to binary as it is usually whether user get refunds or not\n",
    "- `total_long_distance_fee` & `avg_gb_download_monthly` are highly correlated due to both measuring the same metric but of a different period, hence to drop `total_long_distance_fee`\n",
    "- Remaining numeric columns to undergo standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean3['num_dependents_bin'] = pd.cut(df_clean3['num_dependents'], \n",
    "                                         bins = [-1,0,1,max(df_clean3['num_dependents'])], \n",
    "                                         labels = [\"Nil\", \"1\", \"2 or more\"])\n",
    "\n",
    "df_clean3[df_clean3['num_dependents_bin'] == \"Nil\"][['num_dependents', 'num_dependents_bin']]['num_dependents'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 7, 8, 9, 10, 11]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bins are created based on the (i) min to mean; (ii) mean to 75th percentile and (iii) above 75th percentile\n",
    "df_clean3['num_referrals_bin'] = pd.cut(df_clean3['num_referrals'], bins = [-1,0,3,max(df_clean3['num_referrals'])], labels = [\"Nil\", \"1-3\", \"4 or more\"])\n",
    "\n",
    "sorted(df_clean3[df_clean3['num_referrals_bin'] == \"4 or more\"][['num_referrals', 'num_referrals_bin']]['num_referrals'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bins are created based on the respective percentile\n",
    "df_clean3['age_group'] = pd.cut(df_clean3['age'], bins = [min(df_clean3['age']-1),32, 46, 60, max(df_clean3['age'])], labels = [\"Young_Adult\", \"Middle_aged_Adult\", 'Older_Adults', 'Elderly'])\n",
    "\n",
    "sorted(df_clean3[df_clean3['age_group'] == \"Elderly\"][['age', 'age_group']]['age'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_refunds\n",
       "0    9635\n",
       "1     713\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean3['total_refunds'] = np.where(df_clean3['total_refunds'] == 0.00, 0, 1)\n",
    "df_clean3['total_refunds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tenure_year</th>\n",
       "      <th>tenure_months</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10343</th>\n",
       "      <td>4.0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10344</th>\n",
       "      <td>2.0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10345</th>\n",
       "      <td>5.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10346</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10347</th>\n",
       "      <td>5.0</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10348 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tenure_year  tenure_months\n",
       "0              1.0              8\n",
       "1              1.0             10\n",
       "2              6.0             72\n",
       "3              5.0             56\n",
       "4              1.0              2\n",
       "...            ...            ...\n",
       "10343          4.0             39\n",
       "10344          2.0             23\n",
       "10345          5.0             50\n",
       "10346          1.0              7\n",
       "10347          5.0             59\n",
       "\n",
       "[10348 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To use np.ceil() to round up as putting 0 on those with less than 6 mths tenure does not make sense\n",
    "df_clean3['tenure_year'] = np.ceil(df_clean3['tenure_months']/12)\n",
    "df_clean3[['tenure_year', 'tenure_months']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_clean3['total_charges_quarter_cbrt'] = np.cbrt(df_clean3['total_charges_quarter'])\n",
    "\n",
    "df_clean3[['total_charges_quarter', 'total_charges_quarter_cbrt']].fillna(\"\").hist(bins = 10, figsize=(10,5), grid = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.557420104905714"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_tenure.transform([[200]])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_monthly_fee</th>\n",
       "      <th>avg_gb_download_monthly</th>\n",
       "      <th>avg_long_distance_fee_monthly</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85.20</td>\n",
       "      <td>16</td>\n",
       "      <td>26.41</td>\n",
       "      <td>29847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79.85</td>\n",
       "      <td>42</td>\n",
       "      <td>48.38</td>\n",
       "      <td>31963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114.05</td>\n",
       "      <td>29</td>\n",
       "      <td>5.81</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94.80</td>\n",
       "      <td>28</td>\n",
       "      <td>10.57</td>\n",
       "      <td>4101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95.10</td>\n",
       "      <td>20</td>\n",
       "      <td>12.96</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10343</th>\n",
       "      <td>105.65</td>\n",
       "      <td>26</td>\n",
       "      <td>33.25</td>\n",
       "      <td>32804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10344</th>\n",
       "      <td>73.65</td>\n",
       "      <td>48</td>\n",
       "      <td>14.53</td>\n",
       "      <td>1107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10345</th>\n",
       "      <td>71.05</td>\n",
       "      <td>24</td>\n",
       "      <td>4.60</td>\n",
       "      <td>1745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10346</th>\n",
       "      <td>53.65</td>\n",
       "      <td>17</td>\n",
       "      <td>40.42</td>\n",
       "      <td>40077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10347</th>\n",
       "      <td>79.20</td>\n",
       "      <td>2</td>\n",
       "      <td>6.13</td>\n",
       "      <td>1007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10348 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_monthly_fee  avg_gb_download_monthly  \\\n",
       "0                  85.20                       16   \n",
       "1                  79.85                       42   \n",
       "2                 114.05                       29   \n",
       "3                  94.80                       28   \n",
       "4                  95.10                       20   \n",
       "...                  ...                      ...   \n",
       "10343             105.65                       26   \n",
       "10344              73.65                       48   \n",
       "10345              71.05                       24   \n",
       "10346              53.65                       17   \n",
       "10347              79.20                        2   \n",
       "\n",
       "       avg_long_distance_fee_monthly  population  \n",
       "0                              26.41       29847  \n",
       "1                              48.38       31963  \n",
       "2                               5.81         404  \n",
       "3                              10.57        4101  \n",
       "4                              12.96         282  \n",
       "...                              ...         ...  \n",
       "10343                          33.25       32804  \n",
       "10344                          14.53        1107  \n",
       "10345                           4.60        1745  \n",
       "10346                          40.42       40077  \n",
       "10347                           6.13        1007  \n",
       "\n",
       "[10348 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFTER\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_monthly_fee</th>\n",
       "      <th>avg_gb_download_monthly</th>\n",
       "      <th>avg_long_distance_fee_monthly</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.594205</td>\n",
       "      <td>-0.259884</td>\n",
       "      <td>0.211196</td>\n",
       "      <td>0.327516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.409508</td>\n",
       "      <td>1.049235</td>\n",
       "      <td>1.637442</td>\n",
       "      <td>0.426803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.590187</td>\n",
       "      <td>0.394675</td>\n",
       "      <td>-1.126113</td>\n",
       "      <td>-1.054007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.925623</td>\n",
       "      <td>0.344325</td>\n",
       "      <td>-0.817104</td>\n",
       "      <td>-0.880536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.935980</td>\n",
       "      <td>-0.058481</td>\n",
       "      <td>-0.661950</td>\n",
       "      <td>-1.059731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10343</th>\n",
       "      <td>1.300196</td>\n",
       "      <td>0.243623</td>\n",
       "      <td>0.655234</td>\n",
       "      <td>0.466264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10344</th>\n",
       "      <td>0.195467</td>\n",
       "      <td>1.351340</td>\n",
       "      <td>-0.560029</td>\n",
       "      <td>-1.021021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10345</th>\n",
       "      <td>0.105707</td>\n",
       "      <td>0.142922</td>\n",
       "      <td>-1.204664</td>\n",
       "      <td>-0.991084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10346</th>\n",
       "      <td>-0.494989</td>\n",
       "      <td>-0.209534</td>\n",
       "      <td>1.120696</td>\n",
       "      <td>0.807527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10347</th>\n",
       "      <td>0.387068</td>\n",
       "      <td>-0.964795</td>\n",
       "      <td>-1.105339</td>\n",
       "      <td>-1.025713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10348 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_monthly_fee  avg_gb_download_monthly  \\\n",
       "0               0.594205                -0.259884   \n",
       "1               0.409508                 1.049235   \n",
       "2               1.590187                 0.394675   \n",
       "3               0.925623                 0.344325   \n",
       "4               0.935980                -0.058481   \n",
       "...                  ...                      ...   \n",
       "10343           1.300196                 0.243623   \n",
       "10344           0.195467                 1.351340   \n",
       "10345           0.105707                 0.142922   \n",
       "10346          -0.494989                -0.209534   \n",
       "10347           0.387068                -0.964795   \n",
       "\n",
       "       avg_long_distance_fee_monthly  population  \n",
       "0                           0.211196    0.327516  \n",
       "1                           1.637442    0.426803  \n",
       "2                          -1.126113   -1.054007  \n",
       "3                          -0.817104   -0.880536  \n",
       "4                          -0.661950   -1.059731  \n",
       "...                              ...         ...  \n",
       "10343                       0.655234    0.466264  \n",
       "10344                      -0.560029   -1.021021  \n",
       "10345                      -1.204664   -0.991084  \n",
       "10346                       1.120696    0.807527  \n",
       "10347                      -1.105339   -1.025713  \n",
       "\n",
       "[10348 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "std_cols = ['total_monthly_fee', \n",
    "            'avg_gb_download_monthly', \n",
    "            'avg_long_distance_fee_monthly', 'population']\n",
    "df_clean4 = df_clean3.copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_clean4[std_cols] = scaler.fit_transform(df_clean4[std_cols])\n",
    "\n",
    "\n",
    "print(\"BEFORE\")\n",
    "display(df_clean3[std_cols])\n",
    "print(\"\\nAFTER\")\n",
    "display(df_clean4[std_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\models\\\\StdScalar_AvgMthlyLongDistFee.pkl']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler().fit(df_clean3[['total_monthly_fee', 'avg_gb_download_monthly', 'avg_long_distance_fee_monthly']])\n",
    "# scaler_tenure = StandardScaler().fit(df_clean3[['total_monthly_fee']])\n",
    "# scaler_gb_dl = StandardScaler().fit(df_clean3[['avg_gb_download_monthly']])\n",
    "# scaler_dist_fee = StandardScaler().fit(df_clean3[['avg_long_distance_fee_monthly']])\n",
    "\n",
    "joblib.dump(scaler, r'..\\models\\StdScalar.pkl')\n",
    "# joblib.dump(scaler_tenure, r'..\\models\\StdScalar_TenureMthFee.pkl')\n",
    "# joblib.dump(scaler_gb_dl, r'..\\models\\StdScalar_AvgMthlyGBDownload.pkl')\n",
    "# joblib.dump(scaler_dist_fee, r'..\\models\\StdScalar_AvgMthlyLongDistFee.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Unnecessary Columns\n",
    "\n",
    "<b><u>Reason(s) for Removal</b></u>\n",
    "- id-related columns: Does not play a part in determining the churn status\n",
    "- `churn_category`, `status`& `churn_reason`: An extension of the target variable\n",
    "- `age`, `num_referrals`, & `num_dependents`: Bin columns are created above for easy training of model\n",
    "- `city`, `latitude` & `longitude`: Simplified by using first 2 digits of zipcode and have high collinearity with zipcode\n",
    "- `total_charges_quarter`: Transform to `total_charges_quarter_cbrt` using cube root due to it being highly skewed\n",
    "- `total_long_distance_fee`: Due to high correlation with `avg_gb_download_monthly`\n",
    "- 'population`: Not a good and relevant question to ask user to find out from the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip_code</th>\n",
       "      <th>gender</th>\n",
       "      <th>senior_citizen</th>\n",
       "      <th>married</th>\n",
       "      <th>tenure_months</th>\n",
       "      <th>has_internet_service</th>\n",
       "      <th>internet_type</th>\n",
       "      <th>has_unlimited_data</th>\n",
       "      <th>has_phone_service</th>\n",
       "      <th>has_multiple_lines</th>\n",
       "      <th>has_premium_tech_support</th>\n",
       "      <th>has_online_security</th>\n",
       "      <th>has_online_backup</th>\n",
       "      <th>has_device_protection</th>\n",
       "      <th>contract_type</th>\n",
       "      <th>paperless_billing</th>\n",
       "      <th>payment_method</th>\n",
       "      <th>avg_long_distance_fee_monthly</th>\n",
       "      <th>avg_gb_download_monthly</th>\n",
       "      <th>stream_tv</th>\n",
       "      <th>stream_movie</th>\n",
       "      <th>stream_music</th>\n",
       "      <th>total_monthly_fee</th>\n",
       "      <th>total_refunds</th>\n",
       "      <th>churn_label</th>\n",
       "      <th>num_dependents_bin</th>\n",
       "      <th>num_referrals_bin</th>\n",
       "      <th>age_group</th>\n",
       "      <th>tenure_year</th>\n",
       "      <th>total_charges_quarter_cbrt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Fiber Optic</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Month-to-Month</td>\n",
       "      <td>1</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>0.211196</td>\n",
       "      <td>-0.259884</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.594205</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Young_Adult</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.560810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Fiber Optic</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Month-to-Month</td>\n",
       "      <td>1</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>1.637442</td>\n",
       "      <td>1.049235</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.409508</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Young_Adult</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.609445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>Fiber Optic</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Two Year</td>\n",
       "      <td>1</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>-1.126113</td>\n",
       "      <td>0.394675</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.590187</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4 or more</td>\n",
       "      <td>Older_Adults</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.238153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  zip_code  gender  senior_citizen  married  tenure_months  \\\n",
       "0       92  Female               0        1              8   \n",
       "1       90    Male               0        0             10   \n",
       "2       95    Male               0        1             72   \n",
       "\n",
       "   has_internet_service internet_type  has_unlimited_data  has_phone_service  \\\n",
       "0                     1   Fiber Optic                   1                  1   \n",
       "1                     1   Fiber Optic                   1                  1   \n",
       "2                     1   Fiber Optic                   1                  1   \n",
       "\n",
       "   has_multiple_lines  has_premium_tech_support  has_online_security  \\\n",
       "0                   1                         0                    0   \n",
       "1                   0                         0                    1   \n",
       "2                   1                         1                    1   \n",
       "\n",
       "   has_online_backup  has_device_protection   contract_type  \\\n",
       "0                  0                      0  Month-to-Month   \n",
       "1                  0                      1  Month-to-Month   \n",
       "2                  1                      1        Two Year   \n",
       "\n",
       "   paperless_billing payment_method  avg_long_distance_fee_monthly  \\\n",
       "0                  1    Credit Card                       0.211196   \n",
       "1                  1    Credit Card                       1.637442   \n",
       "2                  1    Credit Card                      -1.126113   \n",
       "\n",
       "   avg_gb_download_monthly  stream_tv  stream_movie  stream_music  \\\n",
       "0                -0.259884          0             1             1   \n",
       "1                 1.049235          0             0             0   \n",
       "2                 0.394675          1             1             1   \n",
       "\n",
       "   total_monthly_fee  total_refunds  churn_label num_dependents_bin  \\\n",
       "0           0.594205              0            1                Nil   \n",
       "1           0.409508              0            0                Nil   \n",
       "2           1.590187              0            0                  1   \n",
       "\n",
       "  num_referrals_bin     age_group  tenure_year  total_charges_quarter_cbrt  \n",
       "0               Nil   Young_Adult          1.0                    8.560810  \n",
       "1               Nil   Young_Adult          1.0                    9.609445  \n",
       "2         4 or more  Older_Adults          6.0                   20.238153  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_clean5 = df_clean4.copy()\n",
    "\n",
    "cols_remove = [\n",
    "    'customer_id', 'account_id', 'area_id',\n",
    "    'city', 'latitude', 'longitude',\n",
    "    'churn_category', 'churn_reason', 'status',\n",
    "    'age', 'num_referrals', 'num_dependents',\n",
    "    'total_long_distance_fee', 'total_charges_quarter', 'population'\n",
    "]\n",
    "\n",
    "\n",
    "df_clean5.drop(cols_remove, axis = 1, inplace = True)\n",
    "assert set(df_clean5.columns).isdisjoint(set(cols_remove)), display(set(df_clean5.columns) & set(cols_remove))\n",
    "display(df_clean5.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into features and target columns\n",
    "feat_selection = df_clean5.columns.tolist()\n",
    "feat_selection.remove(\"churn_label\")\n",
    "\n",
    "# Make the observations more randomised\n",
    "df_clean6 = shuffle(df_clean5, random_state = 1).reset_index()\n",
    "\n",
    "# Train-test Split\n",
    "X = df_clean6[feat_selection]\n",
    "Y = df_clean6['churn_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.027943\n",
      "0:\tlearn: 0.6746373\ttotal: 203ms\tremaining: 3m 22s\n",
      "1:\tlearn: 0.6580038\ttotal: 244ms\tremaining: 2m 1s\n",
      "2:\tlearn: 0.6425180\ttotal: 285ms\tremaining: 1m 34s\n",
      "3:\tlearn: 0.6253833\ttotal: 325ms\tremaining: 1m 21s\n",
      "4:\tlearn: 0.6102101\ttotal: 366ms\tremaining: 1m 12s\n",
      "5:\tlearn: 0.5953354\ttotal: 403ms\tremaining: 1m 6s\n",
      "6:\tlearn: 0.5831085\ttotal: 447ms\tremaining: 1m 3s\n",
      "7:\tlearn: 0.5714114\ttotal: 483ms\tremaining: 59.9s\n",
      "8:\tlearn: 0.5612882\ttotal: 525ms\tremaining: 57.8s\n",
      "9:\tlearn: 0.5517315\ttotal: 568ms\tremaining: 56.2s\n",
      "10:\tlearn: 0.5435145\ttotal: 611ms\tremaining: 54.9s\n",
      "11:\tlearn: 0.5348697\ttotal: 650ms\tremaining: 53.5s\n",
      "12:\tlearn: 0.5258007\ttotal: 689ms\tremaining: 52.3s\n",
      "13:\tlearn: 0.5182190\ttotal: 727ms\tremaining: 51.2s\n",
      "14:\tlearn: 0.5106137\ttotal: 769ms\tremaining: 50.5s\n",
      "15:\tlearn: 0.5040217\ttotal: 811ms\tremaining: 49.9s\n",
      "16:\tlearn: 0.4975098\ttotal: 850ms\tremaining: 49.2s\n",
      "17:\tlearn: 0.4920633\ttotal: 894ms\tremaining: 48.8s\n",
      "18:\tlearn: 0.4867695\ttotal: 934ms\tremaining: 48.2s\n",
      "19:\tlearn: 0.4817206\ttotal: 972ms\tremaining: 47.6s\n",
      "20:\tlearn: 0.4771978\ttotal: 1.01s\tremaining: 47s\n",
      "21:\tlearn: 0.4721206\ttotal: 1.04s\tremaining: 46.5s\n",
      "22:\tlearn: 0.4674809\ttotal: 1.11s\tremaining: 47.2s\n",
      "23:\tlearn: 0.4635466\ttotal: 1.16s\tremaining: 47s\n",
      "24:\tlearn: 0.4600177\ttotal: 1.2s\tremaining: 46.6s\n",
      "25:\tlearn: 0.4562942\ttotal: 1.24s\tremaining: 46.3s\n",
      "26:\tlearn: 0.4526984\ttotal: 1.27s\tremaining: 45.9s\n",
      "27:\tlearn: 0.4498051\ttotal: 1.31s\tremaining: 45.5s\n",
      "28:\tlearn: 0.4467148\ttotal: 1.35s\tremaining: 45.2s\n",
      "29:\tlearn: 0.4435399\ttotal: 1.39s\tremaining: 44.9s\n",
      "30:\tlearn: 0.4405847\ttotal: 1.43s\tremaining: 44.7s\n",
      "31:\tlearn: 0.4382501\ttotal: 1.47s\tremaining: 44.5s\n",
      "32:\tlearn: 0.4361625\ttotal: 1.51s\tremaining: 44.3s\n",
      "33:\tlearn: 0.4335603\ttotal: 1.55s\tremaining: 44.1s\n",
      "34:\tlearn: 0.4310229\ttotal: 1.6s\tremaining: 44.2s\n",
      "35:\tlearn: 0.4289191\ttotal: 1.64s\tremaining: 44s\n",
      "36:\tlearn: 0.4270801\ttotal: 1.69s\tremaining: 43.9s\n",
      "37:\tlearn: 0.4251123\ttotal: 1.72s\tremaining: 43.7s\n",
      "38:\tlearn: 0.4232887\ttotal: 1.76s\tremaining: 43.5s\n",
      "39:\tlearn: 0.4211993\ttotal: 1.8s\tremaining: 43.3s\n",
      "40:\tlearn: 0.4195713\ttotal: 1.84s\tremaining: 43.1s\n",
      "41:\tlearn: 0.4185730\ttotal: 1.87s\tremaining: 42.6s\n",
      "42:\tlearn: 0.4169446\ttotal: 1.91s\tremaining: 42.5s\n",
      "43:\tlearn: 0.4153911\ttotal: 1.96s\tremaining: 42.5s\n",
      "44:\tlearn: 0.4137868\ttotal: 2s\tremaining: 42.5s\n",
      "45:\tlearn: 0.4119013\ttotal: 2.06s\tremaining: 42.7s\n",
      "46:\tlearn: 0.4105884\ttotal: 2.1s\tremaining: 42.7s\n",
      "47:\tlearn: 0.4092403\ttotal: 2.15s\tremaining: 42.6s\n",
      "48:\tlearn: 0.4077636\ttotal: 2.18s\tremaining: 42.4s\n",
      "49:\tlearn: 0.4065147\ttotal: 2.22s\tremaining: 42.3s\n",
      "50:\tlearn: 0.4052492\ttotal: 2.27s\tremaining: 42.3s\n",
      "51:\tlearn: 0.4042348\ttotal: 2.31s\tremaining: 42.2s\n",
      "52:\tlearn: 0.4029056\ttotal: 2.35s\tremaining: 42.1s\n",
      "53:\tlearn: 0.4017905\ttotal: 2.4s\tremaining: 42s\n",
      "54:\tlearn: 0.4006930\ttotal: 2.45s\tremaining: 42s\n",
      "55:\tlearn: 0.3998277\ttotal: 2.49s\tremaining: 41.9s\n",
      "56:\tlearn: 0.3988918\ttotal: 2.56s\tremaining: 42.3s\n",
      "57:\tlearn: 0.3976596\ttotal: 2.6s\tremaining: 42.3s\n",
      "58:\tlearn: 0.3968991\ttotal: 2.65s\tremaining: 42.2s\n",
      "59:\tlearn: 0.3958939\ttotal: 2.69s\tremaining: 42.1s\n",
      "60:\tlearn: 0.3947882\ttotal: 2.73s\tremaining: 42.1s\n",
      "61:\tlearn: 0.3937804\ttotal: 2.78s\tremaining: 42s\n",
      "62:\tlearn: 0.3925677\ttotal: 2.81s\tremaining: 41.9s\n",
      "63:\tlearn: 0.3915322\ttotal: 2.85s\tremaining: 41.8s\n",
      "64:\tlearn: 0.3906508\ttotal: 2.9s\tremaining: 41.7s\n",
      "65:\tlearn: 0.3899561\ttotal: 2.94s\tremaining: 41.6s\n",
      "66:\tlearn: 0.3890078\ttotal: 2.98s\tremaining: 41.5s\n",
      "67:\tlearn: 0.3880634\ttotal: 3.02s\tremaining: 41.4s\n",
      "68:\tlearn: 0.3874813\ttotal: 3.06s\tremaining: 41.3s\n",
      "69:\tlearn: 0.3866466\ttotal: 3.1s\tremaining: 41.2s\n",
      "70:\tlearn: 0.3860792\ttotal: 3.15s\tremaining: 41.3s\n",
      "71:\tlearn: 0.3854427\ttotal: 3.2s\tremaining: 41.3s\n",
      "72:\tlearn: 0.3847201\ttotal: 3.24s\tremaining: 41.2s\n",
      "73:\tlearn: 0.3841139\ttotal: 3.28s\tremaining: 41.1s\n",
      "74:\tlearn: 0.3832435\ttotal: 3.32s\tremaining: 41s\n",
      "75:\tlearn: 0.3823793\ttotal: 3.36s\tremaining: 40.9s\n",
      "76:\tlearn: 0.3818413\ttotal: 3.4s\tremaining: 40.8s\n",
      "77:\tlearn: 0.3811378\ttotal: 3.45s\tremaining: 40.8s\n",
      "78:\tlearn: 0.3804704\ttotal: 3.49s\tremaining: 40.6s\n",
      "79:\tlearn: 0.3799219\ttotal: 3.53s\tremaining: 40.6s\n",
      "80:\tlearn: 0.3790410\ttotal: 3.57s\tremaining: 40.5s\n",
      "81:\tlearn: 0.3784943\ttotal: 3.61s\tremaining: 40.4s\n",
      "82:\tlearn: 0.3779078\ttotal: 3.66s\tremaining: 40.4s\n",
      "83:\tlearn: 0.3773814\ttotal: 3.7s\tremaining: 40.3s\n",
      "84:\tlearn: 0.3769088\ttotal: 3.74s\tremaining: 40.2s\n",
      "85:\tlearn: 0.3764001\ttotal: 3.78s\tremaining: 40.2s\n",
      "86:\tlearn: 0.3757317\ttotal: 3.82s\tremaining: 40.1s\n",
      "87:\tlearn: 0.3751694\ttotal: 3.86s\tremaining: 40s\n",
      "88:\tlearn: 0.3746376\ttotal: 3.9s\tremaining: 40s\n",
      "89:\tlearn: 0.3740236\ttotal: 3.94s\tremaining: 39.9s\n",
      "90:\tlearn: 0.3733281\ttotal: 3.98s\tremaining: 39.8s\n",
      "91:\tlearn: 0.3727774\ttotal: 4.02s\tremaining: 39.7s\n",
      "92:\tlearn: 0.3721091\ttotal: 4.06s\tremaining: 39.6s\n",
      "93:\tlearn: 0.3717635\ttotal: 4.11s\tremaining: 39.6s\n",
      "94:\tlearn: 0.3710776\ttotal: 4.15s\tremaining: 39.5s\n",
      "95:\tlearn: 0.3705727\ttotal: 4.19s\tremaining: 39.5s\n",
      "96:\tlearn: 0.3700122\ttotal: 4.23s\tremaining: 39.4s\n",
      "97:\tlearn: 0.3694552\ttotal: 4.28s\tremaining: 39.4s\n",
      "98:\tlearn: 0.3689382\ttotal: 4.32s\tremaining: 39.4s\n",
      "99:\tlearn: 0.3682664\ttotal: 4.37s\tremaining: 39.3s\n",
      "100:\tlearn: 0.3680786\ttotal: 4.4s\tremaining: 39.1s\n",
      "101:\tlearn: 0.3673385\ttotal: 4.43s\tremaining: 39.1s\n",
      "102:\tlearn: 0.3668256\ttotal: 4.48s\tremaining: 39s\n",
      "103:\tlearn: 0.3665765\ttotal: 4.52s\tremaining: 39s\n",
      "104:\tlearn: 0.3658519\ttotal: 4.58s\tremaining: 39s\n",
      "105:\tlearn: 0.3654373\ttotal: 4.62s\tremaining: 39s\n",
      "106:\tlearn: 0.3651205\ttotal: 4.66s\tremaining: 38.9s\n",
      "107:\tlearn: 0.3646209\ttotal: 4.71s\tremaining: 38.9s\n",
      "108:\tlearn: 0.3640645\ttotal: 4.75s\tremaining: 38.8s\n",
      "109:\tlearn: 0.3636273\ttotal: 4.79s\tremaining: 38.8s\n",
      "110:\tlearn: 0.3631361\ttotal: 4.83s\tremaining: 38.7s\n",
      "111:\tlearn: 0.3626374\ttotal: 4.87s\tremaining: 38.6s\n",
      "112:\tlearn: 0.3621745\ttotal: 4.92s\tremaining: 38.6s\n",
      "113:\tlearn: 0.3617755\ttotal: 4.97s\tremaining: 38.7s\n",
      "114:\tlearn: 0.3613248\ttotal: 5.02s\tremaining: 38.6s\n",
      "115:\tlearn: 0.3608199\ttotal: 5.06s\tremaining: 38.6s\n",
      "116:\tlearn: 0.3604360\ttotal: 5.1s\tremaining: 38.5s\n",
      "117:\tlearn: 0.3599446\ttotal: 5.14s\tremaining: 38.5s\n",
      "118:\tlearn: 0.3596774\ttotal: 5.19s\tremaining: 38.4s\n",
      "119:\tlearn: 0.3594602\ttotal: 5.23s\tremaining: 38.3s\n",
      "120:\tlearn: 0.3590219\ttotal: 5.27s\tremaining: 38.3s\n",
      "121:\tlearn: 0.3586315\ttotal: 5.31s\tremaining: 38.2s\n",
      "122:\tlearn: 0.3583615\ttotal: 5.35s\tremaining: 38.1s\n",
      "123:\tlearn: 0.3579309\ttotal: 5.39s\tremaining: 38.1s\n",
      "124:\tlearn: 0.3575171\ttotal: 5.45s\tremaining: 38.1s\n",
      "125:\tlearn: 0.3571020\ttotal: 5.49s\tremaining: 38.1s\n",
      "126:\tlearn: 0.3567071\ttotal: 5.54s\tremaining: 38.1s\n",
      "127:\tlearn: 0.3564711\ttotal: 5.58s\tremaining: 38s\n",
      "128:\tlearn: 0.3561114\ttotal: 5.62s\tremaining: 38s\n",
      "129:\tlearn: 0.3557053\ttotal: 5.67s\tremaining: 37.9s\n",
      "130:\tlearn: 0.3552270\ttotal: 5.71s\tremaining: 37.9s\n",
      "131:\tlearn: 0.3552236\ttotal: 5.73s\tremaining: 37.7s\n",
      "132:\tlearn: 0.3548046\ttotal: 5.78s\tremaining: 37.7s\n",
      "133:\tlearn: 0.3543658\ttotal: 5.82s\tremaining: 37.6s\n",
      "134:\tlearn: 0.3542951\ttotal: 5.84s\tremaining: 37.4s\n",
      "135:\tlearn: 0.3538194\ttotal: 5.88s\tremaining: 37.4s\n",
      "136:\tlearn: 0.3535592\ttotal: 5.93s\tremaining: 37.3s\n",
      "137:\tlearn: 0.3533873\ttotal: 5.97s\tremaining: 37.3s\n",
      "138:\tlearn: 0.3530973\ttotal: 6s\tremaining: 37.2s\n",
      "139:\tlearn: 0.3527866\ttotal: 6.04s\tremaining: 37.1s\n",
      "140:\tlearn: 0.3522659\ttotal: 6.08s\tremaining: 37.1s\n",
      "141:\tlearn: 0.3518927\ttotal: 6.13s\tremaining: 37s\n",
      "142:\tlearn: 0.3516338\ttotal: 6.17s\tremaining: 37s\n",
      "143:\tlearn: 0.3513659\ttotal: 6.21s\tremaining: 36.9s\n",
      "144:\tlearn: 0.3509710\ttotal: 6.25s\tremaining: 36.9s\n",
      "145:\tlearn: 0.3506602\ttotal: 6.3s\tremaining: 36.9s\n",
      "146:\tlearn: 0.3502941\ttotal: 6.35s\tremaining: 36.8s\n",
      "147:\tlearn: 0.3499841\ttotal: 6.39s\tremaining: 36.8s\n",
      "148:\tlearn: 0.3497144\ttotal: 6.43s\tremaining: 36.7s\n",
      "149:\tlearn: 0.3495828\ttotal: 6.47s\tremaining: 36.7s\n",
      "150:\tlearn: 0.3492858\ttotal: 6.51s\tremaining: 36.6s\n",
      "151:\tlearn: 0.3489818\ttotal: 6.55s\tremaining: 36.6s\n",
      "152:\tlearn: 0.3487931\ttotal: 6.6s\tremaining: 36.5s\n",
      "153:\tlearn: 0.3482342\ttotal: 6.64s\tremaining: 36.5s\n",
      "154:\tlearn: 0.3478909\ttotal: 6.68s\tremaining: 36.4s\n",
      "155:\tlearn: 0.3476374\ttotal: 6.72s\tremaining: 36.3s\n",
      "156:\tlearn: 0.3472847\ttotal: 6.76s\tremaining: 36.3s\n",
      "157:\tlearn: 0.3470238\ttotal: 6.79s\tremaining: 36.2s\n",
      "158:\tlearn: 0.3465940\ttotal: 6.83s\tremaining: 36.1s\n",
      "159:\tlearn: 0.3463902\ttotal: 6.87s\tremaining: 36.1s\n",
      "160:\tlearn: 0.3459895\ttotal: 6.91s\tremaining: 36s\n",
      "161:\tlearn: 0.3459742\ttotal: 6.94s\tremaining: 35.9s\n",
      "162:\tlearn: 0.3458391\ttotal: 6.98s\tremaining: 35.9s\n",
      "163:\tlearn: 0.3456135\ttotal: 7.02s\tremaining: 35.8s\n",
      "164:\tlearn: 0.3453217\ttotal: 7.07s\tremaining: 35.8s\n",
      "165:\tlearn: 0.3449563\ttotal: 7.11s\tremaining: 35.7s\n",
      "166:\tlearn: 0.3447408\ttotal: 7.15s\tremaining: 35.7s\n",
      "167:\tlearn: 0.3444391\ttotal: 7.19s\tremaining: 35.6s\n",
      "168:\tlearn: 0.3442669\ttotal: 7.23s\tremaining: 35.5s\n",
      "169:\tlearn: 0.3440680\ttotal: 7.27s\tremaining: 35.5s\n",
      "170:\tlearn: 0.3439043\ttotal: 7.31s\tremaining: 35.4s\n",
      "171:\tlearn: 0.3435664\ttotal: 7.35s\tremaining: 35.4s\n",
      "172:\tlearn: 0.3432438\ttotal: 7.39s\tremaining: 35.4s\n",
      "173:\tlearn: 0.3428828\ttotal: 7.43s\tremaining: 35.3s\n",
      "174:\tlearn: 0.3426491\ttotal: 7.47s\tremaining: 35.2s\n",
      "175:\tlearn: 0.3422848\ttotal: 7.51s\tremaining: 35.2s\n",
      "176:\tlearn: 0.3420762\ttotal: 7.56s\tremaining: 35.1s\n",
      "177:\tlearn: 0.3417984\ttotal: 7.6s\tremaining: 35.1s\n",
      "178:\tlearn: 0.3415287\ttotal: 7.64s\tremaining: 35.1s\n",
      "179:\tlearn: 0.3414685\ttotal: 7.68s\tremaining: 35s\n",
      "180:\tlearn: 0.3412948\ttotal: 7.72s\tremaining: 34.9s\n",
      "181:\tlearn: 0.3409400\ttotal: 7.76s\tremaining: 34.9s\n",
      "182:\tlearn: 0.3408224\ttotal: 7.8s\tremaining: 34.8s\n",
      "183:\tlearn: 0.3405827\ttotal: 7.85s\tremaining: 34.8s\n",
      "184:\tlearn: 0.3403098\ttotal: 7.89s\tremaining: 34.8s\n",
      "185:\tlearn: 0.3400722\ttotal: 7.93s\tremaining: 34.7s\n",
      "186:\tlearn: 0.3397094\ttotal: 7.97s\tremaining: 34.7s\n",
      "187:\tlearn: 0.3393471\ttotal: 8.02s\tremaining: 34.6s\n",
      "188:\tlearn: 0.3391603\ttotal: 8.06s\tremaining: 34.6s\n",
      "189:\tlearn: 0.3390580\ttotal: 8.1s\tremaining: 34.5s\n",
      "190:\tlearn: 0.3388626\ttotal: 8.14s\tremaining: 34.5s\n",
      "191:\tlearn: 0.3387310\ttotal: 8.18s\tremaining: 34.4s\n",
      "192:\tlearn: 0.3386035\ttotal: 8.23s\tremaining: 34.4s\n",
      "193:\tlearn: 0.3384352\ttotal: 8.27s\tremaining: 34.4s\n",
      "194:\tlearn: 0.3380154\ttotal: 8.31s\tremaining: 34.3s\n",
      "195:\tlearn: 0.3377309\ttotal: 8.35s\tremaining: 34.3s\n",
      "196:\tlearn: 0.3372988\ttotal: 8.39s\tremaining: 34.2s\n",
      "197:\tlearn: 0.3370881\ttotal: 8.44s\tremaining: 34.2s\n",
      "198:\tlearn: 0.3367573\ttotal: 8.48s\tremaining: 34.1s\n",
      "199:\tlearn: 0.3365813\ttotal: 8.53s\tremaining: 34.1s\n",
      "200:\tlearn: 0.3365028\ttotal: 8.57s\tremaining: 34.1s\n",
      "201:\tlearn: 0.3362775\ttotal: 8.61s\tremaining: 34s\n",
      "202:\tlearn: 0.3359592\ttotal: 8.66s\tremaining: 34s\n",
      "203:\tlearn: 0.3357766\ttotal: 8.7s\tremaining: 33.9s\n",
      "204:\tlearn: 0.3355336\ttotal: 8.73s\tremaining: 33.9s\n",
      "205:\tlearn: 0.3354209\ttotal: 8.77s\tremaining: 33.8s\n",
      "206:\tlearn: 0.3352224\ttotal: 8.81s\tremaining: 33.8s\n",
      "207:\tlearn: 0.3349179\ttotal: 8.85s\tremaining: 33.7s\n",
      "208:\tlearn: 0.3345474\ttotal: 8.89s\tremaining: 33.7s\n",
      "209:\tlearn: 0.3342862\ttotal: 8.93s\tremaining: 33.6s\n",
      "210:\tlearn: 0.3341377\ttotal: 8.97s\tremaining: 33.5s\n",
      "211:\tlearn: 0.3339886\ttotal: 9.01s\tremaining: 33.5s\n",
      "212:\tlearn: 0.3336792\ttotal: 9.05s\tremaining: 33.4s\n",
      "213:\tlearn: 0.3334210\ttotal: 9.09s\tremaining: 33.4s\n",
      "214:\tlearn: 0.3331448\ttotal: 9.13s\tremaining: 33.3s\n",
      "215:\tlearn: 0.3329313\ttotal: 9.17s\tremaining: 33.3s\n",
      "216:\tlearn: 0.3326673\ttotal: 9.21s\tremaining: 33.2s\n",
      "217:\tlearn: 0.3324512\ttotal: 9.24s\tremaining: 33.2s\n",
      "218:\tlearn: 0.3320867\ttotal: 9.29s\tremaining: 33.1s\n",
      "219:\tlearn: 0.3319458\ttotal: 9.32s\tremaining: 33.1s\n",
      "220:\tlearn: 0.3316396\ttotal: 9.36s\tremaining: 33s\n",
      "221:\tlearn: 0.3316208\ttotal: 9.38s\tremaining: 32.9s\n",
      "222:\tlearn: 0.3314869\ttotal: 9.42s\tremaining: 32.8s\n",
      "223:\tlearn: 0.3312457\ttotal: 9.46s\tremaining: 32.8s\n",
      "224:\tlearn: 0.3310018\ttotal: 9.49s\tremaining: 32.7s\n",
      "225:\tlearn: 0.3308616\ttotal: 9.53s\tremaining: 32.6s\n",
      "226:\tlearn: 0.3308371\ttotal: 9.55s\tremaining: 32.5s\n",
      "227:\tlearn: 0.3307060\ttotal: 9.59s\tremaining: 32.5s\n",
      "228:\tlearn: 0.3305142\ttotal: 9.63s\tremaining: 32.4s\n",
      "229:\tlearn: 0.3304640\ttotal: 9.65s\tremaining: 32.3s\n",
      "230:\tlearn: 0.3303672\ttotal: 9.69s\tremaining: 32.2s\n",
      "231:\tlearn: 0.3301641\ttotal: 9.72s\tremaining: 32.2s\n",
      "232:\tlearn: 0.3300947\ttotal: 9.76s\tremaining: 32.1s\n",
      "233:\tlearn: 0.3299105\ttotal: 9.8s\tremaining: 32.1s\n",
      "234:\tlearn: 0.3295607\ttotal: 9.84s\tremaining: 32s\n",
      "235:\tlearn: 0.3292041\ttotal: 9.88s\tremaining: 32s\n",
      "236:\tlearn: 0.3289426\ttotal: 9.92s\tremaining: 31.9s\n",
      "237:\tlearn: 0.3289410\ttotal: 9.94s\tremaining: 31.8s\n",
      "238:\tlearn: 0.3287329\ttotal: 9.98s\tremaining: 31.8s\n",
      "239:\tlearn: 0.3285234\ttotal: 10s\tremaining: 31.7s\n",
      "240:\tlearn: 0.3283472\ttotal: 10.1s\tremaining: 31.7s\n",
      "241:\tlearn: 0.3278910\ttotal: 10.1s\tremaining: 31.6s\n",
      "242:\tlearn: 0.3276547\ttotal: 10.1s\tremaining: 31.6s\n",
      "243:\tlearn: 0.3276151\ttotal: 10.2s\tremaining: 31.5s\n",
      "244:\tlearn: 0.3273823\ttotal: 10.2s\tremaining: 31.5s\n",
      "245:\tlearn: 0.3272743\ttotal: 10.3s\tremaining: 31.4s\n",
      "246:\tlearn: 0.3272027\ttotal: 10.3s\tremaining: 31.4s\n",
      "247:\tlearn: 0.3270389\ttotal: 10.3s\tremaining: 31.3s\n",
      "248:\tlearn: 0.3269858\ttotal: 10.4s\tremaining: 31.3s\n",
      "249:\tlearn: 0.3269079\ttotal: 10.4s\tremaining: 31.2s\n",
      "250:\tlearn: 0.3267486\ttotal: 10.4s\tremaining: 31.2s\n",
      "251:\tlearn: 0.3266039\ttotal: 10.5s\tremaining: 31.1s\n",
      "252:\tlearn: 0.3263982\ttotal: 10.5s\tremaining: 31.1s\n",
      "253:\tlearn: 0.3261056\ttotal: 10.6s\tremaining: 31s\n",
      "254:\tlearn: 0.3257918\ttotal: 10.6s\tremaining: 31s\n",
      "255:\tlearn: 0.3254573\ttotal: 10.6s\tremaining: 30.9s\n",
      "256:\tlearn: 0.3251862\ttotal: 10.7s\tremaining: 30.9s\n",
      "257:\tlearn: 0.3248592\ttotal: 10.7s\tremaining: 30.8s\n",
      "258:\tlearn: 0.3244616\ttotal: 10.8s\tremaining: 30.8s\n",
      "259:\tlearn: 0.3243656\ttotal: 10.8s\tremaining: 30.7s\n",
      "260:\tlearn: 0.3241501\ttotal: 10.8s\tremaining: 30.7s\n",
      "261:\tlearn: 0.3240250\ttotal: 10.9s\tremaining: 30.6s\n",
      "262:\tlearn: 0.3236999\ttotal: 10.9s\tremaining: 30.6s\n",
      "263:\tlearn: 0.3236867\ttotal: 10.9s\tremaining: 30.5s\n",
      "264:\tlearn: 0.3234669\ttotal: 11s\tremaining: 30.5s\n",
      "265:\tlearn: 0.3232179\ttotal: 11s\tremaining: 30.4s\n",
      "266:\tlearn: 0.3231103\ttotal: 11.1s\tremaining: 30.4s\n",
      "267:\tlearn: 0.3228750\ttotal: 11.1s\tremaining: 30.3s\n",
      "268:\tlearn: 0.3227466\ttotal: 11.1s\tremaining: 30.3s\n",
      "269:\tlearn: 0.3225839\ttotal: 11.2s\tremaining: 30.2s\n",
      "270:\tlearn: 0.3223906\ttotal: 11.2s\tremaining: 30.2s\n",
      "271:\tlearn: 0.3222001\ttotal: 11.3s\tremaining: 30.1s\n",
      "272:\tlearn: 0.3220898\ttotal: 11.3s\tremaining: 30.1s\n",
      "273:\tlearn: 0.3218951\ttotal: 11.4s\tremaining: 30.1s\n",
      "274:\tlearn: 0.3218274\ttotal: 11.4s\tremaining: 30.1s\n",
      "275:\tlearn: 0.3216608\ttotal: 11.5s\tremaining: 30.1s\n",
      "276:\tlearn: 0.3216095\ttotal: 11.5s\tremaining: 30s\n",
      "277:\tlearn: 0.3212561\ttotal: 11.6s\tremaining: 30s\n",
      "278:\tlearn: 0.3210134\ttotal: 11.6s\tremaining: 30s\n",
      "279:\tlearn: 0.3208062\ttotal: 11.6s\tremaining: 29.9s\n",
      "280:\tlearn: 0.3205527\ttotal: 11.7s\tremaining: 29.9s\n",
      "281:\tlearn: 0.3203748\ttotal: 11.7s\tremaining: 29.8s\n",
      "282:\tlearn: 0.3200127\ttotal: 11.8s\tremaining: 29.8s\n",
      "283:\tlearn: 0.3198550\ttotal: 11.8s\tremaining: 29.7s\n",
      "284:\tlearn: 0.3196907\ttotal: 11.8s\tremaining: 29.7s\n",
      "285:\tlearn: 0.3194746\ttotal: 11.9s\tremaining: 29.7s\n",
      "286:\tlearn: 0.3191538\ttotal: 11.9s\tremaining: 29.6s\n",
      "287:\tlearn: 0.3190084\ttotal: 12s\tremaining: 29.6s\n",
      "288:\tlearn: 0.3188896\ttotal: 12s\tremaining: 29.6s\n",
      "289:\tlearn: 0.3186679\ttotal: 12.1s\tremaining: 29.5s\n",
      "290:\tlearn: 0.3186662\ttotal: 12.1s\tremaining: 29.4s\n",
      "291:\tlearn: 0.3185452\ttotal: 12.1s\tremaining: 29.4s\n",
      "292:\tlearn: 0.3183670\ttotal: 12.2s\tremaining: 29.3s\n",
      "293:\tlearn: 0.3181799\ttotal: 12.2s\tremaining: 29.3s\n",
      "294:\tlearn: 0.3177476\ttotal: 12.2s\tremaining: 29.3s\n",
      "295:\tlearn: 0.3176106\ttotal: 12.3s\tremaining: 29.2s\n",
      "296:\tlearn: 0.3174064\ttotal: 12.3s\tremaining: 29.2s\n",
      "297:\tlearn: 0.3173451\ttotal: 12.4s\tremaining: 29.1s\n",
      "298:\tlearn: 0.3171844\ttotal: 12.4s\tremaining: 29.1s\n",
      "299:\tlearn: 0.3171837\ttotal: 12.4s\tremaining: 29s\n",
      "300:\tlearn: 0.3170354\ttotal: 12.5s\tremaining: 29s\n",
      "301:\tlearn: 0.3167653\ttotal: 12.5s\tremaining: 28.9s\n",
      "302:\tlearn: 0.3166566\ttotal: 12.6s\tremaining: 28.9s\n",
      "303:\tlearn: 0.3165003\ttotal: 12.6s\tremaining: 28.9s\n",
      "304:\tlearn: 0.3163964\ttotal: 12.7s\tremaining: 28.9s\n",
      "305:\tlearn: 0.3161153\ttotal: 12.7s\tremaining: 28.8s\n",
      "306:\tlearn: 0.3159095\ttotal: 12.8s\tremaining: 28.8s\n",
      "307:\tlearn: 0.3156670\ttotal: 12.8s\tremaining: 28.8s\n",
      "308:\tlearn: 0.3153308\ttotal: 12.8s\tremaining: 28.7s\n",
      "309:\tlearn: 0.3152542\ttotal: 12.9s\tremaining: 28.7s\n",
      "310:\tlearn: 0.3150238\ttotal: 12.9s\tremaining: 28.6s\n",
      "311:\tlearn: 0.3147900\ttotal: 13s\tremaining: 28.6s\n",
      "312:\tlearn: 0.3146157\ttotal: 13s\tremaining: 28.6s\n",
      "313:\tlearn: 0.3144980\ttotal: 13.1s\tremaining: 28.5s\n",
      "314:\tlearn: 0.3144529\ttotal: 13.1s\tremaining: 28.5s\n",
      "315:\tlearn: 0.3141344\ttotal: 13.1s\tremaining: 28.4s\n",
      "316:\tlearn: 0.3138022\ttotal: 13.2s\tremaining: 28.4s\n",
      "317:\tlearn: 0.3137727\ttotal: 13.2s\tremaining: 28.4s\n",
      "318:\tlearn: 0.3133934\ttotal: 13.3s\tremaining: 28.3s\n",
      "319:\tlearn: 0.3131823\ttotal: 13.3s\tremaining: 28.3s\n",
      "320:\tlearn: 0.3131456\ttotal: 13.4s\tremaining: 28.2s\n",
      "321:\tlearn: 0.3131456\ttotal: 13.4s\tremaining: 28.2s\n",
      "322:\tlearn: 0.3130771\ttotal: 13.4s\tremaining: 28.1s\n",
      "323:\tlearn: 0.3128735\ttotal: 13.5s\tremaining: 28.1s\n",
      "324:\tlearn: 0.3127296\ttotal: 13.5s\tremaining: 28s\n",
      "325:\tlearn: 0.3125707\ttotal: 13.5s\tremaining: 28s\n",
      "326:\tlearn: 0.3122804\ttotal: 13.6s\tremaining: 28s\n",
      "327:\tlearn: 0.3120709\ttotal: 13.6s\tremaining: 27.9s\n",
      "328:\tlearn: 0.3118289\ttotal: 13.7s\tremaining: 27.9s\n",
      "329:\tlearn: 0.3115756\ttotal: 13.7s\tremaining: 27.8s\n",
      "330:\tlearn: 0.3113932\ttotal: 13.7s\tremaining: 27.8s\n",
      "331:\tlearn: 0.3112230\ttotal: 13.8s\tremaining: 27.7s\n",
      "332:\tlearn: 0.3112134\ttotal: 13.8s\tremaining: 27.7s\n",
      "333:\tlearn: 0.3109507\ttotal: 13.9s\tremaining: 27.6s\n",
      "334:\tlearn: 0.3107620\ttotal: 13.9s\tremaining: 27.6s\n",
      "335:\tlearn: 0.3105131\ttotal: 14s\tremaining: 27.6s\n",
      "336:\tlearn: 0.3103251\ttotal: 14s\tremaining: 27.5s\n",
      "337:\tlearn: 0.3100544\ttotal: 14s\tremaining: 27.5s\n",
      "338:\tlearn: 0.3097901\ttotal: 14.1s\tremaining: 27.4s\n",
      "339:\tlearn: 0.3095363\ttotal: 14.1s\tremaining: 27.4s\n",
      "340:\tlearn: 0.3092558\ttotal: 14.2s\tremaining: 27.4s\n",
      "341:\tlearn: 0.3089757\ttotal: 14.2s\tremaining: 27.3s\n",
      "342:\tlearn: 0.3086497\ttotal: 14.2s\tremaining: 27.3s\n",
      "343:\tlearn: 0.3084443\ttotal: 14.3s\tremaining: 27.2s\n",
      "344:\tlearn: 0.3082419\ttotal: 14.3s\tremaining: 27.2s\n",
      "345:\tlearn: 0.3081073\ttotal: 14.4s\tremaining: 27.1s\n",
      "346:\tlearn: 0.3076891\ttotal: 14.4s\tremaining: 27.1s\n",
      "347:\tlearn: 0.3074651\ttotal: 14.4s\tremaining: 27s\n",
      "348:\tlearn: 0.3072794\ttotal: 14.5s\tremaining: 27s\n",
      "349:\tlearn: 0.3072550\ttotal: 14.5s\tremaining: 26.9s\n",
      "350:\tlearn: 0.3070397\ttotal: 14.6s\tremaining: 26.9s\n",
      "351:\tlearn: 0.3069411\ttotal: 14.6s\tremaining: 26.9s\n",
      "352:\tlearn: 0.3069028\ttotal: 14.6s\tremaining: 26.8s\n",
      "353:\tlearn: 0.3067083\ttotal: 14.7s\tremaining: 26.8s\n",
      "354:\tlearn: 0.3063355\ttotal: 14.7s\tremaining: 26.7s\n",
      "355:\tlearn: 0.3061667\ttotal: 14.8s\tremaining: 26.7s\n",
      "356:\tlearn: 0.3059792\ttotal: 14.8s\tremaining: 26.7s\n",
      "357:\tlearn: 0.3056678\ttotal: 14.8s\tremaining: 26.6s\n",
      "358:\tlearn: 0.3055722\ttotal: 14.9s\tremaining: 26.6s\n",
      "359:\tlearn: 0.3055312\ttotal: 14.9s\tremaining: 26.6s\n",
      "360:\tlearn: 0.3051778\ttotal: 15s\tremaining: 26.5s\n",
      "361:\tlearn: 0.3050043\ttotal: 15s\tremaining: 26.5s\n",
      "362:\tlearn: 0.3045916\ttotal: 15.1s\tremaining: 26.4s\n",
      "363:\tlearn: 0.3043426\ttotal: 15.1s\tremaining: 26.4s\n",
      "364:\tlearn: 0.3042759\ttotal: 15.1s\tremaining: 26.3s\n",
      "365:\tlearn: 0.3040072\ttotal: 15.2s\tremaining: 26.3s\n",
      "366:\tlearn: 0.3037022\ttotal: 15.2s\tremaining: 26.3s\n",
      "367:\tlearn: 0.3033689\ttotal: 15.3s\tremaining: 26.2s\n",
      "368:\tlearn: 0.3032948\ttotal: 15.3s\tremaining: 26.2s\n",
      "369:\tlearn: 0.3031111\ttotal: 15.4s\tremaining: 26.1s\n",
      "370:\tlearn: 0.3029056\ttotal: 15.4s\tremaining: 26.1s\n",
      "371:\tlearn: 0.3028453\ttotal: 15.4s\tremaining: 26.1s\n",
      "372:\tlearn: 0.3025166\ttotal: 15.5s\tremaining: 26s\n",
      "373:\tlearn: 0.3024105\ttotal: 15.5s\tremaining: 26s\n",
      "374:\tlearn: 0.3023045\ttotal: 15.6s\tremaining: 25.9s\n",
      "375:\tlearn: 0.3019463\ttotal: 15.6s\tremaining: 25.9s\n",
      "376:\tlearn: 0.3018777\ttotal: 15.6s\tremaining: 25.9s\n",
      "377:\tlearn: 0.3015873\ttotal: 15.7s\tremaining: 25.8s\n",
      "378:\tlearn: 0.3012055\ttotal: 15.7s\tremaining: 25.8s\n",
      "379:\tlearn: 0.3011247\ttotal: 15.8s\tremaining: 25.7s\n",
      "380:\tlearn: 0.3010702\ttotal: 15.8s\tremaining: 25.7s\n",
      "381:\tlearn: 0.3009407\ttotal: 15.9s\tremaining: 25.6s\n",
      "382:\tlearn: 0.3005901\ttotal: 15.9s\tremaining: 25.6s\n",
      "383:\tlearn: 0.3004267\ttotal: 15.9s\tremaining: 25.6s\n",
      "384:\tlearn: 0.3002926\ttotal: 16s\tremaining: 25.5s\n",
      "385:\tlearn: 0.3000215\ttotal: 16s\tremaining: 25.5s\n",
      "386:\tlearn: 0.2997314\ttotal: 16.1s\tremaining: 25.4s\n",
      "387:\tlearn: 0.2994523\ttotal: 16.1s\tremaining: 25.4s\n",
      "388:\tlearn: 0.2991990\ttotal: 16.1s\tremaining: 25.4s\n",
      "389:\tlearn: 0.2991367\ttotal: 16.2s\tremaining: 25.3s\n",
      "390:\tlearn: 0.2991108\ttotal: 16.2s\tremaining: 25.3s\n",
      "391:\tlearn: 0.2987894\ttotal: 16.3s\tremaining: 25.2s\n",
      "392:\tlearn: 0.2984513\ttotal: 16.3s\tremaining: 25.2s\n",
      "393:\tlearn: 0.2982320\ttotal: 16.4s\tremaining: 25.1s\n",
      "394:\tlearn: 0.2982006\ttotal: 16.4s\tremaining: 25.1s\n",
      "395:\tlearn: 0.2979442\ttotal: 16.4s\tremaining: 25.1s\n",
      "396:\tlearn: 0.2978521\ttotal: 16.5s\tremaining: 25s\n",
      "397:\tlearn: 0.2976316\ttotal: 16.5s\tremaining: 25s\n",
      "398:\tlearn: 0.2973528\ttotal: 16.6s\tremaining: 25s\n",
      "399:\tlearn: 0.2971107\ttotal: 16.6s\tremaining: 24.9s\n",
      "400:\tlearn: 0.2970492\ttotal: 16.7s\tremaining: 24.9s\n",
      "401:\tlearn: 0.2967936\ttotal: 16.7s\tremaining: 24.8s\n",
      "402:\tlearn: 0.2964884\ttotal: 16.7s\tremaining: 24.8s\n",
      "403:\tlearn: 0.2964663\ttotal: 16.8s\tremaining: 24.8s\n",
      "404:\tlearn: 0.2962296\ttotal: 16.8s\tremaining: 24.7s\n",
      "405:\tlearn: 0.2959528\ttotal: 16.9s\tremaining: 24.7s\n",
      "406:\tlearn: 0.2958462\ttotal: 16.9s\tremaining: 24.6s\n",
      "407:\tlearn: 0.2956035\ttotal: 17s\tremaining: 24.6s\n",
      "408:\tlearn: 0.2953823\ttotal: 17s\tremaining: 24.6s\n",
      "409:\tlearn: 0.2950749\ttotal: 17s\tremaining: 24.5s\n",
      "410:\tlearn: 0.2948446\ttotal: 17.1s\tremaining: 24.5s\n",
      "411:\tlearn: 0.2945980\ttotal: 17.1s\tremaining: 24.4s\n",
      "412:\tlearn: 0.2943526\ttotal: 17.2s\tremaining: 24.4s\n",
      "413:\tlearn: 0.2943495\ttotal: 17.2s\tremaining: 24.3s\n",
      "414:\tlearn: 0.2940684\ttotal: 17.2s\tremaining: 24.3s\n",
      "415:\tlearn: 0.2938433\ttotal: 17.3s\tremaining: 24.3s\n",
      "416:\tlearn: 0.2935382\ttotal: 17.3s\tremaining: 24.2s\n",
      "417:\tlearn: 0.2933565\ttotal: 17.4s\tremaining: 24.2s\n",
      "418:\tlearn: 0.2932022\ttotal: 17.4s\tremaining: 24.1s\n",
      "419:\tlearn: 0.2928935\ttotal: 17.4s\tremaining: 24.1s\n",
      "420:\tlearn: 0.2928240\ttotal: 17.5s\tremaining: 24.1s\n",
      "421:\tlearn: 0.2926774\ttotal: 17.5s\tremaining: 24s\n",
      "422:\tlearn: 0.2924488\ttotal: 17.6s\tremaining: 24s\n",
      "423:\tlearn: 0.2923108\ttotal: 17.6s\tremaining: 23.9s\n",
      "424:\tlearn: 0.2921957\ttotal: 17.7s\tremaining: 23.9s\n",
      "425:\tlearn: 0.2919577\ttotal: 17.7s\tremaining: 23.9s\n",
      "426:\tlearn: 0.2918882\ttotal: 17.7s\tremaining: 23.8s\n",
      "427:\tlearn: 0.2916252\ttotal: 17.8s\tremaining: 23.8s\n",
      "428:\tlearn: 0.2914565\ttotal: 17.8s\tremaining: 23.7s\n",
      "429:\tlearn: 0.2912338\ttotal: 17.9s\tremaining: 23.7s\n",
      "430:\tlearn: 0.2909168\ttotal: 17.9s\tremaining: 23.6s\n",
      "431:\tlearn: 0.2905960\ttotal: 18s\tremaining: 23.6s\n",
      "432:\tlearn: 0.2902960\ttotal: 18s\tremaining: 23.6s\n",
      "433:\tlearn: 0.2900593\ttotal: 18s\tremaining: 23.5s\n",
      "434:\tlearn: 0.2899201\ttotal: 18.1s\tremaining: 23.5s\n",
      "435:\tlearn: 0.2896507\ttotal: 18.1s\tremaining: 23.4s\n",
      "436:\tlearn: 0.2894183\ttotal: 18.2s\tremaining: 23.4s\n",
      "437:\tlearn: 0.2893590\ttotal: 18.2s\tremaining: 23.4s\n",
      "438:\tlearn: 0.2892498\ttotal: 18.3s\tremaining: 23.3s\n",
      "439:\tlearn: 0.2891836\ttotal: 18.3s\tremaining: 23.3s\n",
      "440:\tlearn: 0.2889534\ttotal: 18.3s\tremaining: 23.2s\n",
      "441:\tlearn: 0.2888824\ttotal: 18.4s\tremaining: 23.2s\n",
      "442:\tlearn: 0.2886549\ttotal: 18.4s\tremaining: 23.2s\n",
      "443:\tlearn: 0.2885464\ttotal: 18.5s\tremaining: 23.1s\n",
      "444:\tlearn: 0.2882399\ttotal: 18.5s\tremaining: 23.1s\n",
      "445:\tlearn: 0.2879842\ttotal: 18.5s\tremaining: 23s\n",
      "446:\tlearn: 0.2878656\ttotal: 18.6s\tremaining: 23s\n",
      "447:\tlearn: 0.2877101\ttotal: 18.6s\tremaining: 22.9s\n",
      "448:\tlearn: 0.2874455\ttotal: 18.7s\tremaining: 22.9s\n",
      "449:\tlearn: 0.2871818\ttotal: 18.7s\tremaining: 22.9s\n",
      "450:\tlearn: 0.2869467\ttotal: 18.8s\tremaining: 22.8s\n",
      "451:\tlearn: 0.2868119\ttotal: 18.8s\tremaining: 22.8s\n",
      "452:\tlearn: 0.2865260\ttotal: 18.8s\tremaining: 22.8s\n",
      "453:\tlearn: 0.2865127\ttotal: 18.9s\tremaining: 22.7s\n",
      "454:\tlearn: 0.2863200\ttotal: 18.9s\tremaining: 22.7s\n",
      "455:\tlearn: 0.2860409\ttotal: 19s\tremaining: 22.6s\n",
      "456:\tlearn: 0.2858470\ttotal: 19s\tremaining: 22.6s\n",
      "457:\tlearn: 0.2856337\ttotal: 19s\tremaining: 22.5s\n",
      "458:\tlearn: 0.2854818\ttotal: 19.1s\tremaining: 22.5s\n",
      "459:\tlearn: 0.2853909\ttotal: 19.1s\tremaining: 22.4s\n",
      "460:\tlearn: 0.2851397\ttotal: 19.2s\tremaining: 22.4s\n",
      "461:\tlearn: 0.2850053\ttotal: 19.2s\tremaining: 22.4s\n",
      "462:\tlearn: 0.2847245\ttotal: 19.2s\tremaining: 22.3s\n",
      "463:\tlearn: 0.2846854\ttotal: 19.3s\tremaining: 22.3s\n",
      "464:\tlearn: 0.2845907\ttotal: 19.3s\tremaining: 22.2s\n",
      "465:\tlearn: 0.2844200\ttotal: 19.4s\tremaining: 22.2s\n",
      "466:\tlearn: 0.2841266\ttotal: 19.4s\tremaining: 22.1s\n",
      "467:\tlearn: 0.2839740\ttotal: 19.4s\tremaining: 22.1s\n",
      "468:\tlearn: 0.2837217\ttotal: 19.5s\tremaining: 22s\n",
      "469:\tlearn: 0.2834694\ttotal: 19.5s\tremaining: 22s\n",
      "470:\tlearn: 0.2832231\ttotal: 19.5s\tremaining: 22s\n",
      "471:\tlearn: 0.2828910\ttotal: 19.6s\tremaining: 21.9s\n",
      "472:\tlearn: 0.2828092\ttotal: 19.6s\tremaining: 21.9s\n",
      "473:\tlearn: 0.2826106\ttotal: 19.7s\tremaining: 21.8s\n",
      "474:\tlearn: 0.2825932\ttotal: 19.7s\tremaining: 21.8s\n",
      "475:\tlearn: 0.2824982\ttotal: 19.8s\tremaining: 21.8s\n",
      "476:\tlearn: 0.2824395\ttotal: 19.8s\tremaining: 21.7s\n",
      "477:\tlearn: 0.2823557\ttotal: 19.8s\tremaining: 21.7s\n",
      "478:\tlearn: 0.2822790\ttotal: 19.9s\tremaining: 21.6s\n",
      "479:\tlearn: 0.2822367\ttotal: 19.9s\tremaining: 21.6s\n",
      "480:\tlearn: 0.2819917\ttotal: 20s\tremaining: 21.5s\n",
      "481:\tlearn: 0.2818819\ttotal: 20s\tremaining: 21.5s\n",
      "482:\tlearn: 0.2816103\ttotal: 20s\tremaining: 21.5s\n",
      "483:\tlearn: 0.2814064\ttotal: 20.1s\tremaining: 21.4s\n",
      "484:\tlearn: 0.2813111\ttotal: 20.1s\tremaining: 21.4s\n",
      "485:\tlearn: 0.2812342\ttotal: 20.2s\tremaining: 21.3s\n",
      "486:\tlearn: 0.2811909\ttotal: 20.2s\tremaining: 21.3s\n",
      "487:\tlearn: 0.2810089\ttotal: 20.2s\tremaining: 21.2s\n",
      "488:\tlearn: 0.2808279\ttotal: 20.3s\tremaining: 21.2s\n",
      "489:\tlearn: 0.2807826\ttotal: 20.3s\tremaining: 21.2s\n",
      "490:\tlearn: 0.2804383\ttotal: 20.4s\tremaining: 21.1s\n",
      "491:\tlearn: 0.2803800\ttotal: 20.4s\tremaining: 21.1s\n",
      "492:\tlearn: 0.2801556\ttotal: 20.5s\tremaining: 21s\n",
      "493:\tlearn: 0.2799456\ttotal: 20.5s\tremaining: 21s\n",
      "494:\tlearn: 0.2798746\ttotal: 20.5s\tremaining: 21s\n",
      "495:\tlearn: 0.2797969\ttotal: 20.6s\tremaining: 20.9s\n",
      "496:\tlearn: 0.2795436\ttotal: 20.6s\tremaining: 20.9s\n",
      "497:\tlearn: 0.2793995\ttotal: 20.7s\tremaining: 20.8s\n",
      "498:\tlearn: 0.2791356\ttotal: 20.7s\tremaining: 20.8s\n",
      "499:\tlearn: 0.2788867\ttotal: 20.7s\tremaining: 20.7s\n",
      "500:\tlearn: 0.2787207\ttotal: 20.8s\tremaining: 20.7s\n",
      "501:\tlearn: 0.2784844\ttotal: 20.8s\tremaining: 20.7s\n",
      "502:\tlearn: 0.2783989\ttotal: 20.9s\tremaining: 20.6s\n",
      "503:\tlearn: 0.2782285\ttotal: 20.9s\tremaining: 20.6s\n",
      "504:\tlearn: 0.2781277\ttotal: 20.9s\tremaining: 20.5s\n",
      "505:\tlearn: 0.2779910\ttotal: 21s\tremaining: 20.5s\n",
      "506:\tlearn: 0.2778386\ttotal: 21s\tremaining: 20.4s\n",
      "507:\tlearn: 0.2776714\ttotal: 21.1s\tremaining: 20.4s\n",
      "508:\tlearn: 0.2775643\ttotal: 21.1s\tremaining: 20.4s\n",
      "509:\tlearn: 0.2773891\ttotal: 21.2s\tremaining: 20.3s\n",
      "510:\tlearn: 0.2773408\ttotal: 21.2s\tremaining: 20.3s\n",
      "511:\tlearn: 0.2772013\ttotal: 21.2s\tremaining: 20.2s\n",
      "512:\tlearn: 0.2769500\ttotal: 21.3s\tremaining: 20.2s\n",
      "513:\tlearn: 0.2768969\ttotal: 21.3s\tremaining: 20.2s\n",
      "514:\tlearn: 0.2765605\ttotal: 21.4s\tremaining: 20.1s\n",
      "515:\tlearn: 0.2764127\ttotal: 21.4s\tremaining: 20.1s\n",
      "516:\tlearn: 0.2761496\ttotal: 21.4s\tremaining: 20s\n",
      "517:\tlearn: 0.2759642\ttotal: 21.5s\tremaining: 20s\n",
      "518:\tlearn: 0.2759545\ttotal: 21.5s\tremaining: 19.9s\n",
      "519:\tlearn: 0.2757474\ttotal: 21.6s\tremaining: 19.9s\n",
      "520:\tlearn: 0.2756114\ttotal: 21.6s\tremaining: 19.9s\n",
      "521:\tlearn: 0.2754346\ttotal: 21.6s\tremaining: 19.8s\n",
      "522:\tlearn: 0.2753121\ttotal: 21.7s\tremaining: 19.8s\n",
      "523:\tlearn: 0.2752207\ttotal: 21.7s\tremaining: 19.7s\n",
      "524:\tlearn: 0.2750642\ttotal: 21.8s\tremaining: 19.7s\n",
      "525:\tlearn: 0.2749219\ttotal: 21.8s\tremaining: 19.6s\n",
      "526:\tlearn: 0.2746876\ttotal: 21.9s\tremaining: 19.6s\n",
      "527:\tlearn: 0.2743968\ttotal: 21.9s\tremaining: 19.6s\n",
      "528:\tlearn: 0.2742243\ttotal: 21.9s\tremaining: 19.5s\n",
      "529:\tlearn: 0.2740773\ttotal: 22s\tremaining: 19.5s\n",
      "530:\tlearn: 0.2739057\ttotal: 22s\tremaining: 19.4s\n",
      "531:\tlearn: 0.2736794\ttotal: 22s\tremaining: 19.4s\n",
      "532:\tlearn: 0.2734371\ttotal: 22.1s\tremaining: 19.4s\n",
      "533:\tlearn: 0.2733149\ttotal: 22.1s\tremaining: 19.3s\n",
      "534:\tlearn: 0.2732223\ttotal: 22.2s\tremaining: 19.3s\n",
      "535:\tlearn: 0.2731140\ttotal: 22.2s\tremaining: 19.2s\n",
      "536:\tlearn: 0.2729505\ttotal: 22.3s\tremaining: 19.2s\n",
      "537:\tlearn: 0.2727599\ttotal: 22.3s\tremaining: 19.1s\n",
      "538:\tlearn: 0.2726476\ttotal: 22.3s\tremaining: 19.1s\n",
      "539:\tlearn: 0.2725156\ttotal: 22.4s\tremaining: 19.1s\n",
      "540:\tlearn: 0.2723355\ttotal: 22.4s\tremaining: 19s\n",
      "541:\tlearn: 0.2721053\ttotal: 22.4s\tremaining: 19s\n",
      "542:\tlearn: 0.2719366\ttotal: 22.5s\tremaining: 18.9s\n",
      "543:\tlearn: 0.2718052\ttotal: 22.5s\tremaining: 18.9s\n",
      "544:\tlearn: 0.2717106\ttotal: 22.6s\tremaining: 18.8s\n",
      "545:\tlearn: 0.2715895\ttotal: 22.6s\tremaining: 18.8s\n",
      "546:\tlearn: 0.2714916\ttotal: 22.6s\tremaining: 18.8s\n",
      "547:\tlearn: 0.2713472\ttotal: 22.7s\tremaining: 18.7s\n",
      "548:\tlearn: 0.2713033\ttotal: 22.7s\tremaining: 18.7s\n",
      "549:\tlearn: 0.2710250\ttotal: 22.8s\tremaining: 18.6s\n",
      "550:\tlearn: 0.2707764\ttotal: 22.8s\tremaining: 18.6s\n",
      "551:\tlearn: 0.2706267\ttotal: 22.8s\tremaining: 18.5s\n",
      "552:\tlearn: 0.2706219\ttotal: 22.9s\tremaining: 18.5s\n",
      "553:\tlearn: 0.2704874\ttotal: 22.9s\tremaining: 18.4s\n",
      "554:\tlearn: 0.2702490\ttotal: 23s\tremaining: 18.4s\n",
      "555:\tlearn: 0.2700809\ttotal: 23s\tremaining: 18.4s\n",
      "556:\tlearn: 0.2699761\ttotal: 23s\tremaining: 18.3s\n",
      "557:\tlearn: 0.2698975\ttotal: 23.1s\tremaining: 18.3s\n",
      "558:\tlearn: 0.2697574\ttotal: 23.1s\tremaining: 18.2s\n",
      "559:\tlearn: 0.2696530\ttotal: 23.2s\tremaining: 18.2s\n",
      "560:\tlearn: 0.2693645\ttotal: 23.2s\tremaining: 18.2s\n",
      "561:\tlearn: 0.2691501\ttotal: 23.2s\tremaining: 18.1s\n",
      "562:\tlearn: 0.2689420\ttotal: 23.3s\tremaining: 18.1s\n",
      "563:\tlearn: 0.2687291\ttotal: 23.3s\tremaining: 18s\n",
      "564:\tlearn: 0.2685491\ttotal: 23.4s\tremaining: 18s\n",
      "565:\tlearn: 0.2683814\ttotal: 23.4s\tremaining: 17.9s\n",
      "566:\tlearn: 0.2681720\ttotal: 23.4s\tremaining: 17.9s\n",
      "567:\tlearn: 0.2680535\ttotal: 23.5s\tremaining: 17.9s\n",
      "568:\tlearn: 0.2679500\ttotal: 23.5s\tremaining: 17.8s\n",
      "569:\tlearn: 0.2677536\ttotal: 23.6s\tremaining: 17.8s\n",
      "570:\tlearn: 0.2675689\ttotal: 23.6s\tremaining: 17.7s\n",
      "571:\tlearn: 0.2673558\ttotal: 23.7s\tremaining: 17.7s\n",
      "572:\tlearn: 0.2672057\ttotal: 23.7s\tremaining: 17.7s\n",
      "573:\tlearn: 0.2670995\ttotal: 23.7s\tremaining: 17.6s\n",
      "574:\tlearn: 0.2669830\ttotal: 23.8s\tremaining: 17.6s\n",
      "575:\tlearn: 0.2668462\ttotal: 23.8s\tremaining: 17.5s\n",
      "576:\tlearn: 0.2667796\ttotal: 23.9s\tremaining: 17.5s\n",
      "577:\tlearn: 0.2666019\ttotal: 23.9s\tremaining: 17.4s\n",
      "578:\tlearn: 0.2664059\ttotal: 23.9s\tremaining: 17.4s\n",
      "579:\tlearn: 0.2662809\ttotal: 24s\tremaining: 17.4s\n",
      "580:\tlearn: 0.2660931\ttotal: 24s\tremaining: 17.3s\n",
      "581:\tlearn: 0.2659209\ttotal: 24.1s\tremaining: 17.3s\n",
      "582:\tlearn: 0.2657115\ttotal: 24.1s\tremaining: 17.2s\n",
      "583:\tlearn: 0.2655657\ttotal: 24.1s\tremaining: 17.2s\n",
      "584:\tlearn: 0.2654953\ttotal: 24.2s\tremaining: 17.2s\n",
      "585:\tlearn: 0.2651778\ttotal: 24.2s\tremaining: 17.1s\n",
      "586:\tlearn: 0.2650604\ttotal: 24.3s\tremaining: 17.1s\n",
      "587:\tlearn: 0.2649316\ttotal: 24.3s\tremaining: 17s\n",
      "588:\tlearn: 0.2647488\ttotal: 24.3s\tremaining: 17s\n",
      "589:\tlearn: 0.2646832\ttotal: 24.4s\tremaining: 16.9s\n",
      "590:\tlearn: 0.2646148\ttotal: 24.4s\tremaining: 16.9s\n",
      "591:\tlearn: 0.2645321\ttotal: 24.5s\tremaining: 16.9s\n",
      "592:\tlearn: 0.2644144\ttotal: 24.5s\tremaining: 16.8s\n",
      "593:\tlearn: 0.2642372\ttotal: 24.5s\tremaining: 16.8s\n",
      "594:\tlearn: 0.2640955\ttotal: 24.6s\tremaining: 16.7s\n",
      "595:\tlearn: 0.2640418\ttotal: 24.6s\tremaining: 16.7s\n",
      "596:\tlearn: 0.2639937\ttotal: 24.7s\tremaining: 16.7s\n",
      "597:\tlearn: 0.2638890\ttotal: 24.7s\tremaining: 16.6s\n",
      "598:\tlearn: 0.2637781\ttotal: 24.8s\tremaining: 16.6s\n",
      "599:\tlearn: 0.2637107\ttotal: 24.8s\tremaining: 16.5s\n",
      "600:\tlearn: 0.2636059\ttotal: 24.8s\tremaining: 16.5s\n",
      "601:\tlearn: 0.2635373\ttotal: 24.9s\tremaining: 16.4s\n",
      "602:\tlearn: 0.2634341\ttotal: 24.9s\tremaining: 16.4s\n",
      "603:\tlearn: 0.2633445\ttotal: 25s\tremaining: 16.4s\n",
      "604:\tlearn: 0.2631900\ttotal: 25s\tremaining: 16.3s\n",
      "605:\tlearn: 0.2630316\ttotal: 25s\tremaining: 16.3s\n",
      "606:\tlearn: 0.2629004\ttotal: 25.1s\tremaining: 16.2s\n",
      "607:\tlearn: 0.2626876\ttotal: 25.1s\tremaining: 16.2s\n",
      "608:\tlearn: 0.2625682\ttotal: 25.1s\tremaining: 16.1s\n",
      "609:\tlearn: 0.2623399\ttotal: 25.2s\tremaining: 16.1s\n",
      "610:\tlearn: 0.2620493\ttotal: 25.2s\tremaining: 16.1s\n",
      "611:\tlearn: 0.2619465\ttotal: 25.3s\tremaining: 16s\n",
      "612:\tlearn: 0.2618608\ttotal: 25.3s\tremaining: 16s\n",
      "613:\tlearn: 0.2617177\ttotal: 25.4s\tremaining: 15.9s\n",
      "614:\tlearn: 0.2616107\ttotal: 25.4s\tremaining: 15.9s\n",
      "615:\tlearn: 0.2614510\ttotal: 25.4s\tremaining: 15.9s\n",
      "616:\tlearn: 0.2612480\ttotal: 25.5s\tremaining: 15.8s\n",
      "617:\tlearn: 0.2610957\ttotal: 25.5s\tremaining: 15.8s\n",
      "618:\tlearn: 0.2609353\ttotal: 25.5s\tremaining: 15.7s\n",
      "619:\tlearn: 0.2607652\ttotal: 25.6s\tremaining: 15.7s\n",
      "620:\tlearn: 0.2606429\ttotal: 25.6s\tremaining: 15.6s\n",
      "621:\tlearn: 0.2605488\ttotal: 25.7s\tremaining: 15.6s\n",
      "622:\tlearn: 0.2604093\ttotal: 25.7s\tremaining: 15.6s\n",
      "623:\tlearn: 0.2603257\ttotal: 25.7s\tremaining: 15.5s\n",
      "624:\tlearn: 0.2602506\ttotal: 25.8s\tremaining: 15.5s\n",
      "625:\tlearn: 0.2599611\ttotal: 25.8s\tremaining: 15.4s\n",
      "626:\tlearn: 0.2598235\ttotal: 25.9s\tremaining: 15.4s\n",
      "627:\tlearn: 0.2598033\ttotal: 25.9s\tremaining: 15.3s\n",
      "628:\tlearn: 0.2596983\ttotal: 25.9s\tremaining: 15.3s\n",
      "629:\tlearn: 0.2595529\ttotal: 26s\tremaining: 15.3s\n",
      "630:\tlearn: 0.2594187\ttotal: 26s\tremaining: 15.2s\n",
      "631:\tlearn: 0.2593305\ttotal: 26.1s\tremaining: 15.2s\n",
      "632:\tlearn: 0.2592684\ttotal: 26.1s\tremaining: 15.1s\n",
      "633:\tlearn: 0.2591539\ttotal: 26.1s\tremaining: 15.1s\n",
      "634:\tlearn: 0.2590727\ttotal: 26.2s\tremaining: 15s\n",
      "635:\tlearn: 0.2589756\ttotal: 26.2s\tremaining: 15s\n",
      "636:\tlearn: 0.2589213\ttotal: 26.3s\tremaining: 15s\n",
      "637:\tlearn: 0.2587540\ttotal: 26.3s\tremaining: 14.9s\n",
      "638:\tlearn: 0.2586663\ttotal: 26.3s\tremaining: 14.9s\n",
      "639:\tlearn: 0.2585985\ttotal: 26.4s\tremaining: 14.8s\n",
      "640:\tlearn: 0.2584009\ttotal: 26.4s\tremaining: 14.8s\n",
      "641:\tlearn: 0.2582122\ttotal: 26.5s\tremaining: 14.8s\n",
      "642:\tlearn: 0.2581503\ttotal: 26.5s\tremaining: 14.7s\n",
      "643:\tlearn: 0.2580953\ttotal: 26.5s\tremaining: 14.7s\n",
      "644:\tlearn: 0.2579884\ttotal: 26.6s\tremaining: 14.6s\n",
      "645:\tlearn: 0.2578491\ttotal: 26.6s\tremaining: 14.6s\n",
      "646:\tlearn: 0.2577748\ttotal: 26.7s\tremaining: 14.5s\n",
      "647:\tlearn: 0.2576400\ttotal: 26.7s\tremaining: 14.5s\n",
      "648:\tlearn: 0.2575989\ttotal: 26.7s\tremaining: 14.5s\n",
      "649:\tlearn: 0.2574261\ttotal: 26.8s\tremaining: 14.4s\n",
      "650:\tlearn: 0.2573678\ttotal: 26.8s\tremaining: 14.4s\n",
      "651:\tlearn: 0.2572343\ttotal: 26.9s\tremaining: 14.3s\n",
      "652:\tlearn: 0.2570939\ttotal: 26.9s\tremaining: 14.3s\n",
      "653:\tlearn: 0.2568836\ttotal: 26.9s\tremaining: 14.3s\n",
      "654:\tlearn: 0.2567574\ttotal: 27s\tremaining: 14.2s\n",
      "655:\tlearn: 0.2566228\ttotal: 27s\tremaining: 14.2s\n",
      "656:\tlearn: 0.2564833\ttotal: 27.1s\tremaining: 14.1s\n",
      "657:\tlearn: 0.2564025\ttotal: 27.1s\tremaining: 14.1s\n",
      "658:\tlearn: 0.2563292\ttotal: 27.1s\tremaining: 14s\n",
      "659:\tlearn: 0.2562665\ttotal: 27.2s\tremaining: 14s\n",
      "660:\tlearn: 0.2560776\ttotal: 27.2s\tremaining: 14s\n",
      "661:\tlearn: 0.2559857\ttotal: 27.3s\tremaining: 13.9s\n",
      "662:\tlearn: 0.2559415\ttotal: 27.3s\tremaining: 13.9s\n",
      "663:\tlearn: 0.2558434\ttotal: 27.4s\tremaining: 13.8s\n",
      "664:\tlearn: 0.2556633\ttotal: 27.4s\tremaining: 13.8s\n",
      "665:\tlearn: 0.2555690\ttotal: 27.4s\tremaining: 13.8s\n",
      "666:\tlearn: 0.2555068\ttotal: 27.5s\tremaining: 13.7s\n",
      "667:\tlearn: 0.2553540\ttotal: 27.5s\tremaining: 13.7s\n",
      "668:\tlearn: 0.2551579\ttotal: 27.6s\tremaining: 13.6s\n",
      "669:\tlearn: 0.2550113\ttotal: 27.6s\tremaining: 13.6s\n",
      "670:\tlearn: 0.2548849\ttotal: 27.7s\tremaining: 13.6s\n",
      "671:\tlearn: 0.2548175\ttotal: 27.7s\tremaining: 13.5s\n",
      "672:\tlearn: 0.2546818\ttotal: 27.7s\tremaining: 13.5s\n",
      "673:\tlearn: 0.2545868\ttotal: 27.8s\tremaining: 13.4s\n",
      "674:\tlearn: 0.2545307\ttotal: 27.8s\tremaining: 13.4s\n",
      "675:\tlearn: 0.2544105\ttotal: 27.9s\tremaining: 13.4s\n",
      "676:\tlearn: 0.2543813\ttotal: 27.9s\tremaining: 13.3s\n",
      "677:\tlearn: 0.2543490\ttotal: 28s\tremaining: 13.3s\n",
      "678:\tlearn: 0.2543430\ttotal: 28s\tremaining: 13.2s\n",
      "679:\tlearn: 0.2542542\ttotal: 28s\tremaining: 13.2s\n",
      "680:\tlearn: 0.2541654\ttotal: 28.1s\tremaining: 13.2s\n",
      "681:\tlearn: 0.2540897\ttotal: 28.1s\tremaining: 13.1s\n",
      "682:\tlearn: 0.2539378\ttotal: 28.2s\tremaining: 13.1s\n",
      "683:\tlearn: 0.2538584\ttotal: 28.2s\tremaining: 13s\n",
      "684:\tlearn: 0.2537568\ttotal: 28.3s\tremaining: 13s\n",
      "685:\tlearn: 0.2536904\ttotal: 28.3s\tremaining: 13s\n",
      "686:\tlearn: 0.2535579\ttotal: 28.3s\tremaining: 12.9s\n",
      "687:\tlearn: 0.2533919\ttotal: 28.4s\tremaining: 12.9s\n",
      "688:\tlearn: 0.2533507\ttotal: 28.4s\tremaining: 12.8s\n",
      "689:\tlearn: 0.2531966\ttotal: 28.5s\tremaining: 12.8s\n",
      "690:\tlearn: 0.2530300\ttotal: 28.5s\tremaining: 12.8s\n",
      "691:\tlearn: 0.2529004\ttotal: 28.6s\tremaining: 12.7s\n",
      "692:\tlearn: 0.2528785\ttotal: 28.6s\tremaining: 12.7s\n",
      "693:\tlearn: 0.2527487\ttotal: 28.6s\tremaining: 12.6s\n",
      "694:\tlearn: 0.2525872\ttotal: 28.7s\tremaining: 12.6s\n",
      "695:\tlearn: 0.2524363\ttotal: 28.7s\tremaining: 12.5s\n",
      "696:\tlearn: 0.2523263\ttotal: 28.8s\tremaining: 12.5s\n",
      "697:\tlearn: 0.2522385\ttotal: 28.8s\tremaining: 12.5s\n",
      "698:\tlearn: 0.2521952\ttotal: 28.9s\tremaining: 12.4s\n",
      "699:\tlearn: 0.2520402\ttotal: 28.9s\tremaining: 12.4s\n",
      "700:\tlearn: 0.2518541\ttotal: 28.9s\tremaining: 12.3s\n",
      "701:\tlearn: 0.2516943\ttotal: 29s\tremaining: 12.3s\n",
      "702:\tlearn: 0.2515644\ttotal: 29s\tremaining: 12.3s\n",
      "703:\tlearn: 0.2514126\ttotal: 29.1s\tremaining: 12.2s\n",
      "704:\tlearn: 0.2512234\ttotal: 29.1s\tremaining: 12.2s\n",
      "705:\tlearn: 0.2510346\ttotal: 29.1s\tremaining: 12.1s\n",
      "706:\tlearn: 0.2508982\ttotal: 29.2s\tremaining: 12.1s\n",
      "707:\tlearn: 0.2508477\ttotal: 29.2s\tremaining: 12.1s\n",
      "708:\tlearn: 0.2506406\ttotal: 29.3s\tremaining: 12s\n",
      "709:\tlearn: 0.2505933\ttotal: 29.3s\tremaining: 12s\n",
      "710:\tlearn: 0.2504467\ttotal: 29.4s\tremaining: 11.9s\n",
      "711:\tlearn: 0.2501605\ttotal: 29.4s\tremaining: 11.9s\n",
      "712:\tlearn: 0.2501038\ttotal: 29.4s\tremaining: 11.9s\n",
      "713:\tlearn: 0.2500339\ttotal: 29.5s\tremaining: 11.8s\n",
      "714:\tlearn: 0.2499917\ttotal: 29.5s\tremaining: 11.8s\n",
      "715:\tlearn: 0.2498797\ttotal: 29.6s\tremaining: 11.7s\n",
      "716:\tlearn: 0.2497884\ttotal: 29.6s\tremaining: 11.7s\n",
      "717:\tlearn: 0.2496421\ttotal: 29.7s\tremaining: 11.6s\n",
      "718:\tlearn: 0.2496221\ttotal: 29.7s\tremaining: 11.6s\n",
      "719:\tlearn: 0.2495128\ttotal: 29.7s\tremaining: 11.6s\n",
      "720:\tlearn: 0.2494710\ttotal: 29.8s\tremaining: 11.5s\n",
      "721:\tlearn: 0.2493116\ttotal: 29.8s\tremaining: 11.5s\n",
      "722:\tlearn: 0.2491345\ttotal: 29.9s\tremaining: 11.4s\n",
      "723:\tlearn: 0.2490040\ttotal: 29.9s\tremaining: 11.4s\n",
      "724:\tlearn: 0.2488294\ttotal: 30s\tremaining: 11.4s\n",
      "725:\tlearn: 0.2487331\ttotal: 30s\tremaining: 11.3s\n",
      "726:\tlearn: 0.2485670\ttotal: 30s\tremaining: 11.3s\n",
      "727:\tlearn: 0.2484942\ttotal: 30.1s\tremaining: 11.2s\n",
      "728:\tlearn: 0.2483747\ttotal: 30.1s\tremaining: 11.2s\n",
      "729:\tlearn: 0.2482627\ttotal: 30.2s\tremaining: 11.2s\n",
      "730:\tlearn: 0.2481835\ttotal: 30.2s\tremaining: 11.1s\n",
      "731:\tlearn: 0.2480174\ttotal: 30.3s\tremaining: 11.1s\n",
      "732:\tlearn: 0.2479664\ttotal: 30.3s\tremaining: 11s\n",
      "733:\tlearn: 0.2477621\ttotal: 30.3s\tremaining: 11s\n",
      "734:\tlearn: 0.2476797\ttotal: 30.4s\tremaining: 11s\n",
      "735:\tlearn: 0.2475004\ttotal: 30.4s\tremaining: 10.9s\n",
      "736:\tlearn: 0.2474102\ttotal: 30.5s\tremaining: 10.9s\n",
      "737:\tlearn: 0.2472223\ttotal: 30.5s\tremaining: 10.8s\n",
      "738:\tlearn: 0.2470993\ttotal: 30.6s\tremaining: 10.8s\n",
      "739:\tlearn: 0.2470310\ttotal: 30.6s\tremaining: 10.8s\n",
      "740:\tlearn: 0.2468695\ttotal: 30.6s\tremaining: 10.7s\n",
      "741:\tlearn: 0.2467023\ttotal: 30.7s\tremaining: 10.7s\n",
      "742:\tlearn: 0.2465596\ttotal: 30.7s\tremaining: 10.6s\n",
      "743:\tlearn: 0.2464684\ttotal: 30.8s\tremaining: 10.6s\n",
      "744:\tlearn: 0.2464116\ttotal: 30.8s\tremaining: 10.5s\n",
      "745:\tlearn: 0.2462275\ttotal: 30.8s\tremaining: 10.5s\n",
      "746:\tlearn: 0.2460878\ttotal: 30.9s\tremaining: 10.5s\n",
      "747:\tlearn: 0.2459648\ttotal: 30.9s\tremaining: 10.4s\n",
      "748:\tlearn: 0.2458966\ttotal: 31s\tremaining: 10.4s\n",
      "749:\tlearn: 0.2457096\ttotal: 31s\tremaining: 10.3s\n",
      "750:\tlearn: 0.2455563\ttotal: 31.1s\tremaining: 10.3s\n",
      "751:\tlearn: 0.2454806\ttotal: 31.1s\tremaining: 10.3s\n",
      "752:\tlearn: 0.2453274\ttotal: 31.1s\tremaining: 10.2s\n",
      "753:\tlearn: 0.2451692\ttotal: 31.2s\tremaining: 10.2s\n",
      "754:\tlearn: 0.2449895\ttotal: 31.2s\tremaining: 10.1s\n",
      "755:\tlearn: 0.2448383\ttotal: 31.3s\tremaining: 10.1s\n",
      "756:\tlearn: 0.2447817\ttotal: 31.3s\tremaining: 10s\n",
      "757:\tlearn: 0.2446481\ttotal: 31.3s\tremaining: 10s\n",
      "758:\tlearn: 0.2444974\ttotal: 31.4s\tremaining: 9.97s\n",
      "759:\tlearn: 0.2442759\ttotal: 31.4s\tremaining: 9.93s\n",
      "760:\tlearn: 0.2442382\ttotal: 31.5s\tremaining: 9.89s\n",
      "761:\tlearn: 0.2441744\ttotal: 31.5s\tremaining: 9.85s\n",
      "762:\tlearn: 0.2440580\ttotal: 31.6s\tremaining: 9.81s\n",
      "763:\tlearn: 0.2439497\ttotal: 31.6s\tremaining: 9.77s\n",
      "764:\tlearn: 0.2437915\ttotal: 31.7s\tremaining: 9.73s\n",
      "765:\tlearn: 0.2436197\ttotal: 31.7s\tremaining: 9.69s\n",
      "766:\tlearn: 0.2434502\ttotal: 31.8s\tremaining: 9.65s\n",
      "767:\tlearn: 0.2433396\ttotal: 31.8s\tremaining: 9.61s\n",
      "768:\tlearn: 0.2431609\ttotal: 31.9s\tremaining: 9.57s\n",
      "769:\tlearn: 0.2430765\ttotal: 31.9s\tremaining: 9.53s\n",
      "770:\tlearn: 0.2429330\ttotal: 32s\tremaining: 9.49s\n",
      "771:\tlearn: 0.2428879\ttotal: 32s\tremaining: 9.45s\n",
      "772:\tlearn: 0.2427621\ttotal: 32.1s\tremaining: 9.41s\n",
      "773:\tlearn: 0.2426950\ttotal: 32.1s\tremaining: 9.37s\n",
      "774:\tlearn: 0.2425364\ttotal: 32.1s\tremaining: 9.33s\n",
      "775:\tlearn: 0.2425057\ttotal: 32.2s\tremaining: 9.29s\n",
      "776:\tlearn: 0.2424128\ttotal: 32.2s\tremaining: 9.25s\n",
      "777:\tlearn: 0.2423445\ttotal: 32.3s\tremaining: 9.21s\n",
      "778:\tlearn: 0.2422577\ttotal: 32.3s\tremaining: 9.16s\n",
      "779:\tlearn: 0.2421100\ttotal: 32.3s\tremaining: 9.12s\n",
      "780:\tlearn: 0.2419435\ttotal: 32.4s\tremaining: 9.08s\n",
      "781:\tlearn: 0.2418508\ttotal: 32.4s\tremaining: 9.04s\n",
      "782:\tlearn: 0.2417606\ttotal: 32.5s\tremaining: 9s\n",
      "783:\tlearn: 0.2417271\ttotal: 32.5s\tremaining: 8.96s\n",
      "784:\tlearn: 0.2415970\ttotal: 32.6s\tremaining: 8.92s\n",
      "785:\tlearn: 0.2414603\ttotal: 32.6s\tremaining: 8.88s\n",
      "786:\tlearn: 0.2414427\ttotal: 32.6s\tremaining: 8.83s\n",
      "787:\tlearn: 0.2413039\ttotal: 32.7s\tremaining: 8.79s\n",
      "788:\tlearn: 0.2412458\ttotal: 32.7s\tremaining: 8.75s\n",
      "789:\tlearn: 0.2411705\ttotal: 32.8s\tremaining: 8.71s\n",
      "790:\tlearn: 0.2410327\ttotal: 32.8s\tremaining: 8.67s\n",
      "791:\tlearn: 0.2409054\ttotal: 32.8s\tremaining: 8.63s\n",
      "792:\tlearn: 0.2408348\ttotal: 32.9s\tremaining: 8.58s\n",
      "793:\tlearn: 0.2407229\ttotal: 32.9s\tremaining: 8.54s\n",
      "794:\tlearn: 0.2405688\ttotal: 33s\tremaining: 8.5s\n",
      "795:\tlearn: 0.2404246\ttotal: 33s\tremaining: 8.46s\n",
      "796:\tlearn: 0.2403385\ttotal: 33s\tremaining: 8.42s\n",
      "797:\tlearn: 0.2401936\ttotal: 33.1s\tremaining: 8.38s\n",
      "798:\tlearn: 0.2400192\ttotal: 33.1s\tremaining: 8.34s\n",
      "799:\tlearn: 0.2399098\ttotal: 33.2s\tremaining: 8.3s\n",
      "800:\tlearn: 0.2397850\ttotal: 33.2s\tremaining: 8.25s\n",
      "801:\tlearn: 0.2396567\ttotal: 33.3s\tremaining: 8.21s\n",
      "802:\tlearn: 0.2395161\ttotal: 33.3s\tremaining: 8.17s\n",
      "803:\tlearn: 0.2393684\ttotal: 33.3s\tremaining: 8.13s\n",
      "804:\tlearn: 0.2392703\ttotal: 33.4s\tremaining: 8.09s\n",
      "805:\tlearn: 0.2391775\ttotal: 33.4s\tremaining: 8.05s\n",
      "806:\tlearn: 0.2390886\ttotal: 33.5s\tremaining: 8s\n",
      "807:\tlearn: 0.2390146\ttotal: 33.5s\tremaining: 7.96s\n",
      "808:\tlearn: 0.2389139\ttotal: 33.6s\tremaining: 7.92s\n",
      "809:\tlearn: 0.2387522\ttotal: 33.6s\tremaining: 7.88s\n",
      "810:\tlearn: 0.2386263\ttotal: 33.6s\tremaining: 7.84s\n",
      "811:\tlearn: 0.2385330\ttotal: 33.7s\tremaining: 7.79s\n",
      "812:\tlearn: 0.2384226\ttotal: 33.7s\tremaining: 7.75s\n",
      "813:\tlearn: 0.2382600\ttotal: 33.8s\tremaining: 7.71s\n",
      "814:\tlearn: 0.2381212\ttotal: 33.8s\tremaining: 7.67s\n",
      "815:\tlearn: 0.2380172\ttotal: 33.8s\tremaining: 7.63s\n",
      "816:\tlearn: 0.2379182\ttotal: 33.9s\tremaining: 7.59s\n",
      "817:\tlearn: 0.2378576\ttotal: 33.9s\tremaining: 7.55s\n",
      "818:\tlearn: 0.2377771\ttotal: 34s\tremaining: 7.51s\n",
      "819:\tlearn: 0.2375940\ttotal: 34s\tremaining: 7.47s\n",
      "820:\tlearn: 0.2374714\ttotal: 34.1s\tremaining: 7.43s\n",
      "821:\tlearn: 0.2373800\ttotal: 34.1s\tremaining: 7.38s\n",
      "822:\tlearn: 0.2372558\ttotal: 34.1s\tremaining: 7.34s\n",
      "823:\tlearn: 0.2371126\ttotal: 34.2s\tremaining: 7.3s\n",
      "824:\tlearn: 0.2369869\ttotal: 34.2s\tremaining: 7.26s\n",
      "825:\tlearn: 0.2368420\ttotal: 34.3s\tremaining: 7.22s\n",
      "826:\tlearn: 0.2367907\ttotal: 34.3s\tremaining: 7.18s\n",
      "827:\tlearn: 0.2367043\ttotal: 34.4s\tremaining: 7.14s\n",
      "828:\tlearn: 0.2365524\ttotal: 34.4s\tremaining: 7.1s\n",
      "829:\tlearn: 0.2364232\ttotal: 34.5s\tremaining: 7.06s\n",
      "830:\tlearn: 0.2363635\ttotal: 34.5s\tremaining: 7.02s\n",
      "831:\tlearn: 0.2362445\ttotal: 34.5s\tremaining: 6.97s\n",
      "832:\tlearn: 0.2361930\ttotal: 34.6s\tremaining: 6.93s\n",
      "833:\tlearn: 0.2361249\ttotal: 34.6s\tremaining: 6.89s\n",
      "834:\tlearn: 0.2360672\ttotal: 34.7s\tremaining: 6.85s\n",
      "835:\tlearn: 0.2359864\ttotal: 34.7s\tremaining: 6.81s\n",
      "836:\tlearn: 0.2359119\ttotal: 34.8s\tremaining: 6.77s\n",
      "837:\tlearn: 0.2358733\ttotal: 34.8s\tremaining: 6.73s\n",
      "838:\tlearn: 0.2358363\ttotal: 34.9s\tremaining: 6.69s\n",
      "839:\tlearn: 0.2357035\ttotal: 34.9s\tremaining: 6.65s\n",
      "840:\tlearn: 0.2356521\ttotal: 34.9s\tremaining: 6.61s\n",
      "841:\tlearn: 0.2355553\ttotal: 35s\tremaining: 6.56s\n",
      "842:\tlearn: 0.2354758\ttotal: 35s\tremaining: 6.52s\n",
      "843:\tlearn: 0.2353741\ttotal: 35.1s\tremaining: 6.48s\n",
      "844:\tlearn: 0.2352795\ttotal: 35.1s\tremaining: 6.44s\n",
      "845:\tlearn: 0.2351550\ttotal: 35.1s\tremaining: 6.4s\n",
      "846:\tlearn: 0.2350982\ttotal: 35.2s\tremaining: 6.36s\n",
      "847:\tlearn: 0.2349999\ttotal: 35.2s\tremaining: 6.31s\n",
      "848:\tlearn: 0.2348502\ttotal: 35.3s\tremaining: 6.27s\n",
      "849:\tlearn: 0.2347615\ttotal: 35.3s\tremaining: 6.23s\n",
      "850:\tlearn: 0.2346200\ttotal: 35.3s\tremaining: 6.19s\n",
      "851:\tlearn: 0.2344770\ttotal: 35.4s\tremaining: 6.15s\n",
      "852:\tlearn: 0.2343924\ttotal: 35.4s\tremaining: 6.11s\n",
      "853:\tlearn: 0.2343096\ttotal: 35.5s\tremaining: 6.06s\n",
      "854:\tlearn: 0.2342320\ttotal: 35.5s\tremaining: 6.02s\n",
      "855:\tlearn: 0.2341066\ttotal: 35.6s\tremaining: 5.98s\n",
      "856:\tlearn: 0.2339923\ttotal: 35.6s\tremaining: 5.94s\n",
      "857:\tlearn: 0.2339546\ttotal: 35.6s\tremaining: 5.9s\n",
      "858:\tlearn: 0.2338163\ttotal: 35.7s\tremaining: 5.86s\n",
      "859:\tlearn: 0.2337698\ttotal: 35.7s\tremaining: 5.82s\n",
      "860:\tlearn: 0.2337145\ttotal: 35.8s\tremaining: 5.78s\n",
      "861:\tlearn: 0.2335843\ttotal: 35.8s\tremaining: 5.73s\n",
      "862:\tlearn: 0.2335201\ttotal: 35.9s\tremaining: 5.69s\n",
      "863:\tlearn: 0.2334509\ttotal: 35.9s\tremaining: 5.65s\n",
      "864:\tlearn: 0.2334191\ttotal: 35.9s\tremaining: 5.61s\n",
      "865:\tlearn: 0.2333173\ttotal: 36s\tremaining: 5.57s\n",
      "866:\tlearn: 0.2332122\ttotal: 36s\tremaining: 5.53s\n",
      "867:\tlearn: 0.2330449\ttotal: 36.1s\tremaining: 5.49s\n",
      "868:\tlearn: 0.2330225\ttotal: 36.1s\tremaining: 5.44s\n",
      "869:\tlearn: 0.2328755\ttotal: 36.2s\tremaining: 5.4s\n",
      "870:\tlearn: 0.2327450\ttotal: 36.2s\tremaining: 5.36s\n",
      "871:\tlearn: 0.2325282\ttotal: 36.2s\tremaining: 5.32s\n",
      "872:\tlearn: 0.2324466\ttotal: 36.3s\tremaining: 5.28s\n",
      "873:\tlearn: 0.2323567\ttotal: 36.3s\tremaining: 5.23s\n",
      "874:\tlearn: 0.2322296\ttotal: 36.4s\tremaining: 5.19s\n",
      "875:\tlearn: 0.2319981\ttotal: 36.4s\tremaining: 5.15s\n",
      "876:\tlearn: 0.2319259\ttotal: 36.4s\tremaining: 5.11s\n",
      "877:\tlearn: 0.2318667\ttotal: 36.5s\tremaining: 5.07s\n",
      "878:\tlearn: 0.2317764\ttotal: 36.5s\tremaining: 5.03s\n",
      "879:\tlearn: 0.2317147\ttotal: 36.6s\tremaining: 4.99s\n",
      "880:\tlearn: 0.2317024\ttotal: 36.6s\tremaining: 4.94s\n",
      "881:\tlearn: 0.2316657\ttotal: 36.6s\tremaining: 4.9s\n",
      "882:\tlearn: 0.2314222\ttotal: 36.7s\tremaining: 4.86s\n",
      "883:\tlearn: 0.2313060\ttotal: 36.7s\tremaining: 4.82s\n",
      "884:\tlearn: 0.2312055\ttotal: 36.8s\tremaining: 4.78s\n",
      "885:\tlearn: 0.2310996\ttotal: 36.8s\tremaining: 4.74s\n",
      "886:\tlearn: 0.2309080\ttotal: 36.9s\tremaining: 4.69s\n",
      "887:\tlearn: 0.2308389\ttotal: 36.9s\tremaining: 4.65s\n",
      "888:\tlearn: 0.2306766\ttotal: 36.9s\tremaining: 4.61s\n",
      "889:\tlearn: 0.2304793\ttotal: 37s\tremaining: 4.57s\n",
      "890:\tlearn: 0.2303941\ttotal: 37s\tremaining: 4.53s\n",
      "891:\tlearn: 0.2303251\ttotal: 37.1s\tremaining: 4.49s\n",
      "892:\tlearn: 0.2301903\ttotal: 37.1s\tremaining: 4.45s\n",
      "893:\tlearn: 0.2300681\ttotal: 37.1s\tremaining: 4.4s\n",
      "894:\tlearn: 0.2300000\ttotal: 37.2s\tremaining: 4.36s\n",
      "895:\tlearn: 0.2298634\ttotal: 37.2s\tremaining: 4.32s\n",
      "896:\tlearn: 0.2298326\ttotal: 37.3s\tremaining: 4.28s\n",
      "897:\tlearn: 0.2297764\ttotal: 37.3s\tremaining: 4.24s\n",
      "898:\tlearn: 0.2296838\ttotal: 37.3s\tremaining: 4.2s\n",
      "899:\tlearn: 0.2295409\ttotal: 37.4s\tremaining: 4.15s\n",
      "900:\tlearn: 0.2295201\ttotal: 37.4s\tremaining: 4.11s\n",
      "901:\tlearn: 0.2293934\ttotal: 37.5s\tremaining: 4.07s\n",
      "902:\tlearn: 0.2292301\ttotal: 37.5s\tremaining: 4.03s\n",
      "903:\tlearn: 0.2290806\ttotal: 37.6s\tremaining: 3.99s\n",
      "904:\tlearn: 0.2289983\ttotal: 37.6s\tremaining: 3.95s\n",
      "905:\tlearn: 0.2288814\ttotal: 37.6s\tremaining: 3.9s\n",
      "906:\tlearn: 0.2288242\ttotal: 37.7s\tremaining: 3.86s\n",
      "907:\tlearn: 0.2287034\ttotal: 37.7s\tremaining: 3.82s\n",
      "908:\tlearn: 0.2285717\ttotal: 37.8s\tremaining: 3.78s\n",
      "909:\tlearn: 0.2285353\ttotal: 37.8s\tremaining: 3.74s\n",
      "910:\tlearn: 0.2283995\ttotal: 37.8s\tremaining: 3.7s\n",
      "911:\tlearn: 0.2282535\ttotal: 37.9s\tremaining: 3.65s\n",
      "912:\tlearn: 0.2281390\ttotal: 37.9s\tremaining: 3.61s\n",
      "913:\tlearn: 0.2281045\ttotal: 38s\tremaining: 3.57s\n",
      "914:\tlearn: 0.2280549\ttotal: 38s\tremaining: 3.53s\n",
      "915:\tlearn: 0.2278903\ttotal: 38s\tremaining: 3.49s\n",
      "916:\tlearn: 0.2278508\ttotal: 38.1s\tremaining: 3.45s\n",
      "917:\tlearn: 0.2277137\ttotal: 38.1s\tremaining: 3.4s\n",
      "918:\tlearn: 0.2275847\ttotal: 38.2s\tremaining: 3.36s\n",
      "919:\tlearn: 0.2275406\ttotal: 38.2s\tremaining: 3.32s\n",
      "920:\tlearn: 0.2274961\ttotal: 38.3s\tremaining: 3.28s\n",
      "921:\tlearn: 0.2273635\ttotal: 38.3s\tremaining: 3.24s\n",
      "922:\tlearn: 0.2272260\ttotal: 38.3s\tremaining: 3.2s\n",
      "923:\tlearn: 0.2271437\ttotal: 38.4s\tremaining: 3.16s\n",
      "924:\tlearn: 0.2270913\ttotal: 38.4s\tremaining: 3.12s\n",
      "925:\tlearn: 0.2268883\ttotal: 38.5s\tremaining: 3.07s\n",
      "926:\tlearn: 0.2267613\ttotal: 38.5s\tremaining: 3.03s\n",
      "927:\tlearn: 0.2266207\ttotal: 38.6s\tremaining: 2.99s\n",
      "928:\tlearn: 0.2265234\ttotal: 38.6s\tremaining: 2.95s\n",
      "929:\tlearn: 0.2263243\ttotal: 38.6s\tremaining: 2.91s\n",
      "930:\tlearn: 0.2261601\ttotal: 38.7s\tremaining: 2.87s\n",
      "931:\tlearn: 0.2260286\ttotal: 38.7s\tremaining: 2.83s\n",
      "932:\tlearn: 0.2259917\ttotal: 38.8s\tremaining: 2.78s\n",
      "933:\tlearn: 0.2259324\ttotal: 38.8s\tremaining: 2.74s\n",
      "934:\tlearn: 0.2258792\ttotal: 38.9s\tremaining: 2.7s\n",
      "935:\tlearn: 0.2257555\ttotal: 38.9s\tremaining: 2.66s\n",
      "936:\tlearn: 0.2256919\ttotal: 38.9s\tremaining: 2.62s\n",
      "937:\tlearn: 0.2255993\ttotal: 39s\tremaining: 2.58s\n",
      "938:\tlearn: 0.2255115\ttotal: 39s\tremaining: 2.54s\n",
      "939:\tlearn: 0.2254211\ttotal: 39.1s\tremaining: 2.49s\n",
      "940:\tlearn: 0.2253732\ttotal: 39.1s\tremaining: 2.45s\n",
      "941:\tlearn: 0.2253393\ttotal: 39.2s\tremaining: 2.41s\n",
      "942:\tlearn: 0.2253154\ttotal: 39.2s\tremaining: 2.37s\n",
      "943:\tlearn: 0.2252158\ttotal: 39.3s\tremaining: 2.33s\n",
      "944:\tlearn: 0.2251787\ttotal: 39.3s\tremaining: 2.29s\n",
      "945:\tlearn: 0.2251566\ttotal: 39.4s\tremaining: 2.25s\n",
      "946:\tlearn: 0.2250227\ttotal: 39.4s\tremaining: 2.21s\n",
      "947:\tlearn: 0.2248990\ttotal: 39.4s\tremaining: 2.16s\n",
      "948:\tlearn: 0.2248166\ttotal: 39.5s\tremaining: 2.12s\n",
      "949:\tlearn: 0.2247608\ttotal: 39.5s\tremaining: 2.08s\n",
      "950:\tlearn: 0.2246194\ttotal: 39.6s\tremaining: 2.04s\n",
      "951:\tlearn: 0.2244896\ttotal: 39.6s\tremaining: 2s\n",
      "952:\tlearn: 0.2243795\ttotal: 39.7s\tremaining: 1.96s\n",
      "953:\tlearn: 0.2243373\ttotal: 39.7s\tremaining: 1.91s\n",
      "954:\tlearn: 0.2242845\ttotal: 39.7s\tremaining: 1.87s\n",
      "955:\tlearn: 0.2241800\ttotal: 39.8s\tremaining: 1.83s\n",
      "956:\tlearn: 0.2241287\ttotal: 39.8s\tremaining: 1.79s\n",
      "957:\tlearn: 0.2240308\ttotal: 39.9s\tremaining: 1.75s\n",
      "958:\tlearn: 0.2239225\ttotal: 39.9s\tremaining: 1.71s\n",
      "959:\tlearn: 0.2238308\ttotal: 40s\tremaining: 1.66s\n",
      "960:\tlearn: 0.2237749\ttotal: 40s\tremaining: 1.62s\n",
      "961:\tlearn: 0.2237177\ttotal: 40s\tremaining: 1.58s\n",
      "962:\tlearn: 0.2236532\ttotal: 40.1s\tremaining: 1.54s\n",
      "963:\tlearn: 0.2235061\ttotal: 40.1s\tremaining: 1.5s\n",
      "964:\tlearn: 0.2233474\ttotal: 40.2s\tremaining: 1.46s\n",
      "965:\tlearn: 0.2232520\ttotal: 40.2s\tremaining: 1.42s\n",
      "966:\tlearn: 0.2231933\ttotal: 40.3s\tremaining: 1.37s\n",
      "967:\tlearn: 0.2231227\ttotal: 40.3s\tremaining: 1.33s\n",
      "968:\tlearn: 0.2230207\ttotal: 40.3s\tremaining: 1.29s\n",
      "969:\tlearn: 0.2229187\ttotal: 40.4s\tremaining: 1.25s\n",
      "970:\tlearn: 0.2227976\ttotal: 40.4s\tremaining: 1.21s\n",
      "971:\tlearn: 0.2226847\ttotal: 40.5s\tremaining: 1.17s\n",
      "972:\tlearn: 0.2225523\ttotal: 40.5s\tremaining: 1.12s\n",
      "973:\tlearn: 0.2224469\ttotal: 40.6s\tremaining: 1.08s\n",
      "974:\tlearn: 0.2224130\ttotal: 40.6s\tremaining: 1.04s\n",
      "975:\tlearn: 0.2222642\ttotal: 40.6s\tremaining: 1000ms\n",
      "976:\tlearn: 0.2221315\ttotal: 40.7s\tremaining: 958ms\n",
      "977:\tlearn: 0.2220589\ttotal: 40.7s\tremaining: 916ms\n",
      "978:\tlearn: 0.2219033\ttotal: 40.8s\tremaining: 875ms\n",
      "979:\tlearn: 0.2217666\ttotal: 40.8s\tremaining: 833ms\n",
      "980:\tlearn: 0.2217281\ttotal: 40.9s\tremaining: 791ms\n",
      "981:\tlearn: 0.2216001\ttotal: 40.9s\tremaining: 750ms\n",
      "982:\tlearn: 0.2213995\ttotal: 40.9s\tremaining: 708ms\n",
      "983:\tlearn: 0.2213177\ttotal: 41s\tremaining: 666ms\n",
      "984:\tlearn: 0.2211889\ttotal: 41s\tremaining: 625ms\n",
      "985:\tlearn: 0.2210643\ttotal: 41.1s\tremaining: 583ms\n",
      "986:\tlearn: 0.2209417\ttotal: 41.1s\tremaining: 541ms\n",
      "987:\tlearn: 0.2208356\ttotal: 41.1s\tremaining: 500ms\n",
      "988:\tlearn: 0.2207647\ttotal: 41.2s\tremaining: 458ms\n",
      "989:\tlearn: 0.2205991\ttotal: 41.2s\tremaining: 416ms\n",
      "990:\tlearn: 0.2205846\ttotal: 41.3s\tremaining: 375ms\n",
      "991:\tlearn: 0.2204849\ttotal: 41.3s\tremaining: 333ms\n",
      "992:\tlearn: 0.2203208\ttotal: 41.3s\tremaining: 291ms\n",
      "993:\tlearn: 0.2202541\ttotal: 41.4s\tremaining: 250ms\n",
      "994:\tlearn: 0.2201548\ttotal: 41.4s\tremaining: 208ms\n",
      "995:\tlearn: 0.2200742\ttotal: 41.5s\tremaining: 167ms\n",
      "996:\tlearn: 0.2200433\ttotal: 41.5s\tremaining: 125ms\n",
      "997:\tlearn: 0.2199538\ttotal: 41.5s\tremaining: 83.3ms\n",
      "998:\tlearn: 0.2198623\ttotal: 41.6s\tremaining: 41.6ms\n",
      "999:\tlearn: 0.2198525\ttotal: 41.6s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['contract_type',\n",
       " 'num_dependents_bin',\n",
       " 'num_referrals_bin',\n",
       " 'total_monthly_fee',\n",
       " 'tenure_months',\n",
       " 'avg_long_distance_fee_monthly',\n",
       " 'total_charges_quarter_cbrt',\n",
       " 'zip_code',\n",
       " 'payment_method',\n",
       " 'avg_gb_download_monthly']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_feat = ['zip_code', 'gender', 'internet_type', 'contract_type', 'payment_method', 'num_dependents_bin', 'num_referrals_bin', 'age_group' ]\n",
    "catboost = CatBoostClassifier(random_state = 5, cat_features= cat_feat)\n",
    "\n",
    "catboost.fit(X, Y)\n",
    "\n",
    "feature_importance = catboost.feature_importances_\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})\n",
    "importance_df = importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "\n",
    "top10_features = importance_df['Feature'].head(10).tolist()\n",
    "top10_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X[top10_features], Y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4866572\ttotal: 97.6ms\tremaining: 1m 17s\n",
      "1:\tlearn: 0.4114092\ttotal: 182ms\tremaining: 1m 12s\n",
      "2:\tlearn: 0.3909457\ttotal: 286ms\tremaining: 1m 15s\n",
      "3:\tlearn: 0.3748290\ttotal: 362ms\tremaining: 1m 12s\n",
      "4:\tlearn: 0.3641832\ttotal: 453ms\tremaining: 1m 12s\n",
      "5:\tlearn: 0.3529742\ttotal: 546ms\tremaining: 1m 12s\n",
      "6:\tlearn: 0.3468005\ttotal: 684ms\tremaining: 1m 17s\n",
      "7:\tlearn: 0.3444258\ttotal: 815ms\tremaining: 1m 20s\n",
      "8:\tlearn: 0.3354955\ttotal: 943ms\tremaining: 1m 22s\n",
      "9:\tlearn: 0.3335899\ttotal: 1.09s\tremaining: 1m 26s\n",
      "10:\tlearn: 0.3322438\ttotal: 1.18s\tremaining: 1m 24s\n",
      "11:\tlearn: 0.3252196\ttotal: 1.3s\tremaining: 1m 25s\n",
      "12:\tlearn: 0.3201464\ttotal: 1.42s\tremaining: 1m 25s\n",
      "13:\tlearn: 0.3134953\ttotal: 1.52s\tremaining: 1m 25s\n",
      "14:\tlearn: 0.3028740\ttotal: 1.63s\tremaining: 1m 25s\n",
      "15:\tlearn: 0.2996051\ttotal: 1.73s\tremaining: 1m 24s\n",
      "16:\tlearn: 0.2909395\ttotal: 1.82s\tremaining: 1m 23s\n",
      "17:\tlearn: 0.2852285\ttotal: 1.9s\tremaining: 1m 22s\n",
      "18:\tlearn: 0.2817875\ttotal: 1.97s\tremaining: 1m 21s\n",
      "19:\tlearn: 0.2764270\ttotal: 2.06s\tremaining: 1m 20s\n",
      "20:\tlearn: 0.2713169\ttotal: 2.19s\tremaining: 1m 21s\n",
      "21:\tlearn: 0.2664369\ttotal: 2.31s\tremaining: 1m 21s\n",
      "22:\tlearn: 0.2590972\ttotal: 2.38s\tremaining: 1m 20s\n",
      "23:\tlearn: 0.2544770\ttotal: 2.45s\tremaining: 1m 19s\n",
      "24:\tlearn: 0.2487241\ttotal: 2.6s\tremaining: 1m 20s\n",
      "25:\tlearn: 0.2435281\ttotal: 2.73s\tremaining: 1m 21s\n",
      "26:\tlearn: 0.2405559\ttotal: 2.88s\tremaining: 1m 22s\n",
      "27:\tlearn: 0.2405160\ttotal: 3.01s\tremaining: 1m 22s\n",
      "28:\tlearn: 0.2354412\ttotal: 3.13s\tremaining: 1m 23s\n",
      "29:\tlearn: 0.2353559\ttotal: 3.26s\tremaining: 1m 23s\n",
      "30:\tlearn: 0.2315271\ttotal: 3.35s\tremaining: 1m 23s\n",
      "31:\tlearn: 0.2273368\ttotal: 3.46s\tremaining: 1m 22s\n",
      "32:\tlearn: 0.2220200\ttotal: 3.6s\tremaining: 1m 23s\n",
      "33:\tlearn: 0.2195438\ttotal: 3.67s\tremaining: 1m 22s\n",
      "34:\tlearn: 0.2151378\ttotal: 3.75s\tremaining: 1m 21s\n",
      "35:\tlearn: 0.2139107\ttotal: 3.82s\tremaining: 1m 21s\n",
      "36:\tlearn: 0.2104245\ttotal: 3.92s\tremaining: 1m 20s\n",
      "37:\tlearn: 0.2053793\ttotal: 3.99s\tremaining: 1m 20s\n",
      "38:\tlearn: 0.2053787\ttotal: 4.09s\tremaining: 1m 19s\n",
      "39:\tlearn: 0.2012361\ttotal: 4.21s\tremaining: 1m 20s\n",
      "40:\tlearn: 0.1989012\ttotal: 4.32s\tremaining: 1m 20s\n",
      "41:\tlearn: 0.1959975\ttotal: 4.43s\tremaining: 1m 19s\n",
      "42:\tlearn: 0.1936098\ttotal: 4.52s\tremaining: 1m 19s\n",
      "43:\tlearn: 0.1890349\ttotal: 4.62s\tremaining: 1m 19s\n",
      "44:\tlearn: 0.1844052\ttotal: 4.71s\tremaining: 1m 19s\n",
      "45:\tlearn: 0.1799501\ttotal: 4.78s\tremaining: 1m 18s\n",
      "46:\tlearn: 0.1762996\ttotal: 4.84s\tremaining: 1m 17s\n",
      "47:\tlearn: 0.1726320\ttotal: 4.94s\tremaining: 1m 17s\n",
      "48:\tlearn: 0.1700678\ttotal: 5.05s\tremaining: 1m 17s\n",
      "49:\tlearn: 0.1687367\ttotal: 5.17s\tremaining: 1m 17s\n",
      "50:\tlearn: 0.1662462\ttotal: 5.24s\tremaining: 1m 17s\n",
      "51:\tlearn: 0.1623404\ttotal: 5.35s\tremaining: 1m 16s\n",
      "52:\tlearn: 0.1611489\ttotal: 5.46s\tremaining: 1m 16s\n",
      "53:\tlearn: 0.1582284\ttotal: 5.55s\tremaining: 1m 16s\n",
      "54:\tlearn: 0.1576506\ttotal: 5.63s\tremaining: 1m 16s\n",
      "55:\tlearn: 0.1555746\ttotal: 5.75s\tremaining: 1m 16s\n",
      "56:\tlearn: 0.1532259\ttotal: 5.8s\tremaining: 1m 15s\n",
      "57:\tlearn: 0.1515417\ttotal: 5.88s\tremaining: 1m 15s\n",
      "58:\tlearn: 0.1514905\ttotal: 5.96s\tremaining: 1m 14s\n",
      "59:\tlearn: 0.1496287\ttotal: 6.02s\tremaining: 1m 14s\n",
      "60:\tlearn: 0.1486252\ttotal: 6.1s\tremaining: 1m 13s\n",
      "61:\tlearn: 0.1468040\ttotal: 6.19s\tremaining: 1m 13s\n",
      "62:\tlearn: 0.1446101\ttotal: 6.26s\tremaining: 1m 13s\n",
      "63:\tlearn: 0.1430265\ttotal: 6.32s\tremaining: 1m 12s\n",
      "64:\tlearn: 0.1406681\ttotal: 6.39s\tremaining: 1m 12s\n",
      "65:\tlearn: 0.1378244\ttotal: 6.47s\tremaining: 1m 11s\n",
      "66:\tlearn: 0.1361673\ttotal: 6.57s\tremaining: 1m 11s\n",
      "67:\tlearn: 0.1338057\ttotal: 6.66s\tremaining: 1m 11s\n",
      "68:\tlearn: 0.1312849\ttotal: 6.73s\tremaining: 1m 11s\n",
      "69:\tlearn: 0.1296777\ttotal: 6.82s\tremaining: 1m 11s\n",
      "70:\tlearn: 0.1274161\ttotal: 6.91s\tremaining: 1m 10s\n",
      "71:\tlearn: 0.1255270\ttotal: 7.03s\tremaining: 1m 11s\n",
      "72:\tlearn: 0.1243191\ttotal: 7.15s\tremaining: 1m 11s\n",
      "73:\tlearn: 0.1241263\ttotal: 7.26s\tremaining: 1m 11s\n",
      "74:\tlearn: 0.1227355\ttotal: 7.33s\tremaining: 1m 10s\n",
      "75:\tlearn: 0.1206320\ttotal: 7.39s\tremaining: 1m 10s\n",
      "76:\tlearn: 0.1201786\ttotal: 7.47s\tremaining: 1m 10s\n",
      "77:\tlearn: 0.1197855\ttotal: 7.58s\tremaining: 1m 10s\n",
      "78:\tlearn: 0.1187148\ttotal: 7.65s\tremaining: 1m 9s\n",
      "79:\tlearn: 0.1164819\ttotal: 7.72s\tremaining: 1m 9s\n",
      "80:\tlearn: 0.1163178\ttotal: 7.83s\tremaining: 1m 9s\n",
      "81:\tlearn: 0.1146322\ttotal: 7.94s\tremaining: 1m 9s\n",
      "82:\tlearn: 0.1135757\ttotal: 8.01s\tremaining: 1m 9s\n",
      "83:\tlearn: 0.1123396\ttotal: 8.1s\tremaining: 1m 9s\n",
      "84:\tlearn: 0.1115877\ttotal: 8.2s\tremaining: 1m 8s\n",
      "85:\tlearn: 0.1103651\ttotal: 8.32s\tremaining: 1m 9s\n",
      "86:\tlearn: 0.1090712\ttotal: 8.4s\tremaining: 1m 8s\n",
      "87:\tlearn: 0.1084594\ttotal: 8.48s\tremaining: 1m 8s\n",
      "88:\tlearn: 0.1074638\ttotal: 8.56s\tremaining: 1m 8s\n",
      "89:\tlearn: 0.1056752\ttotal: 8.64s\tremaining: 1m 8s\n",
      "90:\tlearn: 0.1044115\ttotal: 8.77s\tremaining: 1m 8s\n",
      "91:\tlearn: 0.1038788\ttotal: 8.87s\tremaining: 1m 8s\n",
      "92:\tlearn: 0.1021936\ttotal: 8.94s\tremaining: 1m 7s\n",
      "93:\tlearn: 0.1017510\ttotal: 9s\tremaining: 1m 7s\n",
      "94:\tlearn: 0.1002753\ttotal: 9.09s\tremaining: 1m 7s\n",
      "95:\tlearn: 0.0991085\ttotal: 9.18s\tremaining: 1m 7s\n",
      "96:\tlearn: 0.0980327\ttotal: 9.26s\tremaining: 1m 7s\n",
      "97:\tlearn: 0.0972032\ttotal: 9.33s\tremaining: 1m 6s\n",
      "98:\tlearn: 0.0958843\ttotal: 9.41s\tremaining: 1m 6s\n",
      "99:\tlearn: 0.0941840\ttotal: 9.49s\tremaining: 1m 6s\n",
      "100:\tlearn: 0.0931372\ttotal: 9.63s\tremaining: 1m 6s\n",
      "101:\tlearn: 0.0921481\ttotal: 9.73s\tremaining: 1m 6s\n",
      "102:\tlearn: 0.0906709\ttotal: 9.79s\tremaining: 1m 6s\n",
      "103:\tlearn: 0.0900628\ttotal: 9.89s\tremaining: 1m 6s\n",
      "104:\tlearn: 0.0887482\ttotal: 9.99s\tremaining: 1m 6s\n",
      "105:\tlearn: 0.0873978\ttotal: 10.1s\tremaining: 1m 6s\n",
      "106:\tlearn: 0.0864369\ttotal: 10.2s\tremaining: 1m 6s\n",
      "107:\tlearn: 0.0858395\ttotal: 10.3s\tremaining: 1m 5s\n",
      "108:\tlearn: 0.0855418\ttotal: 10.4s\tremaining: 1m 5s\n",
      "109:\tlearn: 0.0851875\ttotal: 10.5s\tremaining: 1m 5s\n",
      "110:\tlearn: 0.0838107\ttotal: 10.6s\tremaining: 1m 5s\n",
      "111:\tlearn: 0.0829507\ttotal: 10.7s\tremaining: 1m 5s\n",
      "112:\tlearn: 0.0818109\ttotal: 10.8s\tremaining: 1m 5s\n",
      "113:\tlearn: 0.0811711\ttotal: 10.8s\tremaining: 1m 5s\n",
      "114:\tlearn: 0.0802758\ttotal: 10.9s\tremaining: 1m 4s\n",
      "115:\tlearn: 0.0798249\ttotal: 11s\tremaining: 1m 4s\n",
      "116:\tlearn: 0.0786464\ttotal: 11.1s\tremaining: 1m 4s\n",
      "117:\tlearn: 0.0780170\ttotal: 11.2s\tremaining: 1m 4s\n",
      "118:\tlearn: 0.0769815\ttotal: 11.2s\tremaining: 1m 4s\n",
      "119:\tlearn: 0.0753706\ttotal: 11.3s\tremaining: 1m 4s\n",
      "120:\tlearn: 0.0743846\ttotal: 11.4s\tremaining: 1m 3s\n",
      "121:\tlearn: 0.0736652\ttotal: 11.5s\tremaining: 1m 3s\n",
      "122:\tlearn: 0.0729477\ttotal: 11.6s\tremaining: 1m 3s\n",
      "123:\tlearn: 0.0721139\ttotal: 11.7s\tremaining: 1m 3s\n",
      "124:\tlearn: 0.0714562\ttotal: 11.8s\tremaining: 1m 3s\n",
      "125:\tlearn: 0.0708892\ttotal: 11.9s\tremaining: 1m 3s\n",
      "126:\tlearn: 0.0696066\ttotal: 12s\tremaining: 1m 3s\n",
      "127:\tlearn: 0.0682889\ttotal: 12.1s\tremaining: 1m 3s\n",
      "128:\tlearn: 0.0674945\ttotal: 12.2s\tremaining: 1m 3s\n",
      "129:\tlearn: 0.0661733\ttotal: 12.3s\tremaining: 1m 3s\n",
      "130:\tlearn: 0.0656932\ttotal: 12.4s\tremaining: 1m 3s\n",
      "131:\tlearn: 0.0653665\ttotal: 12.5s\tremaining: 1m 3s\n",
      "132:\tlearn: 0.0646462\ttotal: 12.6s\tremaining: 1m 3s\n",
      "133:\tlearn: 0.0640483\ttotal: 12.6s\tremaining: 1m 2s\n",
      "134:\tlearn: 0.0628391\ttotal: 12.8s\tremaining: 1m 2s\n",
      "135:\tlearn: 0.0624186\ttotal: 12.9s\tremaining: 1m 2s\n",
      "136:\tlearn: 0.0618238\ttotal: 13s\tremaining: 1m 2s\n",
      "137:\tlearn: 0.0609126\ttotal: 13.1s\tremaining: 1m 2s\n",
      "138:\tlearn: 0.0605858\ttotal: 13.1s\tremaining: 1m 2s\n",
      "139:\tlearn: 0.0596237\ttotal: 13.2s\tremaining: 1m 2s\n",
      "140:\tlearn: 0.0589955\ttotal: 13.3s\tremaining: 1m 2s\n",
      "141:\tlearn: 0.0588313\ttotal: 13.4s\tremaining: 1m 1s\n",
      "142:\tlearn: 0.0583042\ttotal: 13.5s\tremaining: 1m 1s\n",
      "143:\tlearn: 0.0582078\ttotal: 13.5s\tremaining: 1m 1s\n",
      "144:\tlearn: 0.0577063\ttotal: 13.6s\tremaining: 1m 1s\n",
      "145:\tlearn: 0.0570632\ttotal: 13.7s\tremaining: 1m 1s\n",
      "146:\tlearn: 0.0570117\ttotal: 13.8s\tremaining: 1m 1s\n",
      "147:\tlearn: 0.0569267\ttotal: 13.8s\tremaining: 1m\n",
      "148:\tlearn: 0.0565122\ttotal: 13.9s\tremaining: 1m\n",
      "149:\tlearn: 0.0560361\ttotal: 14s\tremaining: 1m\n",
      "150:\tlearn: 0.0553975\ttotal: 14s\tremaining: 1m\n",
      "151:\tlearn: 0.0549069\ttotal: 14.1s\tremaining: 1m\n",
      "152:\tlearn: 0.0547685\ttotal: 14.2s\tremaining: 1m\n",
      "153:\tlearn: 0.0537231\ttotal: 14.4s\tremaining: 1m\n",
      "154:\tlearn: 0.0533741\ttotal: 14.4s\tremaining: 1m\n",
      "155:\tlearn: 0.0525684\ttotal: 14.5s\tremaining: 59.9s\n",
      "156:\tlearn: 0.0520439\ttotal: 14.6s\tremaining: 59.7s\n",
      "157:\tlearn: 0.0515697\ttotal: 14.7s\tremaining: 59.6s\n",
      "158:\tlearn: 0.0515374\ttotal: 14.8s\tremaining: 59.5s\n",
      "159:\tlearn: 0.0509499\ttotal: 14.8s\tremaining: 59.3s\n",
      "160:\tlearn: 0.0507308\ttotal: 14.9s\tremaining: 59.1s\n",
      "161:\tlearn: 0.0503121\ttotal: 15s\tremaining: 59s\n",
      "162:\tlearn: 0.0500196\ttotal: 15.2s\tremaining: 59.3s\n",
      "163:\tlearn: 0.0497526\ttotal: 15.3s\tremaining: 59.2s\n",
      "164:\tlearn: 0.0495406\ttotal: 15.4s\tremaining: 59.1s\n",
      "165:\tlearn: 0.0495261\ttotal: 15.5s\tremaining: 59.1s\n",
      "166:\tlearn: 0.0489446\ttotal: 15.6s\tremaining: 59.2s\n",
      "167:\tlearn: 0.0484644\ttotal: 15.7s\tremaining: 59.2s\n",
      "168:\tlearn: 0.0482642\ttotal: 15.8s\tremaining: 59.1s\n",
      "169:\tlearn: 0.0481467\ttotal: 15.9s\tremaining: 59s\n",
      "170:\tlearn: 0.0475603\ttotal: 16.1s\tremaining: 59s\n",
      "171:\tlearn: 0.0471902\ttotal: 16.1s\tremaining: 59s\n",
      "172:\tlearn: 0.0467387\ttotal: 16.3s\tremaining: 59s\n",
      "173:\tlearn: 0.0467070\ttotal: 16.4s\tremaining: 59.1s\n",
      "174:\tlearn: 0.0463190\ttotal: 16.5s\tremaining: 59s\n",
      "175:\tlearn: 0.0458622\ttotal: 16.6s\tremaining: 59s\n",
      "176:\tlearn: 0.0454759\ttotal: 16.7s\tremaining: 58.9s\n",
      "177:\tlearn: 0.0451832\ttotal: 16.9s\tremaining: 58.9s\n",
      "178:\tlearn: 0.0448552\ttotal: 16.9s\tremaining: 58.7s\n",
      "179:\tlearn: 0.0445428\ttotal: 17s\tremaining: 58.6s\n",
      "180:\tlearn: 0.0441364\ttotal: 17.1s\tremaining: 58.6s\n",
      "181:\tlearn: 0.0438098\ttotal: 17.2s\tremaining: 58.4s\n",
      "182:\tlearn: 0.0433397\ttotal: 17.2s\tremaining: 58.1s\n",
      "183:\tlearn: 0.0428861\ttotal: 17.3s\tremaining: 58s\n",
      "184:\tlearn: 0.0426411\ttotal: 17.4s\tremaining: 57.8s\n",
      "185:\tlearn: 0.0419114\ttotal: 17.5s\tremaining: 57.6s\n",
      "186:\tlearn: 0.0415119\ttotal: 17.5s\tremaining: 57.5s\n",
      "187:\tlearn: 0.0411483\ttotal: 17.6s\tremaining: 57.3s\n",
      "188:\tlearn: 0.0408642\ttotal: 17.7s\tremaining: 57.3s\n",
      "189:\tlearn: 0.0408190\ttotal: 17.8s\tremaining: 57.2s\n",
      "190:\tlearn: 0.0407862\ttotal: 17.9s\tremaining: 57.1s\n",
      "191:\tlearn: 0.0405392\ttotal: 18s\tremaining: 57s\n",
      "192:\tlearn: 0.0402459\ttotal: 18.1s\tremaining: 56.9s\n",
      "193:\tlearn: 0.0398758\ttotal: 18.2s\tremaining: 56.8s\n",
      "194:\tlearn: 0.0394949\ttotal: 18.3s\tremaining: 56.7s\n",
      "195:\tlearn: 0.0387691\ttotal: 18.3s\tremaining: 56.5s\n",
      "196:\tlearn: 0.0382317\ttotal: 18.4s\tremaining: 56.4s\n",
      "197:\tlearn: 0.0377699\ttotal: 18.5s\tremaining: 56.2s\n",
      "198:\tlearn: 0.0375605\ttotal: 18.6s\tremaining: 56.1s\n",
      "199:\tlearn: 0.0371867\ttotal: 18.7s\tremaining: 56s\n",
      "200:\tlearn: 0.0368801\ttotal: 18.7s\tremaining: 55.9s\n",
      "201:\tlearn: 0.0363989\ttotal: 18.9s\tremaining: 55.8s\n",
      "202:\tlearn: 0.0362786\ttotal: 19s\tremaining: 55.8s\n",
      "203:\tlearn: 0.0361921\ttotal: 19.1s\tremaining: 55.7s\n",
      "204:\tlearn: 0.0358887\ttotal: 19.2s\tremaining: 55.7s\n",
      "205:\tlearn: 0.0357737\ttotal: 19.3s\tremaining: 55.6s\n",
      "206:\tlearn: 0.0353700\ttotal: 19.4s\tremaining: 55.5s\n",
      "207:\tlearn: 0.0348784\ttotal: 19.5s\tremaining: 55.5s\n",
      "208:\tlearn: 0.0346268\ttotal: 19.6s\tremaining: 55.5s\n",
      "209:\tlearn: 0.0341701\ttotal: 19.7s\tremaining: 55.4s\n",
      "210:\tlearn: 0.0339136\ttotal: 19.9s\tremaining: 55.4s\n",
      "211:\tlearn: 0.0335420\ttotal: 19.9s\tremaining: 55.3s\n",
      "212:\tlearn: 0.0334115\ttotal: 20s\tremaining: 55.2s\n",
      "213:\tlearn: 0.0331551\ttotal: 20.1s\tremaining: 55.1s\n",
      "214:\tlearn: 0.0328166\ttotal: 20.2s\tremaining: 54.9s\n",
      "215:\tlearn: 0.0324594\ttotal: 20.3s\tremaining: 54.8s\n",
      "216:\tlearn: 0.0318416\ttotal: 20.3s\tremaining: 54.6s\n",
      "217:\tlearn: 0.0315916\ttotal: 20.4s\tremaining: 54.5s\n",
      "218:\tlearn: 0.0313992\ttotal: 20.5s\tremaining: 54.5s\n",
      "219:\tlearn: 0.0311188\ttotal: 20.6s\tremaining: 54.4s\n",
      "220:\tlearn: 0.0309297\ttotal: 20.7s\tremaining: 54.3s\n",
      "221:\tlearn: 0.0308120\ttotal: 20.8s\tremaining: 54.2s\n",
      "222:\tlearn: 0.0306239\ttotal: 20.9s\tremaining: 54.1s\n",
      "223:\tlearn: 0.0304222\ttotal: 21s\tremaining: 54s\n",
      "224:\tlearn: 0.0301914\ttotal: 21.1s\tremaining: 53.9s\n",
      "225:\tlearn: 0.0299570\ttotal: 21.2s\tremaining: 53.9s\n",
      "226:\tlearn: 0.0296095\ttotal: 21.3s\tremaining: 53.8s\n",
      "227:\tlearn: 0.0292393\ttotal: 21.4s\tremaining: 53.7s\n",
      "228:\tlearn: 0.0290954\ttotal: 21.5s\tremaining: 53.7s\n",
      "229:\tlearn: 0.0289870\ttotal: 21.6s\tremaining: 53.5s\n",
      "230:\tlearn: 0.0284929\ttotal: 21.7s\tremaining: 53.4s\n",
      "231:\tlearn: 0.0280732\ttotal: 21.8s\tremaining: 53.3s\n",
      "232:\tlearn: 0.0276354\ttotal: 21.8s\tremaining: 53.2s\n",
      "233:\tlearn: 0.0273642\ttotal: 21.9s\tremaining: 53.1s\n",
      "234:\tlearn: 0.0270902\ttotal: 22s\tremaining: 52.9s\n",
      "235:\tlearn: 0.0269911\ttotal: 22.1s\tremaining: 52.8s\n",
      "236:\tlearn: 0.0268003\ttotal: 22.2s\tremaining: 52.7s\n",
      "237:\tlearn: 0.0266900\ttotal: 22.3s\tremaining: 52.6s\n",
      "238:\tlearn: 0.0265482\ttotal: 22.4s\tremaining: 52.5s\n",
      "239:\tlearn: 0.0263475\ttotal: 22.5s\tremaining: 52.5s\n",
      "240:\tlearn: 0.0261663\ttotal: 22.6s\tremaining: 52.4s\n",
      "241:\tlearn: 0.0260420\ttotal: 22.7s\tremaining: 52.4s\n",
      "242:\tlearn: 0.0259005\ttotal: 22.8s\tremaining: 52.2s\n",
      "243:\tlearn: 0.0257988\ttotal: 22.9s\tremaining: 52.1s\n",
      "244:\tlearn: 0.0256433\ttotal: 22.9s\tremaining: 52s\n",
      "245:\tlearn: 0.0254075\ttotal: 23s\tremaining: 51.8s\n",
      "246:\tlearn: 0.0252863\ttotal: 23.1s\tremaining: 51.7s\n",
      "247:\tlearn: 0.0250871\ttotal: 23.2s\tremaining: 51.6s\n",
      "248:\tlearn: 0.0248681\ttotal: 23.2s\tremaining: 51.4s\n",
      "249:\tlearn: 0.0248526\ttotal: 23.3s\tremaining: 51.3s\n",
      "250:\tlearn: 0.0246982\ttotal: 23.4s\tremaining: 51.1s\n",
      "251:\tlearn: 0.0245417\ttotal: 23.4s\tremaining: 51s\n",
      "252:\tlearn: 0.0243958\ttotal: 23.5s\tremaining: 50.8s\n",
      "253:\tlearn: 0.0243439\ttotal: 23.6s\tremaining: 50.7s\n",
      "254:\tlearn: 0.0242869\ttotal: 23.7s\tremaining: 50.7s\n",
      "255:\tlearn: 0.0241820\ttotal: 23.8s\tremaining: 50.6s\n",
      "256:\tlearn: 0.0240572\ttotal: 23.9s\tremaining: 50.6s\n",
      "257:\tlearn: 0.0239137\ttotal: 24s\tremaining: 50.4s\n",
      "258:\tlearn: 0.0237912\ttotal: 24.1s\tremaining: 50.4s\n",
      "259:\tlearn: 0.0235878\ttotal: 24.2s\tremaining: 50.3s\n",
      "260:\tlearn: 0.0235657\ttotal: 24.3s\tremaining: 50.2s\n",
      "261:\tlearn: 0.0235653\ttotal: 24.4s\tremaining: 50.1s\n",
      "262:\tlearn: 0.0234824\ttotal: 24.4s\tremaining: 49.9s\n",
      "263:\tlearn: 0.0234367\ttotal: 24.5s\tremaining: 49.8s\n",
      "264:\tlearn: 0.0233509\ttotal: 24.6s\tremaining: 49.7s\n",
      "265:\tlearn: 0.0233150\ttotal: 24.7s\tremaining: 49.6s\n",
      "266:\tlearn: 0.0232618\ttotal: 24.8s\tremaining: 49.5s\n",
      "267:\tlearn: 0.0231234\ttotal: 24.9s\tremaining: 49.4s\n",
      "268:\tlearn: 0.0230141\ttotal: 25s\tremaining: 49.3s\n",
      "269:\tlearn: 0.0229039\ttotal: 25.1s\tremaining: 49.3s\n",
      "270:\tlearn: 0.0227588\ttotal: 25.2s\tremaining: 49.2s\n",
      "271:\tlearn: 0.0224400\ttotal: 25.3s\tremaining: 49.1s\n",
      "272:\tlearn: 0.0222722\ttotal: 25.4s\tremaining: 49s\n",
      "273:\tlearn: 0.0220522\ttotal: 25.5s\tremaining: 49s\n",
      "274:\tlearn: 0.0219629\ttotal: 25.6s\tremaining: 48.8s\n",
      "275:\tlearn: 0.0218999\ttotal: 25.7s\tremaining: 48.8s\n",
      "276:\tlearn: 0.0217092\ttotal: 25.8s\tremaining: 48.7s\n",
      "277:\tlearn: 0.0215396\ttotal: 25.9s\tremaining: 48.6s\n",
      "278:\tlearn: 0.0213645\ttotal: 26s\tremaining: 48.5s\n",
      "279:\tlearn: 0.0212424\ttotal: 26.1s\tremaining: 48.5s\n",
      "280:\tlearn: 0.0211675\ttotal: 26.2s\tremaining: 48.4s\n",
      "281:\tlearn: 0.0209542\ttotal: 26.3s\tremaining: 48.3s\n",
      "282:\tlearn: 0.0208504\ttotal: 26.4s\tremaining: 48.2s\n",
      "283:\tlearn: 0.0207881\ttotal: 26.5s\tremaining: 48.1s\n",
      "284:\tlearn: 0.0206047\ttotal: 26.5s\tremaining: 48s\n",
      "285:\tlearn: 0.0204556\ttotal: 26.7s\tremaining: 47.9s\n",
      "286:\tlearn: 0.0203970\ttotal: 26.7s\tremaining: 47.8s\n",
      "287:\tlearn: 0.0202640\ttotal: 26.8s\tremaining: 47.7s\n",
      "288:\tlearn: 0.0201344\ttotal: 26.9s\tremaining: 47.6s\n",
      "289:\tlearn: 0.0199822\ttotal: 27.1s\tremaining: 47.6s\n",
      "290:\tlearn: 0.0198504\ttotal: 27.2s\tremaining: 47.6s\n",
      "291:\tlearn: 0.0198342\ttotal: 27.3s\tremaining: 47.5s\n",
      "292:\tlearn: 0.0196052\ttotal: 27.4s\tremaining: 47.3s\n",
      "293:\tlearn: 0.0194317\ttotal: 27.4s\tremaining: 47.2s\n",
      "294:\tlearn: 0.0193756\ttotal: 27.5s\tremaining: 47.1s\n",
      "295:\tlearn: 0.0193191\ttotal: 27.6s\tremaining: 47s\n",
      "296:\tlearn: 0.0191287\ttotal: 27.7s\tremaining: 46.9s\n",
      "297:\tlearn: 0.0190077\ttotal: 27.7s\tremaining: 46.7s\n",
      "298:\tlearn: 0.0189357\ttotal: 27.8s\tremaining: 46.6s\n",
      "299:\tlearn: 0.0188273\ttotal: 27.9s\tremaining: 46.5s\n",
      "300:\tlearn: 0.0187943\ttotal: 28.1s\tremaining: 46.5s\n",
      "301:\tlearn: 0.0187044\ttotal: 28.2s\tremaining: 46.5s\n",
      "302:\tlearn: 0.0186657\ttotal: 28.3s\tremaining: 46.4s\n",
      "303:\tlearn: 0.0186425\ttotal: 28.4s\tremaining: 46.3s\n",
      "304:\tlearn: 0.0185009\ttotal: 28.5s\tremaining: 46.2s\n",
      "305:\tlearn: 0.0185008\ttotal: 28.5s\tremaining: 46.1s\n",
      "306:\tlearn: 0.0184192\ttotal: 28.6s\tremaining: 46s\n",
      "307:\tlearn: 0.0182632\ttotal: 28.7s\tremaining: 45.9s\n",
      "308:\tlearn: 0.0181295\ttotal: 28.8s\tremaining: 45.8s\n",
      "309:\tlearn: 0.0179504\ttotal: 28.9s\tremaining: 45.7s\n",
      "310:\tlearn: 0.0179278\ttotal: 29s\tremaining: 45.6s\n",
      "311:\tlearn: 0.0179023\ttotal: 29.1s\tremaining: 45.5s\n",
      "312:\tlearn: 0.0178507\ttotal: 29.2s\tremaining: 45.4s\n",
      "313:\tlearn: 0.0178507\ttotal: 29.3s\tremaining: 45.4s\n",
      "314:\tlearn: 0.0177338\ttotal: 29.4s\tremaining: 45.3s\n",
      "315:\tlearn: 0.0176309\ttotal: 29.5s\tremaining: 45.1s\n",
      "316:\tlearn: 0.0175962\ttotal: 29.5s\tremaining: 45s\n",
      "317:\tlearn: 0.0175491\ttotal: 29.6s\tremaining: 44.9s\n",
      "318:\tlearn: 0.0174346\ttotal: 29.7s\tremaining: 44.7s\n",
      "319:\tlearn: 0.0173432\ttotal: 29.8s\tremaining: 44.7s\n",
      "320:\tlearn: 0.0172911\ttotal: 29.8s\tremaining: 44.5s\n",
      "321:\tlearn: 0.0171773\ttotal: 29.9s\tremaining: 44.5s\n",
      "322:\tlearn: 0.0171006\ttotal: 30s\tremaining: 44.3s\n",
      "323:\tlearn: 0.0169666\ttotal: 30.1s\tremaining: 44.2s\n",
      "324:\tlearn: 0.0169048\ttotal: 30.2s\tremaining: 44.1s\n",
      "325:\tlearn: 0.0168289\ttotal: 30.3s\tremaining: 44.1s\n",
      "326:\tlearn: 0.0166613\ttotal: 30.4s\tremaining: 43.9s\n",
      "327:\tlearn: 0.0165421\ttotal: 30.5s\tremaining: 43.9s\n",
      "328:\tlearn: 0.0164694\ttotal: 30.6s\tremaining: 43.8s\n",
      "329:\tlearn: 0.0164218\ttotal: 30.7s\tremaining: 43.7s\n",
      "330:\tlearn: 0.0163350\ttotal: 30.8s\tremaining: 43.6s\n",
      "331:\tlearn: 0.0162951\ttotal: 30.9s\tremaining: 43.6s\n",
      "332:\tlearn: 0.0162540\ttotal: 31s\tremaining: 43.5s\n",
      "333:\tlearn: 0.0161614\ttotal: 31.1s\tremaining: 43.3s\n",
      "334:\tlearn: 0.0161151\ttotal: 31.1s\tremaining: 43.2s\n",
      "335:\tlearn: 0.0161002\ttotal: 31.2s\tremaining: 43.1s\n",
      "336:\tlearn: 0.0160483\ttotal: 31.3s\tremaining: 43s\n",
      "337:\tlearn: 0.0160005\ttotal: 31.4s\tremaining: 42.9s\n",
      "338:\tlearn: 0.0158626\ttotal: 31.5s\tremaining: 42.8s\n",
      "339:\tlearn: 0.0157601\ttotal: 31.6s\tremaining: 42.7s\n",
      "340:\tlearn: 0.0157600\ttotal: 31.7s\tremaining: 42.6s\n",
      "341:\tlearn: 0.0157154\ttotal: 31.7s\tremaining: 42.5s\n",
      "342:\tlearn: 0.0156769\ttotal: 31.8s\tremaining: 42.4s\n",
      "343:\tlearn: 0.0156211\ttotal: 31.9s\tremaining: 42.3s\n",
      "344:\tlearn: 0.0155053\ttotal: 32s\tremaining: 42.2s\n",
      "345:\tlearn: 0.0154763\ttotal: 32.1s\tremaining: 42.1s\n",
      "346:\tlearn: 0.0154763\ttotal: 32.2s\tremaining: 42s\n",
      "347:\tlearn: 0.0154763\ttotal: 32.3s\tremaining: 41.9s\n",
      "348:\tlearn: 0.0154217\ttotal: 32.3s\tremaining: 41.8s\n",
      "349:\tlearn: 0.0154216\ttotal: 32.4s\tremaining: 41.7s\n",
      "350:\tlearn: 0.0153932\ttotal: 32.5s\tremaining: 41.6s\n",
      "351:\tlearn: 0.0153341\ttotal: 32.6s\tremaining: 41.5s\n",
      "352:\tlearn: 0.0153159\ttotal: 32.7s\tremaining: 41.4s\n",
      "353:\tlearn: 0.0152583\ttotal: 32.7s\tremaining: 41.3s\n",
      "354:\tlearn: 0.0152381\ttotal: 32.8s\tremaining: 41.2s\n",
      "355:\tlearn: 0.0152067\ttotal: 33s\tremaining: 41.1s\n",
      "356:\tlearn: 0.0151566\ttotal: 33s\tremaining: 41s\n",
      "357:\tlearn: 0.0150960\ttotal: 33.1s\tremaining: 40.9s\n",
      "358:\tlearn: 0.0149986\ttotal: 33.2s\tremaining: 40.8s\n",
      "359:\tlearn: 0.0149330\ttotal: 33.3s\tremaining: 40.6s\n",
      "360:\tlearn: 0.0148934\ttotal: 33.3s\tremaining: 40.5s\n",
      "361:\tlearn: 0.0148136\ttotal: 33.4s\tremaining: 40.4s\n",
      "362:\tlearn: 0.0147497\ttotal: 33.5s\tremaining: 40.3s\n",
      "363:\tlearn: 0.0145468\ttotal: 33.6s\tremaining: 40.2s\n",
      "364:\tlearn: 0.0144848\ttotal: 33.7s\tremaining: 40.2s\n",
      "365:\tlearn: 0.0144336\ttotal: 33.8s\tremaining: 40.1s\n",
      "366:\tlearn: 0.0144335\ttotal: 33.9s\tremaining: 40s\n",
      "367:\tlearn: 0.0143484\ttotal: 34s\tremaining: 39.9s\n",
      "368:\tlearn: 0.0142762\ttotal: 34s\tremaining: 39.8s\n",
      "369:\tlearn: 0.0142191\ttotal: 34.1s\tremaining: 39.7s\n",
      "370:\tlearn: 0.0141599\ttotal: 34.3s\tremaining: 39.6s\n",
      "371:\tlearn: 0.0140723\ttotal: 34.4s\tremaining: 39.5s\n",
      "372:\tlearn: 0.0139113\ttotal: 34.4s\tremaining: 39.4s\n",
      "373:\tlearn: 0.0138267\ttotal: 34.5s\tremaining: 39.3s\n",
      "374:\tlearn: 0.0137628\ttotal: 34.6s\tremaining: 39.3s\n",
      "375:\tlearn: 0.0136852\ttotal: 34.8s\tremaining: 39.2s\n",
      "376:\tlearn: 0.0136850\ttotal: 34.8s\tremaining: 39.1s\n",
      "377:\tlearn: 0.0136373\ttotal: 34.9s\tremaining: 39s\n",
      "378:\tlearn: 0.0136371\ttotal: 35s\tremaining: 38.9s\n",
      "379:\tlearn: 0.0136053\ttotal: 35.1s\tremaining: 38.8s\n",
      "380:\tlearn: 0.0135777\ttotal: 35.2s\tremaining: 38.7s\n",
      "381:\tlearn: 0.0135320\ttotal: 35.3s\tremaining: 38.6s\n",
      "382:\tlearn: 0.0134794\ttotal: 35.4s\tremaining: 38.5s\n",
      "383:\tlearn: 0.0134298\ttotal: 35.5s\tremaining: 38.4s\n",
      "384:\tlearn: 0.0133071\ttotal: 35.6s\tremaining: 38.3s\n",
      "385:\tlearn: 0.0132191\ttotal: 35.7s\tremaining: 38.3s\n",
      "386:\tlearn: 0.0131588\ttotal: 35.7s\tremaining: 38.1s\n",
      "387:\tlearn: 0.0131142\ttotal: 35.8s\tremaining: 38s\n",
      "388:\tlearn: 0.0129838\ttotal: 35.9s\tremaining: 38s\n",
      "389:\tlearn: 0.0129295\ttotal: 36s\tremaining: 37.9s\n",
      "390:\tlearn: 0.0128671\ttotal: 36.2s\tremaining: 37.8s\n",
      "391:\tlearn: 0.0127982\ttotal: 36.3s\tremaining: 37.7s\n",
      "392:\tlearn: 0.0127982\ttotal: 36.3s\tremaining: 37.6s\n",
      "393:\tlearn: 0.0127307\ttotal: 36.4s\tremaining: 37.5s\n",
      "394:\tlearn: 0.0126830\ttotal: 36.5s\tremaining: 37.4s\n",
      "395:\tlearn: 0.0125858\ttotal: 36.6s\tremaining: 37.3s\n",
      "396:\tlearn: 0.0125472\ttotal: 36.7s\tremaining: 37.2s\n",
      "397:\tlearn: 0.0124768\ttotal: 36.8s\tremaining: 37.1s\n",
      "398:\tlearn: 0.0124765\ttotal: 36.8s\tremaining: 37s\n",
      "399:\tlearn: 0.0124625\ttotal: 36.9s\tremaining: 36.9s\n",
      "400:\tlearn: 0.0124463\ttotal: 37.1s\tremaining: 36.9s\n",
      "401:\tlearn: 0.0124463\ttotal: 37.2s\tremaining: 36.8s\n",
      "402:\tlearn: 0.0124462\ttotal: 37.3s\tremaining: 36.7s\n",
      "403:\tlearn: 0.0123668\ttotal: 37.3s\tremaining: 36.6s\n",
      "404:\tlearn: 0.0122694\ttotal: 37.4s\tremaining: 36.5s\n",
      "405:\tlearn: 0.0122030\ttotal: 37.5s\tremaining: 36.4s\n",
      "406:\tlearn: 0.0122029\ttotal: 37.5s\tremaining: 36.3s\n",
      "407:\tlearn: 0.0122029\ttotal: 37.6s\tremaining: 36.1s\n",
      "408:\tlearn: 0.0121080\ttotal: 37.7s\tremaining: 36s\n",
      "409:\tlearn: 0.0120246\ttotal: 37.7s\tremaining: 35.9s\n",
      "410:\tlearn: 0.0119681\ttotal: 37.8s\tremaining: 35.8s\n",
      "411:\tlearn: 0.0119271\ttotal: 37.9s\tremaining: 35.7s\n",
      "412:\tlearn: 0.0118576\ttotal: 38s\tremaining: 35.6s\n",
      "413:\tlearn: 0.0117981\ttotal: 38.1s\tremaining: 35.5s\n",
      "414:\tlearn: 0.0117534\ttotal: 38.2s\tremaining: 35.4s\n",
      "415:\tlearn: 0.0116965\ttotal: 38.3s\tremaining: 35.3s\n",
      "416:\tlearn: 0.0116513\ttotal: 38.4s\tremaining: 35.3s\n",
      "417:\tlearn: 0.0116280\ttotal: 38.5s\tremaining: 35.2s\n",
      "418:\tlearn: 0.0116280\ttotal: 38.6s\tremaining: 35.1s\n",
      "419:\tlearn: 0.0116280\ttotal: 38.7s\tremaining: 35s\n",
      "420:\tlearn: 0.0115990\ttotal: 38.8s\tremaining: 34.9s\n",
      "421:\tlearn: 0.0115987\ttotal: 38.8s\tremaining: 34.8s\n",
      "422:\tlearn: 0.0115258\ttotal: 38.9s\tremaining: 34.7s\n",
      "423:\tlearn: 0.0115257\ttotal: 39s\tremaining: 34.6s\n",
      "424:\tlearn: 0.0114862\ttotal: 39.1s\tremaining: 34.5s\n",
      "425:\tlearn: 0.0114862\ttotal: 39.2s\tremaining: 34.4s\n",
      "426:\tlearn: 0.0114382\ttotal: 39.3s\tremaining: 34.3s\n",
      "427:\tlearn: 0.0113956\ttotal: 39.4s\tremaining: 34.3s\n",
      "428:\tlearn: 0.0113774\ttotal: 39.5s\tremaining: 34.2s\n",
      "429:\tlearn: 0.0113758\ttotal: 39.6s\tremaining: 34.1s\n",
      "430:\tlearn: 0.0113743\ttotal: 39.8s\tremaining: 34s\n",
      "431:\tlearn: 0.0112795\ttotal: 39.9s\tremaining: 34s\n",
      "432:\tlearn: 0.0112478\ttotal: 40s\tremaining: 33.9s\n",
      "433:\tlearn: 0.0111927\ttotal: 40.1s\tremaining: 33.8s\n",
      "434:\tlearn: 0.0111475\ttotal: 40.2s\tremaining: 33.7s\n",
      "435:\tlearn: 0.0111475\ttotal: 40.3s\tremaining: 33.6s\n",
      "436:\tlearn: 0.0111048\ttotal: 40.4s\tremaining: 33.6s\n",
      "437:\tlearn: 0.0110838\ttotal: 40.5s\tremaining: 33.5s\n",
      "438:\tlearn: 0.0110838\ttotal: 40.7s\tremaining: 33.4s\n",
      "439:\tlearn: 0.0110837\ttotal: 40.8s\tremaining: 33.4s\n",
      "440:\tlearn: 0.0110837\ttotal: 40.9s\tremaining: 33.3s\n",
      "441:\tlearn: 0.0110571\ttotal: 40.9s\tremaining: 33.1s\n",
      "442:\tlearn: 0.0110571\ttotal: 41s\tremaining: 33s\n",
      "443:\tlearn: 0.0110456\ttotal: 41.1s\tremaining: 32.9s\n",
      "444:\tlearn: 0.0110456\ttotal: 41.1s\tremaining: 32.8s\n",
      "445:\tlearn: 0.0110456\ttotal: 41.2s\tremaining: 32.7s\n",
      "446:\tlearn: 0.0110456\ttotal: 41.3s\tremaining: 32.6s\n",
      "447:\tlearn: 0.0110452\ttotal: 41.4s\tremaining: 32.5s\n",
      "448:\tlearn: 0.0110452\ttotal: 41.5s\tremaining: 32.4s\n",
      "449:\tlearn: 0.0110146\ttotal: 41.6s\tremaining: 32.4s\n",
      "450:\tlearn: 0.0110145\ttotal: 41.7s\tremaining: 32.2s\n",
      "451:\tlearn: 0.0110036\ttotal: 41.7s\tremaining: 32.1s\n",
      "452:\tlearn: 0.0110033\ttotal: 41.8s\tremaining: 32s\n",
      "453:\tlearn: 0.0110021\ttotal: 41.9s\tremaining: 31.9s\n",
      "454:\tlearn: 0.0110021\ttotal: 42s\tremaining: 31.9s\n",
      "455:\tlearn: 0.0110021\ttotal: 42.2s\tremaining: 31.8s\n",
      "456:\tlearn: 0.0110019\ttotal: 42.3s\tremaining: 31.7s\n",
      "457:\tlearn: 0.0109892\ttotal: 42.4s\tremaining: 31.6s\n",
      "458:\tlearn: 0.0109889\ttotal: 42.4s\tremaining: 31.5s\n",
      "459:\tlearn: 0.0109889\ttotal: 42.5s\tremaining: 31.4s\n",
      "460:\tlearn: 0.0109887\ttotal: 42.6s\tremaining: 31.3s\n",
      "461:\tlearn: 0.0109883\ttotal: 42.7s\tremaining: 31.2s\n",
      "462:\tlearn: 0.0109882\ttotal: 42.7s\tremaining: 31.1s\n",
      "463:\tlearn: 0.0109882\ttotal: 42.8s\tremaining: 31s\n",
      "464:\tlearn: 0.0109881\ttotal: 42.9s\tremaining: 30.9s\n",
      "465:\tlearn: 0.0109537\ttotal: 43s\tremaining: 30.8s\n",
      "466:\tlearn: 0.0109256\ttotal: 43.1s\tremaining: 30.7s\n",
      "467:\tlearn: 0.0109250\ttotal: 43.2s\tremaining: 30.6s\n",
      "468:\tlearn: 0.0108435\ttotal: 43.3s\tremaining: 30.5s\n",
      "469:\tlearn: 0.0108435\ttotal: 43.4s\tremaining: 30.5s\n",
      "470:\tlearn: 0.0108435\ttotal: 43.5s\tremaining: 30.4s\n",
      "471:\tlearn: 0.0108434\ttotal: 43.5s\tremaining: 30.2s\n",
      "472:\tlearn: 0.0108432\ttotal: 43.6s\tremaining: 30.2s\n",
      "473:\tlearn: 0.0108430\ttotal: 43.8s\tremaining: 30.1s\n",
      "474:\tlearn: 0.0108037\ttotal: 43.9s\tremaining: 30s\n",
      "475:\tlearn: 0.0108036\ttotal: 43.9s\tremaining: 29.9s\n",
      "476:\tlearn: 0.0107834\ttotal: 44s\tremaining: 29.8s\n",
      "477:\tlearn: 0.0107832\ttotal: 44.2s\tremaining: 29.7s\n",
      "478:\tlearn: 0.0107430\ttotal: 44.2s\tremaining: 29.7s\n",
      "479:\tlearn: 0.0106993\ttotal: 44.3s\tremaining: 29.5s\n",
      "480:\tlearn: 0.0106991\ttotal: 44.4s\tremaining: 29.5s\n",
      "481:\tlearn: 0.0106990\ttotal: 44.5s\tremaining: 29.4s\n",
      "482:\tlearn: 0.0106990\ttotal: 44.7s\tremaining: 29.3s\n",
      "483:\tlearn: 0.0106302\ttotal: 44.8s\tremaining: 29.2s\n",
      "484:\tlearn: 0.0106302\ttotal: 44.8s\tremaining: 29.1s\n",
      "485:\tlearn: 0.0106302\ttotal: 44.9s\tremaining: 29s\n",
      "486:\tlearn: 0.0105810\ttotal: 45s\tremaining: 28.9s\n",
      "487:\tlearn: 0.0105798\ttotal: 45s\tremaining: 28.8s\n",
      "488:\tlearn: 0.0105798\ttotal: 45.1s\tremaining: 28.7s\n",
      "489:\tlearn: 0.0105180\ttotal: 45.2s\tremaining: 28.6s\n",
      "490:\tlearn: 0.0104724\ttotal: 45.3s\tremaining: 28.5s\n",
      "491:\tlearn: 0.0104218\ttotal: 45.4s\tremaining: 28.4s\n",
      "492:\tlearn: 0.0103657\ttotal: 45.5s\tremaining: 28.3s\n",
      "493:\tlearn: 0.0102628\ttotal: 45.5s\tremaining: 28.2s\n",
      "494:\tlearn: 0.0102402\ttotal: 45.6s\tremaining: 28.1s\n",
      "495:\tlearn: 0.0102402\ttotal: 45.7s\tremaining: 28s\n",
      "496:\tlearn: 0.0102285\ttotal: 45.8s\tremaining: 27.9s\n",
      "497:\tlearn: 0.0101827\ttotal: 45.9s\tremaining: 27.8s\n",
      "498:\tlearn: 0.0101826\ttotal: 46s\tremaining: 27.7s\n",
      "499:\tlearn: 0.0101825\ttotal: 46s\tremaining: 27.6s\n",
      "500:\tlearn: 0.0101825\ttotal: 46.1s\tremaining: 27.5s\n",
      "501:\tlearn: 0.0101825\ttotal: 46.2s\tremaining: 27.4s\n",
      "502:\tlearn: 0.0101823\ttotal: 46.3s\tremaining: 27.3s\n",
      "503:\tlearn: 0.0101550\ttotal: 46.4s\tremaining: 27.2s\n",
      "504:\tlearn: 0.0101470\ttotal: 46.5s\tremaining: 27.2s\n",
      "505:\tlearn: 0.0101470\ttotal: 46.6s\tremaining: 27.1s\n",
      "506:\tlearn: 0.0101470\ttotal: 46.7s\tremaining: 27s\n",
      "507:\tlearn: 0.0101467\ttotal: 46.8s\tremaining: 26.9s\n",
      "508:\tlearn: 0.0101465\ttotal: 46.9s\tremaining: 26.8s\n",
      "509:\tlearn: 0.0101464\ttotal: 47s\tremaining: 26.7s\n",
      "510:\tlearn: 0.0101463\ttotal: 47.1s\tremaining: 26.6s\n",
      "511:\tlearn: 0.0100930\ttotal: 47.2s\tremaining: 26.5s\n",
      "512:\tlearn: 0.0100127\ttotal: 47.3s\tremaining: 26.5s\n",
      "513:\tlearn: 0.0100125\ttotal: 47.4s\tremaining: 26.4s\n",
      "514:\tlearn: 0.0100114\ttotal: 47.4s\tremaining: 26.2s\n",
      "515:\tlearn: 0.0100110\ttotal: 47.6s\tremaining: 26.2s\n",
      "516:\tlearn: 0.0100108\ttotal: 47.7s\tremaining: 26.1s\n",
      "517:\tlearn: 0.0099432\ttotal: 47.8s\tremaining: 26s\n",
      "518:\tlearn: 0.0099431\ttotal: 47.9s\tremaining: 25.9s\n",
      "519:\tlearn: 0.0099428\ttotal: 47.9s\tremaining: 25.8s\n",
      "520:\tlearn: 0.0099428\ttotal: 48s\tremaining: 25.7s\n",
      "521:\tlearn: 0.0098759\ttotal: 48.1s\tremaining: 25.6s\n",
      "522:\tlearn: 0.0098495\ttotal: 48.2s\tremaining: 25.5s\n",
      "523:\tlearn: 0.0098495\ttotal: 48.3s\tremaining: 25.4s\n",
      "524:\tlearn: 0.0098495\ttotal: 48.4s\tremaining: 25.4s\n",
      "525:\tlearn: 0.0098489\ttotal: 48.5s\tremaining: 25.2s\n",
      "526:\tlearn: 0.0098327\ttotal: 48.5s\tremaining: 25.1s\n",
      "527:\tlearn: 0.0098327\ttotal: 48.6s\tremaining: 25s\n",
      "528:\tlearn: 0.0098314\ttotal: 48.7s\tremaining: 24.9s\n",
      "529:\tlearn: 0.0098313\ttotal: 48.8s\tremaining: 24.8s\n",
      "530:\tlearn: 0.0097934\ttotal: 48.9s\tremaining: 24.7s\n",
      "531:\tlearn: 0.0097934\ttotal: 49s\tremaining: 24.7s\n",
      "532:\tlearn: 0.0097579\ttotal: 49.1s\tremaining: 24.6s\n",
      "533:\tlearn: 0.0097576\ttotal: 49.2s\tremaining: 24.5s\n",
      "534:\tlearn: 0.0097007\ttotal: 49.2s\tremaining: 24.4s\n",
      "535:\tlearn: 0.0097006\ttotal: 49.3s\tremaining: 24.3s\n",
      "536:\tlearn: 0.0097006\ttotal: 49.4s\tremaining: 24.2s\n",
      "537:\tlearn: 0.0096689\ttotal: 49.5s\tremaining: 24.1s\n",
      "538:\tlearn: 0.0096273\ttotal: 49.6s\tremaining: 24s\n",
      "539:\tlearn: 0.0095982\ttotal: 49.7s\tremaining: 23.9s\n",
      "540:\tlearn: 0.0095982\ttotal: 49.8s\tremaining: 23.9s\n",
      "541:\tlearn: 0.0095982\ttotal: 49.9s\tremaining: 23.8s\n",
      "542:\tlearn: 0.0095982\ttotal: 50s\tremaining: 23.7s\n",
      "543:\tlearn: 0.0095431\ttotal: 50.1s\tremaining: 23.6s\n",
      "544:\tlearn: 0.0094924\ttotal: 50.1s\tremaining: 23.5s\n",
      "545:\tlearn: 0.0094462\ttotal: 50.2s\tremaining: 23.3s\n",
      "546:\tlearn: 0.0093952\ttotal: 50.3s\tremaining: 23.3s\n",
      "547:\tlearn: 0.0093952\ttotal: 50.4s\tremaining: 23.2s\n",
      "548:\tlearn: 0.0093952\ttotal: 50.5s\tremaining: 23.1s\n",
      "549:\tlearn: 0.0093951\ttotal: 50.5s\tremaining: 23s\n",
      "550:\tlearn: 0.0093473\ttotal: 50.6s\tremaining: 22.9s\n",
      "551:\tlearn: 0.0093458\ttotal: 50.7s\tremaining: 22.8s\n",
      "552:\tlearn: 0.0093458\ttotal: 50.8s\tremaining: 22.7s\n",
      "553:\tlearn: 0.0093457\ttotal: 50.9s\tremaining: 22.6s\n",
      "554:\tlearn: 0.0093456\ttotal: 51s\tremaining: 22.5s\n",
      "555:\tlearn: 0.0093383\ttotal: 51.1s\tremaining: 22.4s\n",
      "556:\tlearn: 0.0093315\ttotal: 51.1s\tremaining: 22.3s\n",
      "557:\tlearn: 0.0093223\ttotal: 51.2s\tremaining: 22.2s\n",
      "558:\tlearn: 0.0093223\ttotal: 51.3s\tremaining: 22.1s\n",
      "559:\tlearn: 0.0093194\ttotal: 51.4s\tremaining: 22s\n",
      "560:\tlearn: 0.0093194\ttotal: 51.5s\tremaining: 21.9s\n",
      "561:\tlearn: 0.0093192\ttotal: 51.5s\tremaining: 21.8s\n",
      "562:\tlearn: 0.0093190\ttotal: 51.6s\tremaining: 21.7s\n",
      "563:\tlearn: 0.0093190\ttotal: 51.7s\tremaining: 21.6s\n",
      "564:\tlearn: 0.0093183\ttotal: 51.9s\tremaining: 21.6s\n",
      "565:\tlearn: 0.0092958\ttotal: 52s\tremaining: 21.5s\n",
      "566:\tlearn: 0.0092958\ttotal: 52s\tremaining: 21.4s\n",
      "567:\tlearn: 0.0092957\ttotal: 52.1s\tremaining: 21.3s\n",
      "568:\tlearn: 0.0092957\ttotal: 52.2s\tremaining: 21.2s\n",
      "569:\tlearn: 0.0092956\ttotal: 52.3s\tremaining: 21.1s\n",
      "570:\tlearn: 0.0092956\ttotal: 52.4s\tremaining: 21s\n",
      "571:\tlearn: 0.0092617\ttotal: 52.5s\tremaining: 20.9s\n",
      "572:\tlearn: 0.0092617\ttotal: 52.6s\tremaining: 20.8s\n",
      "573:\tlearn: 0.0092617\ttotal: 52.7s\tremaining: 20.7s\n",
      "574:\tlearn: 0.0092616\ttotal: 52.8s\tremaining: 20.6s\n",
      "575:\tlearn: 0.0092616\ttotal: 52.9s\tremaining: 20.6s\n",
      "576:\tlearn: 0.0092616\ttotal: 52.9s\tremaining: 20.5s\n",
      "577:\tlearn: 0.0092615\ttotal: 53s\tremaining: 20.4s\n",
      "578:\tlearn: 0.0092615\ttotal: 53.1s\tremaining: 20.3s\n",
      "579:\tlearn: 0.0092484\ttotal: 53.2s\tremaining: 20.2s\n",
      "580:\tlearn: 0.0092147\ttotal: 53.3s\tremaining: 20.1s\n",
      "581:\tlearn: 0.0092146\ttotal: 53.4s\tremaining: 20s\n",
      "582:\tlearn: 0.0092146\ttotal: 53.5s\tremaining: 19.9s\n",
      "583:\tlearn: 0.0092146\ttotal: 53.6s\tremaining: 19.8s\n",
      "584:\tlearn: 0.0092139\ttotal: 53.7s\tremaining: 19.7s\n",
      "585:\tlearn: 0.0092135\ttotal: 53.7s\tremaining: 19.6s\n",
      "586:\tlearn: 0.0091470\ttotal: 53.8s\tremaining: 19.5s\n",
      "587:\tlearn: 0.0091123\ttotal: 53.9s\tremaining: 19.4s\n",
      "588:\tlearn: 0.0091123\ttotal: 54s\tremaining: 19.3s\n",
      "589:\tlearn: 0.0090843\ttotal: 54.1s\tremaining: 19.3s\n",
      "590:\tlearn: 0.0090842\ttotal: 54.2s\tremaining: 19.2s\n",
      "591:\tlearn: 0.0090836\ttotal: 54.3s\tremaining: 19.1s\n",
      "592:\tlearn: 0.0090835\ttotal: 54.3s\tremaining: 19s\n",
      "593:\tlearn: 0.0090835\ttotal: 54.4s\tremaining: 18.9s\n",
      "594:\tlearn: 0.0090834\ttotal: 54.5s\tremaining: 18.8s\n",
      "595:\tlearn: 0.0090833\ttotal: 54.6s\tremaining: 18.7s\n",
      "596:\tlearn: 0.0090832\ttotal: 54.6s\tremaining: 18.6s\n",
      "597:\tlearn: 0.0090373\ttotal: 54.7s\tremaining: 18.5s\n",
      "598:\tlearn: 0.0089889\ttotal: 54.8s\tremaining: 18.4s\n",
      "599:\tlearn: 0.0089888\ttotal: 54.9s\tremaining: 18.3s\n",
      "600:\tlearn: 0.0089888\ttotal: 55s\tremaining: 18.2s\n",
      "601:\tlearn: 0.0089887\ttotal: 55.1s\tremaining: 18.1s\n",
      "602:\tlearn: 0.0089398\ttotal: 55.2s\tremaining: 18s\n",
      "603:\tlearn: 0.0088851\ttotal: 55.3s\tremaining: 17.9s\n",
      "604:\tlearn: 0.0088851\ttotal: 55.4s\tremaining: 17.8s\n",
      "605:\tlearn: 0.0088627\ttotal: 55.5s\tremaining: 17.8s\n",
      "606:\tlearn: 0.0088626\ttotal: 55.6s\tremaining: 17.7s\n",
      "607:\tlearn: 0.0088625\ttotal: 55.7s\tremaining: 17.6s\n",
      "608:\tlearn: 0.0088619\ttotal: 55.8s\tremaining: 17.5s\n",
      "609:\tlearn: 0.0088619\ttotal: 55.8s\tremaining: 17.4s\n",
      "610:\tlearn: 0.0088618\ttotal: 55.9s\tremaining: 17.3s\n",
      "611:\tlearn: 0.0088618\ttotal: 56s\tremaining: 17.2s\n",
      "612:\tlearn: 0.0088618\ttotal: 56.1s\tremaining: 17.1s\n",
      "613:\tlearn: 0.0088617\ttotal: 56.2s\tremaining: 17s\n",
      "614:\tlearn: 0.0088590\ttotal: 56.3s\tremaining: 16.9s\n",
      "615:\tlearn: 0.0088589\ttotal: 56.4s\tremaining: 16.8s\n",
      "616:\tlearn: 0.0088534\ttotal: 56.4s\tremaining: 16.7s\n",
      "617:\tlearn: 0.0088534\ttotal: 56.5s\tremaining: 16.7s\n",
      "618:\tlearn: 0.0088534\ttotal: 56.6s\tremaining: 16.6s\n",
      "619:\tlearn: 0.0088534\ttotal: 56.7s\tremaining: 16.5s\n",
      "620:\tlearn: 0.0088534\ttotal: 56.8s\tremaining: 16.4s\n",
      "621:\tlearn: 0.0088533\ttotal: 56.9s\tremaining: 16.3s\n",
      "622:\tlearn: 0.0088533\ttotal: 57s\tremaining: 16.2s\n",
      "623:\tlearn: 0.0088526\ttotal: 57.1s\tremaining: 16.1s\n",
      "624:\tlearn: 0.0088526\ttotal: 57.1s\tremaining: 16s\n",
      "625:\tlearn: 0.0088526\ttotal: 57.2s\tremaining: 15.9s\n",
      "626:\tlearn: 0.0088526\ttotal: 57.3s\tremaining: 15.8s\n",
      "627:\tlearn: 0.0088525\ttotal: 57.4s\tremaining: 15.7s\n",
      "628:\tlearn: 0.0088524\ttotal: 57.5s\tremaining: 15.6s\n",
      "629:\tlearn: 0.0088524\ttotal: 57.6s\tremaining: 15.5s\n",
      "630:\tlearn: 0.0088523\ttotal: 57.7s\tremaining: 15.4s\n",
      "631:\tlearn: 0.0088522\ttotal: 57.8s\tremaining: 15.4s\n",
      "632:\tlearn: 0.0088270\ttotal: 57.8s\tremaining: 15.3s\n",
      "633:\tlearn: 0.0088269\ttotal: 57.9s\tremaining: 15.2s\n",
      "634:\tlearn: 0.0088267\ttotal: 58s\tremaining: 15.1s\n",
      "635:\tlearn: 0.0088266\ttotal: 58.1s\tremaining: 15s\n",
      "636:\tlearn: 0.0087772\ttotal: 58.2s\tremaining: 14.9s\n",
      "637:\tlearn: 0.0087772\ttotal: 58.3s\tremaining: 14.8s\n",
      "638:\tlearn: 0.0087483\ttotal: 58.4s\tremaining: 14.7s\n",
      "639:\tlearn: 0.0086839\ttotal: 58.5s\tremaining: 14.6s\n",
      "640:\tlearn: 0.0086280\ttotal: 58.5s\tremaining: 14.5s\n",
      "641:\tlearn: 0.0086279\ttotal: 58.6s\tremaining: 14.4s\n",
      "642:\tlearn: 0.0085732\ttotal: 58.7s\tremaining: 14.3s\n",
      "643:\tlearn: 0.0085732\ttotal: 58.8s\tremaining: 14.3s\n",
      "644:\tlearn: 0.0085731\ttotal: 58.9s\tremaining: 14.2s\n",
      "645:\tlearn: 0.0085731\ttotal: 59s\tremaining: 14.1s\n",
      "646:\tlearn: 0.0085731\ttotal: 59.1s\tremaining: 14s\n",
      "647:\tlearn: 0.0085568\ttotal: 59.1s\tremaining: 13.9s\n",
      "648:\tlearn: 0.0085568\ttotal: 59.2s\tremaining: 13.8s\n",
      "649:\tlearn: 0.0085568\ttotal: 59.3s\tremaining: 13.7s\n",
      "650:\tlearn: 0.0085218\ttotal: 59.4s\tremaining: 13.6s\n",
      "651:\tlearn: 0.0084975\ttotal: 59.5s\tremaining: 13.5s\n",
      "652:\tlearn: 0.0084842\ttotal: 59.6s\tremaining: 13.4s\n",
      "653:\tlearn: 0.0084842\ttotal: 59.6s\tremaining: 13.3s\n",
      "654:\tlearn: 0.0084842\ttotal: 59.7s\tremaining: 13.2s\n",
      "655:\tlearn: 0.0084842\ttotal: 59.8s\tremaining: 13.1s\n",
      "656:\tlearn: 0.0084839\ttotal: 59.8s\tremaining: 13s\n",
      "657:\tlearn: 0.0084838\ttotal: 59.9s\tremaining: 12.9s\n",
      "658:\tlearn: 0.0084838\ttotal: 60s\tremaining: 12.8s\n",
      "659:\tlearn: 0.0084838\ttotal: 1m\tremaining: 12.7s\n",
      "660:\tlearn: 0.0084838\ttotal: 1m\tremaining: 12.7s\n",
      "661:\tlearn: 0.0084838\ttotal: 1m\tremaining: 12.6s\n",
      "662:\tlearn: 0.0084838\ttotal: 1m\tremaining: 12.5s\n",
      "663:\tlearn: 0.0084837\ttotal: 1m\tremaining: 12.4s\n",
      "664:\tlearn: 0.0084836\ttotal: 1m\tremaining: 12.3s\n",
      "665:\tlearn: 0.0084836\ttotal: 1m\tremaining: 12.2s\n",
      "666:\tlearn: 0.0084836\ttotal: 1m\tremaining: 12.1s\n",
      "667:\tlearn: 0.0084835\ttotal: 1m\tremaining: 12s\n",
      "668:\tlearn: 0.0084835\ttotal: 1m\tremaining: 11.9s\n",
      "669:\tlearn: 0.0084835\ttotal: 1m 1s\tremaining: 11.8s\n",
      "670:\tlearn: 0.0084835\ttotal: 1m 1s\tremaining: 11.8s\n",
      "671:\tlearn: 0.0084834\ttotal: 1m 1s\tremaining: 11.7s\n",
      "672:\tlearn: 0.0084834\ttotal: 1m 1s\tremaining: 11.6s\n",
      "673:\tlearn: 0.0084534\ttotal: 1m 1s\tremaining: 11.5s\n",
      "674:\tlearn: 0.0084533\ttotal: 1m 1s\tremaining: 11.4s\n",
      "675:\tlearn: 0.0084533\ttotal: 1m 1s\tremaining: 11.3s\n",
      "676:\tlearn: 0.0084532\ttotal: 1m 1s\tremaining: 11.2s\n",
      "677:\tlearn: 0.0084532\ttotal: 1m 1s\tremaining: 11.1s\n",
      "678:\tlearn: 0.0084531\ttotal: 1m 1s\tremaining: 11s\n",
      "679:\tlearn: 0.0084531\ttotal: 1m 1s\tremaining: 10.9s\n",
      "680:\tlearn: 0.0084531\ttotal: 1m 2s\tremaining: 10.8s\n",
      "681:\tlearn: 0.0084530\ttotal: 1m 2s\tremaining: 10.7s\n",
      "682:\tlearn: 0.0084530\ttotal: 1m 2s\tremaining: 10.7s\n",
      "683:\tlearn: 0.0084530\ttotal: 1m 2s\tremaining: 10.6s\n",
      "684:\tlearn: 0.0084530\ttotal: 1m 2s\tremaining: 10.5s\n",
      "685:\tlearn: 0.0084527\ttotal: 1m 2s\tremaining: 10.4s\n",
      "686:\tlearn: 0.0084527\ttotal: 1m 2s\tremaining: 10.3s\n",
      "687:\tlearn: 0.0084526\ttotal: 1m 2s\tremaining: 10.2s\n",
      "688:\tlearn: 0.0084525\ttotal: 1m 2s\tremaining: 10.1s\n",
      "689:\tlearn: 0.0084525\ttotal: 1m 2s\tremaining: 10s\n",
      "690:\tlearn: 0.0084525\ttotal: 1m 2s\tremaining: 9.92s\n",
      "691:\tlearn: 0.0084525\ttotal: 1m 2s\tremaining: 9.83s\n",
      "692:\tlearn: 0.0084524\ttotal: 1m 3s\tremaining: 9.74s\n",
      "693:\tlearn: 0.0084524\ttotal: 1m 3s\tremaining: 9.65s\n",
      "694:\tlearn: 0.0084524\ttotal: 1m 3s\tremaining: 9.55s\n",
      "695:\tlearn: 0.0084523\ttotal: 1m 3s\tremaining: 9.46s\n",
      "696:\tlearn: 0.0084523\ttotal: 1m 3s\tremaining: 9.37s\n",
      "697:\tlearn: 0.0084522\ttotal: 1m 3s\tremaining: 9.28s\n",
      "698:\tlearn: 0.0084522\ttotal: 1m 3s\tremaining: 9.18s\n",
      "699:\tlearn: 0.0084522\ttotal: 1m 3s\tremaining: 9.1s\n",
      "700:\tlearn: 0.0084522\ttotal: 1m 3s\tremaining: 9s\n",
      "701:\tlearn: 0.0084516\ttotal: 1m 3s\tremaining: 8.92s\n",
      "702:\tlearn: 0.0084516\ttotal: 1m 3s\tremaining: 8.83s\n",
      "703:\tlearn: 0.0084516\ttotal: 1m 4s\tremaining: 8.73s\n",
      "704:\tlearn: 0.0084514\ttotal: 1m 4s\tremaining: 8.64s\n",
      "705:\tlearn: 0.0084514\ttotal: 1m 4s\tremaining: 8.55s\n",
      "706:\tlearn: 0.0084513\ttotal: 1m 4s\tremaining: 8.46s\n",
      "707:\tlearn: 0.0084513\ttotal: 1m 4s\tremaining: 8.37s\n",
      "708:\tlearn: 0.0084513\ttotal: 1m 4s\tremaining: 8.28s\n",
      "709:\tlearn: 0.0084513\ttotal: 1m 4s\tremaining: 8.19s\n",
      "710:\tlearn: 0.0084513\ttotal: 1m 4s\tremaining: 8.1s\n",
      "711:\tlearn: 0.0084512\ttotal: 1m 4s\tremaining: 8.01s\n",
      "712:\tlearn: 0.0084512\ttotal: 1m 4s\tremaining: 7.91s\n",
      "713:\tlearn: 0.0084511\ttotal: 1m 4s\tremaining: 7.82s\n",
      "714:\tlearn: 0.0084511\ttotal: 1m 5s\tremaining: 7.73s\n",
      "715:\tlearn: 0.0084510\ttotal: 1m 5s\tremaining: 7.64s\n",
      "716:\tlearn: 0.0084509\ttotal: 1m 5s\tremaining: 7.55s\n",
      "717:\tlearn: 0.0084508\ttotal: 1m 5s\tremaining: 7.46s\n",
      "718:\tlearn: 0.0084508\ttotal: 1m 5s\tremaining: 7.37s\n",
      "719:\tlearn: 0.0084508\ttotal: 1m 5s\tremaining: 7.28s\n",
      "720:\tlearn: 0.0084507\ttotal: 1m 5s\tremaining: 7.18s\n",
      "721:\tlearn: 0.0084507\ttotal: 1m 5s\tremaining: 7.1s\n",
      "722:\tlearn: 0.0084507\ttotal: 1m 5s\tremaining: 7s\n",
      "723:\tlearn: 0.0084507\ttotal: 1m 5s\tremaining: 6.91s\n",
      "724:\tlearn: 0.0084506\ttotal: 1m 5s\tremaining: 6.82s\n",
      "725:\tlearn: 0.0084505\ttotal: 1m 6s\tremaining: 6.73s\n",
      "726:\tlearn: 0.0084504\ttotal: 1m 6s\tremaining: 6.64s\n",
      "727:\tlearn: 0.0084503\ttotal: 1m 6s\tremaining: 6.55s\n",
      "728:\tlearn: 0.0084500\ttotal: 1m 6s\tremaining: 6.46s\n",
      "729:\tlearn: 0.0084499\ttotal: 1m 6s\tremaining: 6.37s\n",
      "730:\tlearn: 0.0084499\ttotal: 1m 6s\tremaining: 6.28s\n",
      "731:\tlearn: 0.0084498\ttotal: 1m 6s\tremaining: 6.19s\n",
      "732:\tlearn: 0.0084497\ttotal: 1m 6s\tremaining: 6.1s\n",
      "733:\tlearn: 0.0084497\ttotal: 1m 6s\tremaining: 6.01s\n",
      "734:\tlearn: 0.0084496\ttotal: 1m 6s\tremaining: 5.92s\n",
      "735:\tlearn: 0.0084496\ttotal: 1m 7s\tremaining: 5.83s\n",
      "736:\tlearn: 0.0084496\ttotal: 1m 7s\tremaining: 5.74s\n",
      "737:\tlearn: 0.0084495\ttotal: 1m 7s\tremaining: 5.65s\n",
      "738:\tlearn: 0.0084495\ttotal: 1m 7s\tremaining: 5.56s\n",
      "739:\tlearn: 0.0084495\ttotal: 1m 7s\tremaining: 5.46s\n",
      "740:\tlearn: 0.0084199\ttotal: 1m 7s\tremaining: 5.37s\n",
      "741:\tlearn: 0.0084199\ttotal: 1m 7s\tremaining: 5.28s\n",
      "742:\tlearn: 0.0084199\ttotal: 1m 7s\tremaining: 5.19s\n",
      "743:\tlearn: 0.0084198\ttotal: 1m 7s\tremaining: 5.1s\n",
      "744:\tlearn: 0.0084197\ttotal: 1m 7s\tremaining: 5.01s\n",
      "745:\tlearn: 0.0084197\ttotal: 1m 7s\tremaining: 4.91s\n",
      "746:\tlearn: 0.0084091\ttotal: 1m 7s\tremaining: 4.82s\n",
      "747:\tlearn: 0.0084091\ttotal: 1m 8s\tremaining: 4.73s\n",
      "748:\tlearn: 0.0084090\ttotal: 1m 8s\tremaining: 4.64s\n",
      "749:\tlearn: 0.0084090\ttotal: 1m 8s\tremaining: 4.55s\n",
      "750:\tlearn: 0.0083834\ttotal: 1m 8s\tremaining: 4.45s\n",
      "751:\tlearn: 0.0083352\ttotal: 1m 8s\tremaining: 4.36s\n",
      "752:\tlearn: 0.0083352\ttotal: 1m 8s\tremaining: 4.27s\n",
      "753:\tlearn: 0.0083039\ttotal: 1m 8s\tremaining: 4.18s\n",
      "754:\tlearn: 0.0083039\ttotal: 1m 8s\tremaining: 4.09s\n",
      "755:\tlearn: 0.0083039\ttotal: 1m 8s\tremaining: 4s\n",
      "756:\tlearn: 0.0083038\ttotal: 1m 8s\tremaining: 3.91s\n",
      "757:\tlearn: 0.0083037\ttotal: 1m 8s\tremaining: 3.81s\n",
      "758:\tlearn: 0.0083036\ttotal: 1m 8s\tremaining: 3.72s\n",
      "759:\tlearn: 0.0083036\ttotal: 1m 8s\tremaining: 3.63s\n",
      "760:\tlearn: 0.0083035\ttotal: 1m 9s\tremaining: 3.54s\n",
      "761:\tlearn: 0.0082786\ttotal: 1m 9s\tremaining: 3.45s\n",
      "762:\tlearn: 0.0082786\ttotal: 1m 9s\tremaining: 3.36s\n",
      "763:\tlearn: 0.0082785\ttotal: 1m 9s\tremaining: 3.27s\n",
      "764:\tlearn: 0.0082785\ttotal: 1m 9s\tremaining: 3.18s\n",
      "765:\tlearn: 0.0082784\ttotal: 1m 9s\tremaining: 3.08s\n",
      "766:\tlearn: 0.0082783\ttotal: 1m 9s\tremaining: 2.99s\n",
      "767:\tlearn: 0.0082782\ttotal: 1m 9s\tremaining: 2.9s\n",
      "768:\tlearn: 0.0082781\ttotal: 1m 9s\tremaining: 2.81s\n",
      "769:\tlearn: 0.0082780\ttotal: 1m 9s\tremaining: 2.72s\n",
      "770:\tlearn: 0.0082779\ttotal: 1m 9s\tremaining: 2.63s\n",
      "771:\tlearn: 0.0082779\ttotal: 1m 10s\tremaining: 2.54s\n",
      "772:\tlearn: 0.0082779\ttotal: 1m 10s\tremaining: 2.45s\n",
      "773:\tlearn: 0.0082778\ttotal: 1m 10s\tremaining: 2.36s\n",
      "774:\tlearn: 0.0082778\ttotal: 1m 10s\tremaining: 2.27s\n",
      "775:\tlearn: 0.0082777\ttotal: 1m 10s\tremaining: 2.18s\n",
      "776:\tlearn: 0.0082777\ttotal: 1m 10s\tremaining: 2.09s\n",
      "777:\tlearn: 0.0082776\ttotal: 1m 10s\tremaining: 2s\n",
      "778:\tlearn: 0.0082776\ttotal: 1m 10s\tremaining: 1.91s\n",
      "779:\tlearn: 0.0082248\ttotal: 1m 10s\tremaining: 1.81s\n",
      "780:\tlearn: 0.0081902\ttotal: 1m 10s\tremaining: 1.72s\n",
      "781:\tlearn: 0.0081901\ttotal: 1m 10s\tremaining: 1.63s\n",
      "782:\tlearn: 0.0081901\ttotal: 1m 11s\tremaining: 1.54s\n",
      "783:\tlearn: 0.0081900\ttotal: 1m 11s\tremaining: 1.45s\n",
      "784:\tlearn: 0.0081899\ttotal: 1m 11s\tremaining: 1.36s\n",
      "785:\tlearn: 0.0081899\ttotal: 1m 11s\tremaining: 1.27s\n",
      "786:\tlearn: 0.0081899\ttotal: 1m 11s\tremaining: 1.18s\n",
      "787:\tlearn: 0.0081782\ttotal: 1m 11s\tremaining: 1.09s\n",
      "788:\tlearn: 0.0081781\ttotal: 1m 11s\tremaining: 999ms\n",
      "789:\tlearn: 0.0081776\ttotal: 1m 11s\tremaining: 908ms\n",
      "790:\tlearn: 0.0081311\ttotal: 1m 11s\tremaining: 817ms\n",
      "791:\tlearn: 0.0081309\ttotal: 1m 11s\tremaining: 727ms\n",
      "792:\tlearn: 0.0081307\ttotal: 1m 12s\tremaining: 636ms\n",
      "793:\tlearn: 0.0081307\ttotal: 1m 12s\tremaining: 545ms\n",
      "794:\tlearn: 0.0081307\ttotal: 1m 12s\tremaining: 454ms\n",
      "795:\tlearn: 0.0081085\ttotal: 1m 12s\tremaining: 363ms\n",
      "796:\tlearn: 0.0081083\ttotal: 1m 12s\tremaining: 272ms\n",
      "797:\tlearn: 0.0081081\ttotal: 1m 12s\tremaining: 181ms\n",
      "798:\tlearn: 0.0081080\ttotal: 1m 12s\tremaining: 90.7ms\n",
      "799:\tlearn: 0.0081080\ttotal: 1m 12s\tremaining: 0us\n",
      "Best Hyperparameters found: {'max_depth': 8, 'learning_rate': 0.5, 'iterations': 800}\n",
      "Accuracy 0.9033816425120773\n",
      "Recall 0.963151207115629\n",
      "Precision 0.8623435722411832\n",
      "F1 Score 0.9099639855942377\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model object\n",
    "cat_top10features = ['contract_type', 'num_dependents_bin', 'num_referrals_bin', 'zip_code', 'payment_method']\n",
    "catboost = CatBoostClassifier(random_state = 5, cat_features= cat_top10features)\n",
    "param = {'iterations' : list(range(100, 1100, 100)), 'learning_rate': [0.01, 0.05, 0.1, 0.5], 'max_depth' : [2, 4, 8]}\n",
    "\n",
    "catboost_grid = RandomizedSearchCV(catboost, param_distributions = param, n_iter = 3, n_jobs = -1, scoring = 'accuracy')\n",
    "catboost_grid.fit(X_train_cat, y_train_cat)\n",
    "\n",
    "y_pred_proba_cat= catboost_grid.predict_proba(X_test_cat)[:,1]\n",
    "y_pred_cat = catboost_grid.predict(X_test_cat)\n",
    "\n",
    "print(\"Best Hyperparameters found:\", catboost_grid.best_params_)\n",
    "\n",
    "print(\"Accuracy\", metrics.accuracy_score(y_test_cat, y_pred_cat))\n",
    "print(\"Recall\", metrics.recall_score(y_test_cat, y_pred_cat))\n",
    "print(\"Precision\", metrics.precision_score(y_test_cat, y_pred_cat))\n",
    "print(\"F1 Score\", metrics.f1_score(y_test, y_pred_cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "fillpattern": {
          "shape": ""
         },
         "hovertemplate": "False Positive Rate=%{x}<br>True Positive Rate=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "stackgroup": "1",
         "type": "scatter",
         "x": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0013063357282821686,
          0.0013063357282821686,
          0.0013063357282821686,
          0.0013063357282821686,
          0.0013063357282821686,
          0.0013063357282821686,
          0.0013063357282821686,
          0.0013063357282821686,
          0.0013063357282821686,
          0.001959503592423253,
          0.001959503592423253,
          0.001959503592423253,
          0.001959503592423253,
          0.001959503592423253,
          0.001959503592423253,
          0.001959503592423253,
          0.001959503592423253,
          0.001959503592423253,
          0.001959503592423253,
          0.001959503592423253,
          0.001959503592423253,
          0.001959503592423253,
          0.001959503592423253,
          0.002612671456564337,
          0.002612671456564337,
          0.002612671456564337,
          0.002612671456564337,
          0.002612671456564337,
          0.002612671456564337,
          0.002612671456564337,
          0.002612671456564337,
          0.002612671456564337,
          0.002612671456564337,
          0.002612671456564337,
          0.002612671456564337,
          0.002612671456564337,
          0.002612671456564337,
          0.002612671456564337,
          0.0032658393207054214,
          0.0032658393207054214,
          0.0032658393207054214,
          0.0032658393207054214,
          0.0032658393207054214,
          0.003919007184846506,
          0.003919007184846506,
          0.003919007184846506,
          0.003919007184846506,
          0.003919007184846506,
          0.003919007184846506,
          0.003919007184846506,
          0.0045721750489875895,
          0.0045721750489875895,
          0.005225342913128674,
          0.005225342913128674,
          0.005878510777269758,
          0.005878510777269758,
          0.006531678641410843,
          0.006531678641410843,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007838014369693011,
          0.007838014369693011,
          0.007838014369693011,
          0.007838014369693011,
          0.008491182233834096,
          0.008491182233834096,
          0.009144350097975179,
          0.009144350097975179,
          0.009144350097975179,
          0.009144350097975179,
          0.009144350097975179,
          0.009144350097975179,
          0.010450685826257348,
          0.010450685826257348,
          0.010450685826257348,
          0.010450685826257348,
          0.010450685826257348,
          0.010450685826257348,
          0.010450685826257348,
          0.011103853690398433,
          0.011103853690398433,
          0.011103853690398433,
          0.011103853690398433,
          0.011103853690398433,
          0.011103853690398433,
          0.011757021554539516,
          0.011757021554539516,
          0.011757021554539516,
          0.011757021554539516,
          0.012410189418680601,
          0.012410189418680601,
          0.012410189418680601,
          0.012410189418680601,
          0.012410189418680601,
          0.012410189418680601,
          0.012410189418680601,
          0.012410189418680601,
          0.012410189418680601,
          0.012410189418680601,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013716525146962769,
          0.013716525146962769,
          0.013716525146962769,
          0.013716525146962769,
          0.013716525146962769,
          0.013716525146962769,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.015676028739386023,
          0.015676028739386023,
          0.015676028739386023,
          0.015676028739386023,
          0.016982364467668192,
          0.016982364467668192,
          0.016982364467668192,
          0.016982364467668192,
          0.016982364467668192,
          0.016982364467668192,
          0.016982364467668192,
          0.017635532331809273,
          0.017635532331809273,
          0.017635532331809273,
          0.018288700195950358,
          0.018288700195950358,
          0.018288700195950358,
          0.018288700195950358,
          0.018288700195950358,
          0.018288700195950358,
          0.018288700195950358,
          0.018288700195950358,
          0.018288700195950358,
          0.018288700195950358,
          0.018288700195950358,
          0.018288700195950358,
          0.018941868060091443,
          0.018941868060091443,
          0.018941868060091443,
          0.018941868060091443,
          0.019595035924232528,
          0.019595035924232528,
          0.020248203788373612,
          0.020248203788373612,
          0.020248203788373612,
          0.020901371652514697,
          0.020901371652514697,
          0.02155453951665578,
          0.02155453951665578,
          0.022207707380796866,
          0.022207707380796866,
          0.022207707380796866,
          0.022207707380796866,
          0.022207707380796866,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.023514043109079032,
          0.023514043109079032,
          0.023514043109079032,
          0.023514043109079032,
          0.023514043109079032,
          0.023514043109079032,
          0.023514043109079032,
          0.023514043109079032,
          0.023514043109079032,
          0.023514043109079032,
          0.023514043109079032,
          0.023514043109079032,
          0.024167210973220117,
          0.024167210973220117,
          0.024820378837361202,
          0.024820378837361202,
          0.025473546701502287,
          0.025473546701502287,
          0.02612671456564337,
          0.02612671456564337,
          0.02612671456564337,
          0.02612671456564337,
          0.02612671456564337,
          0.02612671456564337,
          0.026779882429784456,
          0.026779882429784456,
          0.027433050293925537,
          0.027433050293925537,
          0.027433050293925537,
          0.027433050293925537,
          0.027433050293925537,
          0.027433050293925537,
          0.027433050293925537,
          0.027433050293925537,
          0.027433050293925537,
          0.027433050293925537,
          0.027433050293925537,
          0.028086218158066622,
          0.028086218158066622,
          0.028086218158066622,
          0.028086218158066622,
          0.028086218158066622,
          0.028739386022207707,
          0.028739386022207707,
          0.028739386022207707,
          0.028739386022207707,
          0.028739386022207707,
          0.028739386022207707,
          0.028739386022207707,
          0.028739386022207707,
          0.028739386022207707,
          0.028739386022207707,
          0.028739386022207707,
          0.02939255388634879,
          0.02939255388634879,
          0.02939255388634879,
          0.030045721750489876,
          0.030045721750489876,
          0.030045721750489876,
          0.030045721750489876,
          0.030045721750489876,
          0.03069888961463096,
          0.03069888961463096,
          0.03069888961463096,
          0.03069888961463096,
          0.03200522534291313,
          0.03200522534291313,
          0.032658393207054215,
          0.032658393207054215,
          0.033311561071195296,
          0.033311561071195296,
          0.033964728935336384,
          0.033964728935336384,
          0.033964728935336384,
          0.033964728935336384,
          0.033964728935336384,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.035924232527759635,
          0.035924232527759635,
          0.037230568256041804,
          0.037230568256041804,
          0.037230568256041804,
          0.037230568256041804,
          0.037230568256041804,
          0.037230568256041804,
          0.037230568256041804,
          0.037230568256041804,
          0.037230568256041804,
          0.037230568256041804,
          0.037230568256041804,
          0.037230568256041804,
          0.037230568256041804,
          0.037230568256041804,
          0.037230568256041804,
          0.037230568256041804,
          0.037883736120182886,
          0.037883736120182886,
          0.037883736120182886,
          0.037883736120182886,
          0.037883736120182886,
          0.037883736120182886,
          0.039190071848465055,
          0.039190071848465055,
          0.039843239712606136,
          0.039843239712606136,
          0.039843239712606136,
          0.040496407576747225,
          0.040496407576747225,
          0.040496407576747225,
          0.040496407576747225,
          0.041149575440888306,
          0.041149575440888306,
          0.041149575440888306,
          0.041149575440888306,
          0.041149575440888306,
          0.041149575440888306,
          0.041149575440888306,
          0.041149575440888306,
          0.041802743305029394,
          0.041802743305029394,
          0.041802743305029394,
          0.041802743305029394,
          0.042455911169170475,
          0.042455911169170475,
          0.042455911169170475,
          0.042455911169170475,
          0.04310907903331156,
          0.04310907903331156,
          0.04441541476159373,
          0.04441541476159373,
          0.045068582625734814,
          0.045068582625734814,
          0.045068582625734814,
          0.045068582625734814,
          0.045068582625734814,
          0.045068582625734814,
          0.045721750489875895,
          0.045721750489875895,
          0.047028086218158065,
          0.047028086218158065,
          0.047028086218158065,
          0.047028086218158065,
          0.047028086218158065,
          0.047028086218158065,
          0.04768125408229915,
          0.04768125408229915,
          0.04768125408229915,
          0.04768125408229915,
          0.04768125408229915,
          0.04768125408229915,
          0.048334421946440234,
          0.048334421946440234,
          0.048334421946440234,
          0.048334421946440234,
          0.048334421946440234,
          0.048334421946440234,
          0.048334421946440234,
          0.048334421946440234,
          0.048334421946440234,
          0.048334421946440234,
          0.048334421946440234,
          0.048334421946440234,
          0.048334421946440234,
          0.04898758981058132,
          0.04898758981058132,
          0.04898758981058132,
          0.04898758981058132,
          0.04898758981058132,
          0.04898758981058132,
          0.04898758981058132,
          0.049640757674722404,
          0.049640757674722404,
          0.049640757674722404,
          0.049640757674722404,
          0.05094709340300457,
          0.05094709340300457,
          0.05094709340300457,
          0.051600261267145654,
          0.051600261267145654,
          0.051600261267145654,
          0.051600261267145654,
          0.051600261267145654,
          0.051600261267145654,
          0.051600261267145654,
          0.05225342913128674,
          0.05225342913128674,
          0.05225342913128674,
          0.05225342913128674,
          0.052906596995427824,
          0.052906596995427824,
          0.052906596995427824,
          0.052906596995427824,
          0.052906596995427824,
          0.052906596995427824,
          0.052906596995427824,
          0.052906596995427824,
          0.052906596995427824,
          0.052906596995427824,
          0.05355976485956891,
          0.05355976485956891,
          0.05421293272370999,
          0.05421293272370999,
          0.05421293272370999,
          0.05421293272370999,
          0.05421293272370999,
          0.05421293272370999,
          0.05421293272370999,
          0.05421293272370999,
          0.05421293272370999,
          0.05551926845199216,
          0.05551926845199216,
          0.056172436316133244,
          0.056172436316133244,
          0.056172436316133244,
          0.056172436316133244,
          0.05682560418027433,
          0.05682560418027433,
          0.05747877204441541,
          0.05747877204441541,
          0.05747877204441541,
          0.05878510777269758,
          0.05878510777269758,
          0.05878510777269758,
          0.05878510777269758,
          0.05878510777269758,
          0.06009144350097975,
          0.06009144350097975,
          0.06009144350097975,
          0.06009144350097975,
          0.06139777922926192,
          0.06139777922926192,
          0.06139777922926192,
          0.06139777922926192,
          0.06139777922926192,
          0.06139777922926192,
          0.06139777922926192,
          0.062050947093403,
          0.062050947093403,
          0.062050947093403,
          0.06270411495754409,
          0.06270411495754409,
          0.06335728282168518,
          0.06335728282168518,
          0.06466361854996734,
          0.06466361854996734,
          0.06466361854996734,
          0.06466361854996734,
          0.06466361854996734,
          0.06531678641410843,
          0.06531678641410843,
          0.06531678641410843,
          0.06531678641410843,
          0.06531678641410843,
          0.06531678641410843,
          0.06531678641410843,
          0.06531678641410843,
          0.06531678641410843,
          0.0659699542782495,
          0.0659699542782495,
          0.0659699542782495,
          0.06662312214239059,
          0.06662312214239059,
          0.06727629000653168,
          0.06727629000653168,
          0.06727629000653168,
          0.06727629000653168,
          0.06727629000653168,
          0.06727629000653168,
          0.06792945787067277,
          0.06792945787067277,
          0.06792945787067277,
          0.06792945787067277,
          0.06858262573481384,
          0.06858262573481384,
          0.06858262573481384,
          0.06858262573481384,
          0.06858262573481384,
          0.06858262573481384,
          0.06858262573481384,
          0.06923579359895493,
          0.06923579359895493,
          0.06988896146309602,
          0.06988896146309602,
          0.06988896146309602,
          0.06988896146309602,
          0.06988896146309602,
          0.0705421293272371,
          0.0705421293272371,
          0.0705421293272371,
          0.07119529719137818,
          0.07119529719137818,
          0.07119529719137818,
          0.07119529719137818,
          0.07119529719137818,
          0.07119529719137818,
          0.07184846505551927,
          0.07184846505551927,
          0.07250163291966036,
          0.07250163291966036,
          0.07250163291966036,
          0.07250163291966036,
          0.07250163291966036,
          0.07250163291966036,
          0.07250163291966036,
          0.07250163291966036,
          0.07250163291966036,
          0.07250163291966036,
          0.07250163291966036,
          0.07315480078380143,
          0.07315480078380143,
          0.07380796864794252,
          0.07380796864794252,
          0.07446113651208361,
          0.07446113651208361,
          0.07446113651208361,
          0.07446113651208361,
          0.07511430437622468,
          0.07511430437622468,
          0.07511430437622468,
          0.07642064010450686,
          0.07642064010450686,
          0.07707380796864795,
          0.07707380796864795,
          0.07707380796864795,
          0.07707380796864795,
          0.07772697583278902,
          0.07772697583278902,
          0.07772697583278902,
          0.07838014369693011,
          0.07838014369693011,
          0.07838014369693011,
          0.0790333115610712,
          0.0790333115610712,
          0.07968647942521227,
          0.07968647942521227,
          0.08033964728935336,
          0.08033964728935336,
          0.08033964728935336,
          0.08033964728935336,
          0.08033964728935336,
          0.08033964728935336,
          0.08164598301763554,
          0.08164598301763554,
          0.08164598301763554,
          0.08425865447419988,
          0.08425865447419988,
          0.08556499020248204,
          0.08556499020248204,
          0.08556499020248204,
          0.08621815806662313,
          0.08621815806662313,
          0.08621815806662313,
          0.08621815806662313,
          0.0868713259307642,
          0.0868713259307642,
          0.08752449379490529,
          0.08752449379490529,
          0.08752449379490529,
          0.08817766165904638,
          0.08817766165904638,
          0.08817766165904638,
          0.09013716525146963,
          0.09013716525146963,
          0.09013716525146963,
          0.09013716525146963,
          0.09144350097975179,
          0.09144350097975179,
          0.09340300457217506,
          0.09340300457217506,
          0.09340300457217506,
          0.09340300457217506,
          0.0953625081645983,
          0.0953625081645983,
          0.09601567602873938,
          0.09601567602873938,
          0.09601567602873938,
          0.09601567602873938,
          0.09601567602873938,
          0.09666884389288047,
          0.09666884389288047,
          0.09862834748530372,
          0.09862834748530372,
          0.0999346832135859,
          0.0999346832135859,
          0.0999346832135859,
          0.0999346832135859,
          0.10058785107772697,
          0.10058785107772697,
          0.10058785107772697,
          0.10058785107772697,
          0.10124101894186806,
          0.10124101894186806,
          0.10189418680600915,
          0.10189418680600915,
          0.10189418680600915,
          0.10189418680600915,
          0.10254735467015023,
          0.10254735467015023,
          0.10254735467015023,
          0.10450685826257348,
          0.10450685826257348,
          0.10450685826257348,
          0.10516002612671456,
          0.10516002612671456,
          0.10516002612671456,
          0.10646636185499674,
          0.10646636185499674,
          0.10711952971913782,
          0.10711952971913782,
          0.10711952971913782,
          0.1077726975832789,
          0.1077726975832789,
          0.1077726975832789,
          0.1077726975832789,
          0.10842586544741999,
          0.10842586544741999,
          0.10842586544741999,
          0.10842586544741999,
          0.11038536903984324,
          0.11038536903984324,
          0.11169170476812541,
          0.11169170476812541,
          0.11365120836054866,
          0.11365120836054866,
          0.11365120836054866,
          0.11365120836054866,
          0.11365120836054866,
          0.11365120836054866,
          0.11430437622468975,
          0.11430437622468975,
          0.11495754408883083,
          0.11495754408883083,
          0.11561071195297191,
          0.11561071195297191,
          0.11561071195297191,
          0.11561071195297191,
          0.11561071195297191,
          0.11691704768125408,
          0.11691704768125408,
          0.11952971913781842,
          0.11952971913781842,
          0.12083605486610059,
          0.12083605486610059,
          0.12083605486610059,
          0.12083605486610059,
          0.12083605486610059,
          0.12214239059438275,
          0.12214239059438275,
          0.1280209013716525,
          0.1280209013716525,
          0.1378184193337688,
          0.1378184193337688,
          0.13847158719790986,
          0.13847158719790986,
          0.13912475506205094,
          0.13912475506205094,
          0.13977792292619204,
          0.13977792292619204,
          0.1417374265186153,
          0.1417374265186153,
          0.14630960156760286,
          0.14630960156760286,
          0.14761593729588504,
          0.14761593729588504,
          0.14892227302416722,
          0.14892227302416722,
          0.15022860875244937,
          0.15022860875244937,
          0.15022860875244937,
          0.15022860875244937,
          0.15218811234487264,
          0.15218811234487264,
          0.1534944480731548,
          0.1534944480731548,
          0.1541476159372959,
          0.1541476159372959,
          0.15545395166557804,
          0.15545395166557804,
          0.1574134552580013,
          0.1574134552580013,
          0.1574134552580013,
          0.16133246244284782,
          0.16133246244284782,
          0.16394513389941215,
          0.16394513389941215,
          0.16394513389941215,
          0.16459830176355322,
          0.16459830176355322,
          0.16655780535597647,
          0.16655780535597647,
          0.16786414108425865,
          0.16786414108425865,
          0.16917047681254083,
          0.16917047681254083,
          0.1822338340953625,
          0.1822338340953625,
          0.18419333768778576,
          0.18419333768778576,
          0.18549967341606793,
          0.18549967341606793,
          0.19137818419333769,
          0.19137818419333769,
          0.20117570215545394,
          0.20117570215545394,
          0.20117570215545394,
          0.2037883736120183,
          0.2037883736120183,
          0.20444154147615937,
          0.20444154147615937,
          0.20640104506858262,
          0.20640104506858262,
          0.2077073807968648,
          0.2077073807968648,
          0.20901371652514697,
          0.20901371652514697,
          0.2122795558458524,
          0.2122795558458524,
          0.21750489875898105,
          0.21750489875898105,
          0.21815806662312215,
          0.21815806662312215,
          0.2201175702155454,
          0.2201175702155454,
          0.22599608099281515,
          0.22599608099281515,
          0.22991508817766165,
          0.22991508817766165,
          0.2364467668190725,
          0.2364467668190725,
          0.24232527759634226,
          0.24232527759634226,
          0.2534291312867407,
          0.2534291312867407,
          0.25473546701502287,
          0.25473546701502287,
          0.26583932070542127,
          0.26583932070542127,
          0.2684519921619856,
          0.2684519921619856,
          0.2691051600261267,
          0.2691051600261267,
          0.2697583278902678,
          0.2697583278902678,
          0.2919660352710647,
          0.2919660352710647,
          0.3030698889614631,
          0.3030698889614631,
          0.3324624428478119,
          0.3324624428478119,
          0.33311561071195295,
          0.33311561071195295,
          0.3396472893533638,
          0.3396472893533638,
          0.3396472893533638,
          0.34683213585891576,
          0.34683213585891576,
          0.35401698236446766,
          0.35401698236446766,
          0.35401698236446766,
          0.3638145003265839,
          0.3638145003265839,
          0.36708033964728937,
          0.36708033964728937,
          0.3912475506205095,
          0.3912475506205095,
          0.3945133899412149,
          0.3945133899412149,
          0.3958197256694971,
          0.3958197256694971,
          0.40039190071848463,
          0.40039190071848463,
          0.418680600914435,
          0.418680600914435,
          0.4245591116917048,
          0.4245591116917048,
          0.44546048334421945,
          0.44546048334421945,
          0.4931417374265186,
          0.4931417374265186,
          0.542782495101241,
          0.542782495101241,
          0.5774003919007185,
          0.5774003919007185,
          0.7119529719137818,
          0.7119529719137818,
          0.7713912475506205,
          0.7713912475506205,
          1
         ],
         "xaxis": "x",
         "y": [
          0,
          0.0019059720457433292,
          0.0025412960609911056,
          0.0038119440914866584,
          0.0044472681067344345,
          0.005717916137229987,
          0.00698856416772554,
          0.009529860228716646,
          0.013341804320203304,
          0.015247776365946633,
          0.018424396442185513,
          0.019695044472681066,
          0.02096569250317662,
          0.022871664548919948,
          0.023506988564167726,
          0.02477763659466328,
          0.026683608640406607,
          0.02795425667090216,
          0.03303684879288437,
          0.03303684879288437,
          0.03748411689961881,
          0.03939008894536213,
          0.044472681067344345,
          0.045743329097839895,
          0.045743329097839895,
          0.04701397712833545,
          0.04891994917407878,
          0.05146124523506988,
          0.05273189326556544,
          0.053367217280813214,
          0.055273189326556546,
          0.056543837357052096,
          0.05717916137229987,
          0.05717916137229987,
          0.05972045743329098,
          0.060991105463786534,
          0.06353240152477764,
          0.06480304955527319,
          0.06670902160101652,
          0.06925031766200762,
          0.0698856416772554,
          0.07179161372299873,
          0.07623888182973317,
          0.0781448538754765,
          0.0806861499364676,
          0.08195679796696315,
          0.08259212198221093,
          0.08259212198221093,
          0.08386277001270648,
          0.08576874205844981,
          0.08703939008894536,
          0.08894536213468869,
          0.09021601016518424,
          0.09085133418043202,
          0.09212198221092757,
          0.0940279542566709,
          0.09529860228716645,
          0.096569250317662,
          0.09783989834815757,
          0.09847522236340533,
          0.10355781448538755,
          0.1048284625158831,
          0.1048284625158831,
          0.10546378653113088,
          0.10800508259212198,
          0.11181702668360864,
          0.11435832274459974,
          0.11435832274459974,
          0.1156289707750953,
          0.11626429479034307,
          0.11753494282083862,
          0.12007623888182974,
          0.12134688691232529,
          0.12198221092757307,
          0.12198221092757307,
          0.12261753494282084,
          0.12261753494282084,
          0.12325285895806862,
          0.12325285895806862,
          0.12515883100381195,
          0.12515883100381195,
          0.12770012706480305,
          0.12770012706480305,
          0.12960609911054638,
          0.13214739517153748,
          0.1340533672172808,
          0.13595933926302414,
          0.1397712833545108,
          0.14104193138500634,
          0.14294790343074967,
          0.14358322744599747,
          0.144853875476493,
          0.1480304955527319,
          0.14930114358322744,
          0.14993646759847523,
          0.15120711562897077,
          0.15374841168996187,
          0.15501905972045743,
          0.15501905972045743,
          0.15819567979669633,
          0.15946632782719186,
          0.16010165184243966,
          0.16010165184243966,
          0.16200762388818296,
          0.16200762388818296,
          0.1639135959339263,
          0.16518424396442186,
          0.16645489199491742,
          0.16772554002541296,
          0.17026683608640406,
          0.17026683608640406,
          0.17153748411689962,
          0.17344345616264295,
          0.1823379923761118,
          0.18424396442185514,
          0.18678526048284624,
          0.18742058449809404,
          0.18742058449809404,
          0.1880559085133418,
          0.18932655654383734,
          0.20012706480304956,
          0.2020330368487929,
          0.20520965692503176,
          0.20520965692503176,
          0.20584498094027953,
          0.20775095298602286,
          0.20838627700127066,
          0.20838627700127066,
          0.20902160101651843,
          0.21092757306226176,
          0.2128335451080051,
          0.21473951715374842,
          0.2153748411689962,
          0.21728081321473952,
          0.2204574332909784,
          0.22172808132147395,
          0.22236340533672172,
          0.22236340533672172,
          0.22363405336721728,
          0.22744599745870395,
          0.22998729351969505,
          0.23252858958068615,
          0.2331639135959339,
          0.23443456162642948,
          0.23824650571791614,
          0.23951715374841168,
          0.241423125794155,
          0.241423125794155,
          0.24269377382465057,
          0.24332909783989834,
          0.24587039390088947,
          0.24841168996188057,
          0.2496823379923761,
          0.2496823379923761,
          0.2579415501905972,
          0.2604828462515883,
          0.2636594663278272,
          0.26493011435832275,
          0.2668360864040661,
          0.2687420584498094,
          0.27001270648030495,
          0.2712833545108005,
          0.2731893265565438,
          0.2738246505717916,
          0.27509529860228715,
          0.27573062261753495,
          0.2770012706480305,
          0.2776365946632783,
          0.2795425667090216,
          0.28081321473951715,
          0.2827191867852605,
          0.2833545108005083,
          0.2852604828462516,
          0.28716645489199494,
          0.2909783989834816,
          0.29224904701397714,
          0.2960609911054638,
          0.2960609911054638,
          0.29669631512071154,
          0.29860228716645487,
          0.29923761118170267,
          0.29923761118170267,
          0.3005082592121982,
          0.30304955527318933,
          0.30559085133418046,
          0.306861499364676,
          0.30813214739517153,
          0.31067344345616266,
          0.31067344345616266,
          0.312579415501906,
          0.3138500635324015,
          0.3138500635324015,
          0.31512071156289706,
          0.31575603557814486,
          0.3170266836086404,
          0.318297331639136,
          0.3195679796696315,
          0.32147395171537485,
          0.3227445997458704,
          0.3240152477763659,
          0.32592121982210925,
          0.3297331639135959,
          0.33227445997458704,
          0.33227445997458704,
          0.3335451080050826,
          0.33481575603557817,
          0.3360864040660737,
          0.3360864040660737,
          0.33989834815756037,
          0.33989834815756037,
          0.3405336721728081,
          0.3418043202033037,
          0.3418043202033037,
          0.34243964421855144,
          0.34243964421855144,
          0.34371029224904703,
          0.34371029224904703,
          0.34498094027954257,
          0.3468869123252859,
          0.34815756035578144,
          0.34942820838627703,
          0.34942820838627703,
          0.35069885641677256,
          0.3526048284625159,
          0.35324015247776364,
          0.3545108005082592,
          0.35641677255400256,
          0.3576874205844981,
          0.3602287166454892,
          0.36149936467598476,
          0.36149936467598476,
          0.3627700127064803,
          0.3646759847522236,
          0.36594663278271916,
          0.36658195679796696,
          0.3678526048284625,
          0.3697585768742058,
          0.37166454891994916,
          0.37229987293519695,
          0.3754764930114358,
          0.37738246505717915,
          0.37801778907242695,
          0.37801778907242695,
          0.3792884371029225,
          0.3792884371029225,
          0.3799237611181703,
          0.3799237611181703,
          0.3811944091486658,
          0.3811944091486658,
          0.3818297331639136,
          0.38373570520965694,
          0.3850063532401525,
          0.3900889453621347,
          0.39072426937738247,
          0.39072426937738247,
          0.39390088945362134,
          0.39390088945362134,
          0.39453621346886913,
          0.39644218551461247,
          0.40088945362134687,
          0.403430749682338,
          0.40470139771283353,
          0.4059720457433291,
          0.40660736975857686,
          0.4078780177890724,
          0.409148665819568,
          0.4104193138500635,
          0.4104193138500635,
          0.41168996188055906,
          0.4135959339263024,
          0.414866581956798,
          0.4167725540025413,
          0.4167725540025413,
          0.4193138500635324,
          0.420584498094028,
          0.4218551461245235,
          0.42312579415501905,
          0.4243964421855146,
          0.4269377382465057,
          0.42884371029224905,
          0.4301143583227446,
          0.4320203303684879,
          0.43519695044472684,
          0.43519695044472684,
          0.4358322744599746,
          0.4371029224904701,
          0.4371029224904701,
          0.43964421855146124,
          0.4409148665819568,
          0.4434561626429479,
          0.4440914866581957,
          0.4440914866581957,
          0.44536213468869124,
          0.44663278271918677,
          0.44790343074968236,
          0.44790343074968236,
          0.4491740787801779,
          0.4491740787801779,
          0.45108005082592123,
          0.45108005082592123,
          0.45171537484116897,
          0.45171537484116897,
          0.4536213468869123,
          0.45552731893265563,
          0.4567979669631512,
          0.45806861499364676,
          0.45806861499364676,
          0.4593392630241423,
          0.4599745870393901,
          0.4625158831003812,
          0.46315120711562896,
          0.4650571791613723,
          0.4656925031766201,
          0.4669631512071156,
          0.4701397712833545,
          0.4714104193138501,
          0.4720457433290978,
          0.4733163913595934,
          0.47395171537484115,
          0.4758576874205845,
          0.4764930114358323,
          0.4777636594663278,
          0.4777636594663278,
          0.47966963151207115,
          0.47966963151207115,
          0.4809402795425667,
          0.4822109275730623,
          0.48284625158831,
          0.4841168996188056,
          0.48602287166454894,
          0.4872935196950445,
          0.48792884371029227,
          0.4898348157560356,
          0.49174078780177893,
          0.49301143583227447,
          0.4936467598475222,
          0.4949174078780178,
          0.49555273189326554,
          0.49682337992376113,
          0.4974587039390089,
          0.4974587039390089,
          0.4993646759847522,
          0.5006353240152478,
          0.5012706480304956,
          0.5025412960609911,
          0.5031766200762389,
          0.5031766200762389,
          0.5050825921219823,
          0.5050825921219823,
          0.5063532401524777,
          0.5076238881829733,
          0.5076238881829733,
          0.5088945362134689,
          0.5108005082592122,
          0.51143583227446,
          0.51143583227446,
          0.5120711562897078,
          0.5133418043202033,
          0.5139771283354511,
          0.5190597204574333,
          0.5209656925031766,
          0.5222363405336722,
          0.5235069885641678,
          0.5235069885641678,
          0.5247776365946633,
          0.5285895806861499,
          0.5292249047013977,
          0.5292249047013977,
          0.5304955527318933,
          0.5324015247776366,
          0.5330368487928844,
          0.5330368487928844,
          0.5336721728081322,
          0.5336721728081322,
          0.5355781448538754,
          0.5355781448538754,
          0.5362134688691232,
          0.5374841168996188,
          0.5381194409148666,
          0.5393900889453621,
          0.5400254129606099,
          0.5400254129606099,
          0.5419313850063533,
          0.5419313850063533,
          0.542566709021601,
          0.5438373570520966,
          0.5476493011435832,
          0.5489199491740788,
          0.5495552731893265,
          0.5495552731893265,
          0.5501905972045743,
          0.5514612452350699,
          0.5520965692503177,
          0.554002541296061,
          0.5552731893265566,
          0.5552731893265566,
          0.5565438373570522,
          0.5584498094027954,
          0.559720457433291,
          0.5641677255400254,
          0.565438373570521,
          0.5667090216010165,
          0.5692503176620076,
          0.5705209656925032,
          0.5717916137229987,
          0.5743329097839899,
          0.5756035578144854,
          0.576874205844981,
          0.576874205844981,
          0.5781448538754765,
          0.5806861499364676,
          0.5819567979669632,
          0.5870393900889453,
          0.5883100381194409,
          0.5889453621346887,
          0.5889453621346887,
          0.590851334180432,
          0.5914866581956798,
          0.5927573062261754,
          0.5927573062261754,
          0.5946632782719187,
          0.5952986022871665,
          0.5952986022871665,
          0.5978398983481575,
          0.6010165184243964,
          0.6035578144853876,
          0.6048284625158831,
          0.6060991105463787,
          0.6067344345616265,
          0.6067344345616265,
          0.6143583227445998,
          0.6156289707750953,
          0.6175349428208387,
          0.6175349428208387,
          0.6200762388818297,
          0.6219822109275731,
          0.6226175349428208,
          0.6238881829733164,
          0.6245235069885642,
          0.6264294790343075,
          0.627700127064803,
          0.6289707750952986,
          0.6296060991105463,
          0.6296060991105463,
          0.6327827191867853,
          0.6327827191867853,
          0.6353240152477764,
          0.6359593392630242,
          0.6372299872935197,
          0.6461245235069886,
          0.6480304955527318,
          0.650571791613723,
          0.6518424396442185,
          0.6524777636594663,
          0.6524777636594663,
          0.6556543837357052,
          0.6556543837357052,
          0.6569250317662008,
          0.6581956797966964,
          0.6594663278271918,
          0.6594663278271918,
          0.6607369758576874,
          0.6607369758576874,
          0.662007623888183,
          0.6639135959339263,
          0.6639135959339263,
          0.6658195679796697,
          0.6670902160101652,
          0.667725540025413,
          0.6689961880559085,
          0.6689961880559085,
          0.670266836086404,
          0.6709021601016518,
          0.6734434561626429,
          0.6734434561626429,
          0.6759847522236341,
          0.6778907242693774,
          0.679161372299873,
          0.681702668360864,
          0.6829733163913596,
          0.6842439644218552,
          0.6842439644218552,
          0.6861499364675985,
          0.6874205844980941,
          0.6874205844980941,
          0.6880559085133418,
          0.6880559085133418,
          0.6893265565438373,
          0.6893265565438373,
          0.6899618805590851,
          0.6912325285895807,
          0.693138500635324,
          0.6937738246505718,
          0.6937738246505718,
          0.6944091486658196,
          0.6956797966963151,
          0.6982210927573063,
          0.7013977128335451,
          0.7026683608640406,
          0.7083862770012707,
          0.7090216010165185,
          0.7109275730622617,
          0.7109275730622617,
          0.7121982210927573,
          0.7134688691232529,
          0.7134688691232529,
          0.7147395171537484,
          0.7147395171537484,
          0.716010165184244,
          0.7172808132147395,
          0.7185514612452351,
          0.7210927573062261,
          0.7217280813214739,
          0.7217280813214739,
          0.727445997458704,
          0.7280813214739518,
          0.7306226175349428,
          0.7306226175349428,
          0.7318932655654383,
          0.7344345616264295,
          0.735705209656925,
          0.7382465057179162,
          0.738881829733164,
          0.741423125794155,
          0.741423125794155,
          0.7426937738246506,
          0.7426937738246506,
          0.747141041931385,
          0.7484116899618806,
          0.7515883100381194,
          0.7554002541296061,
          0.7554002541296061,
          0.7560355781448539,
          0.7573062261753494,
          0.7573062261753494,
          0.758576874205845,
          0.7617534942820838,
          0.7630241423125794,
          0.7649301143583227,
          0.7662007623888183,
          0.7662007623888183,
          0.7674714104193139,
          0.7674714104193139,
          0.7687420584498094,
          0.7693773824650572,
          0.7719186785260482,
          0.7757306226175349,
          0.7770012706480305,
          0.7776365946632783,
          0.7789072426937739,
          0.7801778907242694,
          0.7827191867852605,
          0.7833545108005082,
          0.7833545108005082,
          0.7846251588310038,
          0.7846251588310038,
          0.7858958068614994,
          0.7858958068614994,
          0.7871664548919949,
          0.7878017789072427,
          0.7909783989834815,
          0.7909783989834815,
          0.7947903430749682,
          0.795425667090216,
          0.795425667090216,
          0.7966963151207116,
          0.7966963151207116,
          0.7979669631512071,
          0.7992376111817027,
          0.7998729351969505,
          0.7998729351969505,
          0.8017789072426937,
          0.8036848792884371,
          0.8036848792884371,
          0.8055908513341804,
          0.806861499364676,
          0.806861499364676,
          0.8087674714104193,
          0.8087674714104193,
          0.8100381194409149,
          0.8100381194409149,
          0.8113087674714105,
          0.8132147395171537,
          0.8138500635324015,
          0.8151207115628971,
          0.8157560355781448,
          0.8157560355781448,
          0.818297331639136,
          0.8195679796696315,
          0.8195679796696315,
          0.8214739517153749,
          0.8214739517153749,
          0.8227445997458704,
          0.8233799237611181,
          0.8233799237611181,
          0.8246505717916137,
          0.8265565438373571,
          0.8290978398983482,
          0.8290978398983482,
          0.8303684879288437,
          0.8303684879288437,
          0.8316391359593392,
          0.8335451080050826,
          0.8335451080050826,
          0.8348157560355781,
          0.8360864040660737,
          0.8360864040660737,
          0.8367217280813215,
          0.837992376111817,
          0.8386277001270648,
          0.8386277001270648,
          0.8392630241423126,
          0.8392630241423126,
          0.8405336721728082,
          0.841168996188056,
          0.8449809402795425,
          0.8449809402795425,
          0.8475222363405337,
          0.8475222363405337,
          0.8481575603557815,
          0.849428208386277,
          0.8500635324015248,
          0.8513341804320204,
          0.8513341804320204,
          0.8526048284625158,
          0.8526048284625158,
          0.8545108005082592,
          0.8545108005082592,
          0.8576874205844981,
          0.8595933926302414,
          0.8621346886912326,
          0.8621346886912326,
          0.8627700127064803,
          0.8640406607369758,
          0.8646759847522236,
          0.8646759847522236,
          0.866581956797967,
          0.866581956797967,
          0.8678526048284625,
          0.8703939008894537,
          0.8729351969504447,
          0.8729351969504447,
          0.8735705209656925,
          0.8761118170266836,
          0.8761118170266836,
          0.8767471410419314,
          0.8780177890724269,
          0.8780177890724269,
          0.8799237611181703,
          0.8805590851334181,
          0.8805590851334181,
          0.8811944091486659,
          0.8811944091486659,
          0.8831003811944091,
          0.8850063532401525,
          0.8850063532401525,
          0.8869123252858958,
          0.8881829733163914,
          0.8888182973316391,
          0.8888182973316391,
          0.8900889453621347,
          0.8919949174078781,
          0.8932655654383735,
          0.8932655654383735,
          0.8951715374841169,
          0.8951715374841169,
          0.8958068614993647,
          0.8958068614993647,
          0.8970775095298602,
          0.8996188055908514,
          0.9015247776365947,
          0.9027954256670903,
          0.9047013977128335,
          0.9047013977128335,
          0.9072426937738246,
          0.9072426937738246,
          0.909148665819568,
          0.909148665819568,
          0.9097839898348158,
          0.9110546378653113,
          0.9116899618805591,
          0.9142312579415502,
          0.9142312579415502,
          0.9161372299872935,
          0.9161372299872935,
          0.9167725540025413,
          0.9167725540025413,
          0.9193138500635324,
          0.920584498094028,
          0.9224904701397713,
          0.923125794155019,
          0.923125794155019,
          0.9250317662007624,
          0.9250317662007624,
          0.9256670902160101,
          0.9256670902160101,
          0.9269377382465057,
          0.9269377382465057,
          0.9275730622617535,
          0.9275730622617535,
          0.9282083862770013,
          0.9282083862770013,
          0.9294790343074968,
          0.9294790343074968,
          0.9307496823379924,
          0.9307496823379924,
          0.932020330368488,
          0.932020330368488,
          0.9326556543837357,
          0.9326556543837357,
          0.9351969504447268,
          0.9351969504447268,
          0.9371029224904701,
          0.9383735705209657,
          0.9396442185514613,
          0.9396442185514613,
          0.9409148665819568,
          0.9409148665819568,
          0.9415501905972046,
          0.9415501905972046,
          0.9428208386277002,
          0.9428208386277002,
          0.9434561626429478,
          0.9434561626429478,
          0.9447268106734434,
          0.9453621346886912,
          0.9453621346886912,
          0.9466327827191868,
          0.9466327827191868,
          0.9485387547649301,
          0.9498094027954257,
          0.9498094027954257,
          0.951715374841169,
          0.951715374841169,
          0.9529860228716646,
          0.9529860228716646,
          0.9542566709021602,
          0.9542566709021602,
          0.9548919949174078,
          0.9548919949174078,
          0.9555273189326556,
          0.9555273189326556,
          0.9561626429479034,
          0.9561626429479034,
          0.9593392630241423,
          0.9593392630241423,
          0.9599745870393901,
          0.9599745870393901,
          0.9612452350698857,
          0.9618805590851334,
          0.9618805590851334,
          0.9625158831003812,
          0.9625158831003812,
          0.963151207115629,
          0.963151207115629,
          0.9637865311308768,
          0.9637865311308768,
          0.9650571791613723,
          0.9650571791613723,
          0.9663278271918678,
          0.9663278271918678,
          0.9669631512071156,
          0.9669631512071156,
          0.9682337992376112,
          0.9682337992376112,
          0.9695044472681067,
          0.9695044472681067,
          0.9701397712833545,
          0.9701397712833545,
          0.9707750952986023,
          0.9707750952986023,
          0.9720457433290979,
          0.9720457433290979,
          0.9726810673443456,
          0.9726810673443456,
          0.9733163913595934,
          0.9733163913595934,
          0.974587039390089,
          0.974587039390089,
          0.9752223634053367,
          0.9752223634053367,
          0.9764930114358322,
          0.9764930114358322,
          0.97712833545108,
          0.97712833545108,
          0.9783989834815756,
          0.9783989834815756,
          0.9796696315120712,
          0.9796696315120712,
          0.9809402795425667,
          0.9809402795425667,
          0.9822109275730623,
          0.9822109275730623,
          0.9828462515883101,
          0.9828462515883101,
          0.9834815756035579,
          0.9834815756035579,
          0.9847522236340533,
          0.9866581956797967,
          0.9866581956797967,
          0.9879288437102922,
          0.9879288437102922,
          0.9891994917407878,
          0.9898348157560356,
          0.9898348157560356,
          0.9911054637865311,
          0.9911054637865311,
          0.9917407878017789,
          0.9917407878017789,
          0.9923761118170267,
          0.9923761118170267,
          0.9930114358322745,
          0.9930114358322745,
          0.9936467598475223,
          0.9936467598475223,
          0.9942820838627701,
          0.9942820838627701,
          0.9955527318932655,
          0.9955527318932655,
          0.9961880559085133,
          0.9961880559085133,
          0.9968233799237611,
          0.9968233799237611,
          0.9974587039390089,
          0.9974587039390089,
          0.9980940279542567,
          0.9980940279542567,
          0.9987293519695044,
          0.9987293519695044,
          0.9993646759847522,
          0.9993646759847522,
          1,
          1
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "height": 500,
        "legend": {
         "tracegroupgap": 0
        },
        "shapes": [
         {
          "line": {
           "dash": "dash"
          },
          "type": "line",
          "x0": 0,
          "x1": 1,
          "y0": 0,
          "y1": 1
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "CatBoost ROC Curve (AUC=0.9445)"
        },
        "width": 700,
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "False Positive Rate"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "scaleanchor": "x",
         "scaleratio": 1,
         "title": {
          "text": "True Positive Rate"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the (1) false positive rate, (2) true positive rate, and (3) thresholds\n",
    "y_pred_proba = list(y_pred_proba_cat) # converting to list\n",
    "fpr, tpr, thresholds = roc_curve(y_test_cat, y_pred_proba_cat)\n",
    "\n",
    "# Plotting the chart\n",
    "fig = px.area(\n",
    "    x=fpr, y=tpr,\n",
    "    title=f'CatBoost ROC Curve (AUC={auc(fpr, tpr):.4f})',\n",
    "    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n",
    "    width=700, height=500\n",
    ")\n",
    "\n",
    "# This part is just for formatting & adding the dash-line \n",
    "fig.add_shape(\n",
    "    type='line', line=dict(dash='dash'),\n",
    "    x0=0, x1=1, y0=0, y1=1\n",
    ")\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.update_xaxes(constrain='domain')\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\models\\\\catboost.pkl']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(catboost_grid, r'..\\models\\catboost.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lgb = X.copy()\n",
    "# X_lgb['zip_code'] = X_lgb['zip_code'].astype(str)\n",
    "for col in cat_feat:\n",
    "    X_lgb[col] = X_lgb[col].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5174, number of negative: 5174\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 959\n",
      "[LightGBM] [Info] Number of data points in the train set: 10348, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['total_monthly_fee',\n",
       " 'avg_long_distance_fee_monthly',\n",
       " 'total_charges_quarter_cbrt',\n",
       " 'tenure_months',\n",
       " 'avg_gb_download_monthly',\n",
       " 'num_referrals_bin',\n",
       " 'num_dependents_bin',\n",
       " 'contract_type',\n",
       " 'zip_code',\n",
       " 'payment_method']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm = lgb.LGBMClassifier()\n",
    "lgbm.fit(X_lgb, Y)\n",
    "\n",
    "feature_importance_lgbm = lgbm.feature_importances_\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "importance_df_lgb = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance_lgbm})\n",
    "importance_df_lgb = importance_df_lgb.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "\n",
    "top10_features_lgb = importance_df_lgb['Feature'].head(10).tolist()\n",
    "top10_features_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lgb, X_test_lgb, y_train_lgb, y_test_lgb = train_test_split(X_lgb[top10_features_lgb], Y, test_size=0.3, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001362 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000537 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000788 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000513 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000436 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000853 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000804 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000538 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000479 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000804 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000314 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000567 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000904 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000710 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000566 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000849 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000460 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000516 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000676 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000411 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000626 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000737 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000498 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000477 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000601 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000563 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000624 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000381 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001188 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000689 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000766 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000562 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000525 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000519 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000532 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000680 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000556 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001838 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000760 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000741 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001689 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000679 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001526 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000651 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000643 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001428 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000597 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000376 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000398 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000715 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000682 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000501 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000632 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000683 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000675 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000380 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001337 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000693 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000534 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000678 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000446 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000320 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000591 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000611 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003285 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000377 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001694 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000755 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000539 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003878 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001003 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000760 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000583 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000505 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001517 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000543 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000659 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000411 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000600 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000874 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000528 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001112 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000710 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000702 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000614 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000745 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000530 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002173 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000594 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000605 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000454 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000696 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000335 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000544 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000690 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000680 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000640 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000658 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001162 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001928 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000568 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001078 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000379 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000539 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000528 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000833 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000675 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000588 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000482 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000627 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000773 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000595 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000612 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000616 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000685 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000698 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000576 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000654 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000755 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000579 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000336 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000538 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001518 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002053 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000695 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000583 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000345 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000563 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000544 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000640 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001138 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000396 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000376 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000682 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000647 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000868 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000663 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000371 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000869 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000554 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000946 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000525 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000569 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000649 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000558 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000817 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000563 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000629 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000742 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000548 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000548 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000384 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001571 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000714 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000670 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000504 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000721 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000567 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000510 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000553 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000641 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000618 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000806 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004242 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001644 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000518 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000626 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000544 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000632 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000566 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000561 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000756 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003431 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000354 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000825 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001517 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000549 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002115 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000635 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000722 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000808 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000531 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000403 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000913 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000596 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002100 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000557 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000647 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000987 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000610 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000597 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000532 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000681 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000736 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000633 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000641 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000348 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000521 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000644 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001769 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000593 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000620 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000405 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000558 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000532 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000581 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000593 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001051 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000600 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000595 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000583 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000857 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000581 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000565 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000731 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000993 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000964 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000519 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000685 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000551 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000673 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001650 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000571 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000562 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000647 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000636 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000657 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001035 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000824 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000932 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001597 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000673 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000631 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001322 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000615 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000577 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000330 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000672 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000687 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000671 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000449 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000805 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000645 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000634 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000576 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000661 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001078 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003080 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002673 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000736 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000642 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000822 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001093 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000556 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000693 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000722 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000378 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001801 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001735 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001816 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000435 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000739 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000624 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000601 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000833 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000616 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000681 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000565 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000704 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000550 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000540 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000622 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000653 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000569 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000682 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000439 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000608 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000662 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000494 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000534 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000548 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000678 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000689 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001303 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002862 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001097 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001754 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000592 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000687 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000407 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001907 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000604 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000685 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000524 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000627 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000804 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000879 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000561 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002868 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000786 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000686 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000426 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000648 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000690 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000792 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000707 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000648 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000728 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000731 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000710 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000636 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000520 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000659 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000752 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000902 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000790 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000778 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000419 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000719 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000728 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000668 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000605 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5794, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497066 -> initscore=-0.011736\n",
      "[LightGBM] [Info] Start training from score -0.011736\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000772 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 2880, number of negative: 2915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000539 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496980 -> initscore=-0.012080\n",
      "[LightGBM] [Info] Start training from score -0.012080\n",
      "[LightGBM] [Info] Number of positive: 3600, number of negative: 3643\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000799 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 910\n",
      "[LightGBM] [Info] Number of data points in the train set: 7243, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497032 -> initscore=-0.011874\n",
      "[LightGBM] [Info] Start training from score -0.011874\n",
      "Best Hyperparameters found: {'learning_rate': 0.2, 'max_depth': 20, 'n_estimators': 500, 'num_leaves': 31}\n",
      "Accuracy 0.9069243156199678\n",
      "Recall 0.9625158831003812\n",
      "Precision 0.8681948424068768\n",
      "F1 Score 0.9129255799939741\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'num_leaves': [20, 31, 40],  # Controls complexity\n",
    "    'learning_rate': [0.01, 0.1, 0.2],  # Step size\n",
    "    'n_estimators': [100, 200, 500],  # Number of trees\n",
    "    'max_depth': [-1, 10, 20],  # Tree depth (-1 = unlimited)\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(lgbm, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "grid_search.fit(X_train_lgb, y_train_lgb)\n",
    "\n",
    "y_pred_proba_lgb= grid_search.predict_proba(X_test_lgb)[:,1]\n",
    "y_pred_lgb = grid_search.predict(X_test_lgb)\n",
    "\n",
    "print(\"Best Hyperparameters found:\", grid_search.best_params_)\n",
    "\n",
    "print(\"Accuracy\", metrics.accuracy_score(y_test_lgb, y_pred_lgb))\n",
    "print(\"Recall\", metrics.recall_score(y_test_lgb, y_pred_lgb))\n",
    "print(\"Precision\", metrics.precision_score(y_test_lgb, y_pred_lgb))\n",
    "print(\"F1 Score\", metrics.f1_score(y_test_lgb, y_pred_lgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "fillpattern": {
          "shape": ""
         },
         "hovertemplate": "False Positive Rate=%{x}<br>True Positive Rate=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "stackgroup": "1",
         "type": "scatter",
         "x": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0006531678641410843,
          0.0013063357282821686,
          0.0013063357282821686,
          0.0013063357282821686,
          0.0013063357282821686,
          0.0013063357282821686,
          0.0013063357282821686,
          0.0013063357282821686,
          0.001959503592423253,
          0.001959503592423253,
          0.001959503592423253,
          0.002612671456564337,
          0.002612671456564337,
          0.002612671456564337,
          0.002612671456564337,
          0.0032658393207054214,
          0.0032658393207054214,
          0.0032658393207054214,
          0.0032658393207054214,
          0.0032658393207054214,
          0.0032658393207054214,
          0.0032658393207054214,
          0.0032658393207054214,
          0.0032658393207054214,
          0.0032658393207054214,
          0.003919007184846506,
          0.003919007184846506,
          0.003919007184846506,
          0.0045721750489875895,
          0.0045721750489875895,
          0.0045721750489875895,
          0.0045721750489875895,
          0.0045721750489875895,
          0.0045721750489875895,
          0.0045721750489875895,
          0.0045721750489875895,
          0.0045721750489875895,
          0.0045721750489875895,
          0.0045721750489875895,
          0.0045721750489875895,
          0.0045721750489875895,
          0.0045721750489875895,
          0.0045721750489875895,
          0.0045721750489875895,
          0.005225342913128674,
          0.005225342913128674,
          0.005225342913128674,
          0.005225342913128674,
          0.005878510777269758,
          0.005878510777269758,
          0.006531678641410843,
          0.006531678641410843,
          0.006531678641410843,
          0.006531678641410843,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007184846505551927,
          0.007838014369693011,
          0.007838014369693011,
          0.009144350097975179,
          0.009144350097975179,
          0.009797517962116264,
          0.009797517962116264,
          0.009797517962116264,
          0.009797517962116264,
          0.009797517962116264,
          0.009797517962116264,
          0.010450685826257348,
          0.010450685826257348,
          0.010450685826257348,
          0.010450685826257348,
          0.010450685826257348,
          0.011103853690398433,
          0.011103853690398433,
          0.011103853690398433,
          0.011103853690398433,
          0.011103853690398433,
          0.011103853690398433,
          0.011103853690398433,
          0.011757021554539516,
          0.011757021554539516,
          0.011757021554539516,
          0.011757021554539516,
          0.011757021554539516,
          0.011757021554539516,
          0.011757021554539516,
          0.012410189418680601,
          0.012410189418680601,
          0.012410189418680601,
          0.012410189418680601,
          0.012410189418680601,
          0.012410189418680601,
          0.012410189418680601,
          0.012410189418680601,
          0.012410189418680601,
          0.012410189418680601,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013063357282821686,
          0.013716525146962769,
          0.013716525146962769,
          0.013716525146962769,
          0.013716525146962769,
          0.013716525146962769,
          0.013716525146962769,
          0.013716525146962769,
          0.013716525146962769,
          0.013716525146962769,
          0.013716525146962769,
          0.013716525146962769,
          0.013716525146962769,
          0.014369693011103853,
          0.014369693011103853,
          0.014369693011103853,
          0.015022860875244938,
          0.015022860875244938,
          0.015022860875244938,
          0.015022860875244938,
          0.015022860875244938,
          0.015022860875244938,
          0.015676028739386023,
          0.015676028739386023,
          0.015676028739386023,
          0.015676028739386023,
          0.015676028739386023,
          0.015676028739386023,
          0.015676028739386023,
          0.015676028739386023,
          0.015676028739386023,
          0.015676028739386023,
          0.015676028739386023,
          0.015676028739386023,
          0.015676028739386023,
          0.015676028739386023,
          0.015676028739386023,
          0.015676028739386023,
          0.015676028739386023,
          0.015676028739386023,
          0.015676028739386023,
          0.015676028739386023,
          0.016329196603527107,
          0.016329196603527107,
          0.016329196603527107,
          0.016329196603527107,
          0.016329196603527107,
          0.016329196603527107,
          0.016329196603527107,
          0.016329196603527107,
          0.016329196603527107,
          0.016329196603527107,
          0.016329196603527107,
          0.016329196603527107,
          0.016329196603527107,
          0.016329196603527107,
          0.016329196603527107,
          0.016329196603527107,
          0.016329196603527107,
          0.016982364467668192,
          0.016982364467668192,
          0.016982364467668192,
          0.016982364467668192,
          0.016982364467668192,
          0.016982364467668192,
          0.017635532331809273,
          0.017635532331809273,
          0.018288700195950358,
          0.018288700195950358,
          0.018288700195950358,
          0.018288700195950358,
          0.018941868060091443,
          0.018941868060091443,
          0.018941868060091443,
          0.018941868060091443,
          0.019595035924232528,
          0.019595035924232528,
          0.019595035924232528,
          0.019595035924232528,
          0.019595035924232528,
          0.019595035924232528,
          0.019595035924232528,
          0.020248203788373612,
          0.020248203788373612,
          0.020248203788373612,
          0.020248203788373612,
          0.020248203788373612,
          0.020248203788373612,
          0.020248203788373612,
          0.020248203788373612,
          0.020901371652514697,
          0.020901371652514697,
          0.020901371652514697,
          0.020901371652514697,
          0.02155453951665578,
          0.02155453951665578,
          0.02155453951665578,
          0.02155453951665578,
          0.02155453951665578,
          0.02155453951665578,
          0.02155453951665578,
          0.022207707380796866,
          0.022207707380796866,
          0.022207707380796866,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.022860875244937948,
          0.023514043109079032,
          0.023514043109079032,
          0.023514043109079032,
          0.023514043109079032,
          0.023514043109079032,
          0.023514043109079032,
          0.024167210973220117,
          0.024167210973220117,
          0.024167210973220117,
          0.024167210973220117,
          0.024167210973220117,
          0.024167210973220117,
          0.024167210973220117,
          0.024167210973220117,
          0.024167210973220117,
          0.024167210973220117,
          0.024167210973220117,
          0.024820378837361202,
          0.024820378837361202,
          0.024820378837361202,
          0.025473546701502287,
          0.025473546701502287,
          0.025473546701502287,
          0.025473546701502287,
          0.025473546701502287,
          0.025473546701502287,
          0.025473546701502287,
          0.025473546701502287,
          0.025473546701502287,
          0.025473546701502287,
          0.025473546701502287,
          0.025473546701502287,
          0.025473546701502287,
          0.025473546701502287,
          0.025473546701502287,
          0.025473546701502287,
          0.025473546701502287,
          0.026779882429784456,
          0.026779882429784456,
          0.026779882429784456,
          0.026779882429784456,
          0.026779882429784456,
          0.026779882429784456,
          0.026779882429784456,
          0.026779882429784456,
          0.026779882429784456,
          0.026779882429784456,
          0.026779882429784456,
          0.026779882429784456,
          0.026779882429784456,
          0.026779882429784456,
          0.026779882429784456,
          0.026779882429784456,
          0.027433050293925537,
          0.027433050293925537,
          0.027433050293925537,
          0.027433050293925537,
          0.027433050293925537,
          0.028086218158066622,
          0.028086218158066622,
          0.028739386022207707,
          0.028739386022207707,
          0.02939255388634879,
          0.02939255388634879,
          0.03069888961463096,
          0.03069888961463096,
          0.03069888961463096,
          0.03069888961463096,
          0.031352057478772045,
          0.031352057478772045,
          0.031352057478772045,
          0.031352057478772045,
          0.031352057478772045,
          0.031352057478772045,
          0.031352057478772045,
          0.031352057478772045,
          0.031352057478772045,
          0.03200522534291313,
          0.03200522534291313,
          0.03200522534291313,
          0.03200522534291313,
          0.03200522534291313,
          0.03200522534291313,
          0.03200522534291313,
          0.03200522534291313,
          0.03200522534291313,
          0.03200522534291313,
          0.03200522534291313,
          0.03200522534291313,
          0.03200522534291313,
          0.03200522534291313,
          0.03200522534291313,
          0.03200522534291313,
          0.03200522534291313,
          0.03200522534291313,
          0.03200522534291313,
          0.03200522534291313,
          0.03200522534291313,
          0.03200522534291313,
          0.03200522534291313,
          0.03200522534291313,
          0.032658393207054215,
          0.032658393207054215,
          0.032658393207054215,
          0.032658393207054215,
          0.033311561071195296,
          0.033311561071195296,
          0.033311561071195296,
          0.033311561071195296,
          0.033311561071195296,
          0.033964728935336384,
          0.033964728935336384,
          0.033964728935336384,
          0.033964728935336384,
          0.033964728935336384,
          0.033964728935336384,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.034617896799477466,
          0.03527106466361855,
          0.03527106466361855,
          0.03527106466361855,
          0.03527106466361855,
          0.03527106466361855,
          0.035924232527759635,
          0.035924232527759635,
          0.035924232527759635,
          0.035924232527759635,
          0.035924232527759635,
          0.035924232527759635,
          0.037230568256041804,
          0.037230568256041804,
          0.037230568256041804,
          0.037230568256041804,
          0.037230568256041804,
          0.037230568256041804,
          0.037883736120182886,
          0.037883736120182886,
          0.037883736120182886,
          0.037883736120182886,
          0.037883736120182886,
          0.037883736120182886,
          0.037883736120182886,
          0.037883736120182886,
          0.038536903984323974,
          0.038536903984323974,
          0.038536903984323974,
          0.038536903984323974,
          0.039190071848465055,
          0.039190071848465055,
          0.039190071848465055,
          0.039843239712606136,
          0.039843239712606136,
          0.039843239712606136,
          0.039843239712606136,
          0.040496407576747225,
          0.040496407576747225,
          0.040496407576747225,
          0.040496407576747225,
          0.040496407576747225,
          0.040496407576747225,
          0.040496407576747225,
          0.040496407576747225,
          0.040496407576747225,
          0.040496407576747225,
          0.040496407576747225,
          0.040496407576747225,
          0.041802743305029394,
          0.041802743305029394,
          0.041802743305029394,
          0.041802743305029394,
          0.041802743305029394,
          0.042455911169170475,
          0.042455911169170475,
          0.04310907903331156,
          0.04310907903331156,
          0.043762246897452645,
          0.043762246897452645,
          0.043762246897452645,
          0.043762246897452645,
          0.04441541476159373,
          0.04441541476159373,
          0.045068582625734814,
          0.045068582625734814,
          0.045068582625734814,
          0.045068582625734814,
          0.045068582625734814,
          0.045068582625734814,
          0.045068582625734814,
          0.045721750489875895,
          0.045721750489875895,
          0.047028086218158065,
          0.047028086218158065,
          0.047028086218158065,
          0.04768125408229915,
          0.04768125408229915,
          0.04768125408229915,
          0.048334421946440234,
          0.048334421946440234,
          0.048334421946440234,
          0.048334421946440234,
          0.048334421946440234,
          0.048334421946440234,
          0.04898758981058132,
          0.04898758981058132,
          0.04898758981058132,
          0.049640757674722404,
          0.049640757674722404,
          0.049640757674722404,
          0.049640757674722404,
          0.049640757674722404,
          0.049640757674722404,
          0.05094709340300457,
          0.05094709340300457,
          0.05094709340300457,
          0.05094709340300457,
          0.05094709340300457,
          0.05094709340300457,
          0.05225342913128674,
          0.05225342913128674,
          0.052906596995427824,
          0.052906596995427824,
          0.05355976485956891,
          0.05355976485956891,
          0.05355976485956891,
          0.05421293272370999,
          0.05421293272370999,
          0.05421293272370999,
          0.05421293272370999,
          0.05551926845199216,
          0.05551926845199216,
          0.056172436316133244,
          0.056172436316133244,
          0.0581319399085565,
          0.0581319399085565,
          0.0581319399085565,
          0.0581319399085565,
          0.05878510777269758,
          0.05878510777269758,
          0.05878510777269758,
          0.05878510777269758,
          0.05943827563683867,
          0.05943827563683867,
          0.05943827563683867,
          0.06009144350097975,
          0.06009144350097975,
          0.06074461136512083,
          0.06074461136512083,
          0.06139777922926192,
          0.06139777922926192,
          0.06139777922926192,
          0.06270411495754409,
          0.06270411495754409,
          0.06270411495754409,
          0.06270411495754409,
          0.06270411495754409,
          0.06531678641410843,
          0.06531678641410843,
          0.06662312214239059,
          0.06662312214239059,
          0.06662312214239059,
          0.06727629000653168,
          0.06727629000653168,
          0.06727629000653168,
          0.06858262573481384,
          0.06858262573481384,
          0.06923579359895493,
          0.06923579359895493,
          0.06988896146309602,
          0.06988896146309602,
          0.06988896146309602,
          0.06988896146309602,
          0.0705421293272371,
          0.0705421293272371,
          0.07119529719137818,
          0.07119529719137818,
          0.07119529719137818,
          0.07119529719137818,
          0.07119529719137818,
          0.07380796864794252,
          0.07380796864794252,
          0.07380796864794252,
          0.07380796864794252,
          0.07446113651208361,
          0.07446113651208361,
          0.07511430437622468,
          0.07511430437622468,
          0.07511430437622468,
          0.07576747224036577,
          0.07576747224036577,
          0.07576747224036577,
          0.07576747224036577,
          0.07707380796864795,
          0.07707380796864795,
          0.07772697583278902,
          0.07772697583278902,
          0.07772697583278902,
          0.08099281515349445,
          0.08099281515349445,
          0.08164598301763554,
          0.08164598301763554,
          0.08229915088177661,
          0.08229915088177661,
          0.0829523187459177,
          0.0829523187459177,
          0.0829523187459177,
          0.08360548661005879,
          0.08360548661005879,
          0.08360548661005879,
          0.08425865447419988,
          0.08425865447419988,
          0.08491182233834095,
          0.08491182233834095,
          0.08491182233834095,
          0.08621815806662313,
          0.08621815806662313,
          0.0868713259307642,
          0.0868713259307642,
          0.08817766165904638,
          0.08817766165904638,
          0.09209666884389288,
          0.09209666884389288,
          0.0953625081645983,
          0.0953625081645983,
          0.09601567602873938,
          0.09601567602873938,
          0.0999346832135859,
          0.0999346832135859,
          0.0999346832135859,
          0.10254735467015023,
          0.10254735467015023,
          0.10646636185499674,
          0.10646636185499674,
          0.11038536903984324,
          0.11038536903984324,
          0.11365120836054866,
          0.11365120836054866,
          0.116263879817113,
          0.116263879817113,
          0.116263879817113,
          0.116263879817113,
          0.11757021554539517,
          0.11757021554539517,
          0.11822338340953625,
          0.11822338340953625,
          0.11952971913781842,
          0.11952971913781842,
          0.12083605486610059,
          0.12083605486610059,
          0.12606139777922926,
          0.12606139777922926,
          0.13128674069235793,
          0.13128674069235793,
          0.1338994121489223,
          0.1338994121489223,
          0.1404310907903331,
          0.1404310907903331,
          0.15088177661659047,
          0.15088177661659047,
          0.15545395166557804,
          0.15545395166557804,
          0.16002612671456565,
          0.16002612671456565,
          0.16263879817112997,
          0.16263879817112997,
          0.16394513389941215,
          0.16394513389941215,
          0.16917047681254083,
          0.16917047681254083,
          0.1743958197256695,
          0.1743958197256695,
          0.17504898758981058,
          0.17504898758981058,
          0.17570215545395165,
          0.17570215545395165,
          0.20117570215545394,
          0.20117570215545394,
          0.21423905943827565,
          0.21423905943827565,
          0.2240365774003919,
          0.2240365774003919,
          0.22599608099281515,
          0.22599608099281515,
          0.23775310254735468,
          0.23775310254735468,
          0.23905943827563683,
          0.23905943827563683,
          0.24167210973220118,
          0.24167210973220118,
          0.2599608099281515,
          0.2599608099281515,
          0.2795558458523841,
          0.2795558458523841,
          0.2984977139124755,
          0.2984977139124755,
          0.30633572828216854,
          0.30633572828216854,
          0.3161332462442848,
          0.3161332462442848,
          0.32201175702155455,
          0.32201175702155455,
          0.3233180927498367,
          0.3233180927498367,
          0.32984977139124755,
          0.32984977139124755,
          0.3605486610058785,
          0.3605486610058785,
          0.3690398432397126,
          0.3690398432397126,
          0.395166557805356,
          0.395166557805356,
          0.4036577400391901,
          0.4036577400391901,
          0.43043762246897455,
          0.43043762246897455,
          0.4323971260613978,
          0.4323971260613978,
          0.457217504898759,
          0.457217504898759,
          0.4637491835401698,
          0.4637491835401698,
          0.4689745264532985,
          0.4689745264532985,
          0.47485303723056826,
          0.47485303723056826,
          0.47811887655127366,
          0.47811887655127366,
          0.4813847158719791,
          0.4813847158719791,
          0.5075114304376225,
          0.5075114304376225,
          0.5212279555845852,
          0.5212279555845852,
          0.527106466361855,
          0.527106466361855,
          1
         ],
         "xaxis": "x",
         "y": [
          0,
          0.0006353240152477764,
          0.0012706480304955528,
          0.0038119440914866584,
          0.0044472681067344345,
          0.0063532401524777635,
          0.009529860228716646,
          0.010800508259212199,
          0.01207115628970775,
          0.013341804320203304,
          0.01397712833545108,
          0.015247776365946633,
          0.017789072426937738,
          0.01905972045743329,
          0.02096569250317662,
          0.02096569250317662,
          0.022236340533672173,
          0.022871664548919948,
          0.025412960609911054,
          0.026048284625158832,
          0.027318932655654382,
          0.02986022871664549,
          0.031130876747141042,
          0.03176620076238882,
          0.03430749682337993,
          0.035578144853875476,
          0.03748411689961881,
          0.03811944091486658,
          0.03939008894536213,
          0.04383735705209657,
          0.04510800508259212,
          0.045743329097839895,
          0.048284625158831,
          0.05019059720457433,
          0.05082592121982211,
          0.056543837357052096,
          0.05781448538754765,
          0.060991105463786534,
          0.060991105463786534,
          0.062261753494282084,
          0.06480304955527319,
          0.06607369758576874,
          0.06670902160101652,
          0.06797966963151207,
          0.06861499364675985,
          0.06861499364675985,
          0.06925031766200762,
          0.07052096569250317,
          0.07052096569250317,
          0.07179161372299873,
          0.07306226175349428,
          0.07496823379923762,
          0.07496823379923762,
          0.07560355781448538,
          0.07687420584498093,
          0.07941550190597205,
          0.08132147395171538,
          0.08195679796696315,
          0.08703939008894536,
          0.08767471410419314,
          0.08958068614993647,
          0.09085133418043202,
          0.09085133418043202,
          0.09593392630241424,
          0.09720457433290978,
          0.09720457433290978,
          0.09911054637865312,
          0.10292249047013977,
          0.10546378653113088,
          0.10673443456162643,
          0.10864040660736976,
          0.10991105463786531,
          0.11181702668360864,
          0.11245235069885642,
          0.11372299872935197,
          0.11880559085133419,
          0.1207115628970775,
          0.12388818297331639,
          0.12579415501905972,
          0.12642947903430748,
          0.1289707750952986,
          0.1289707750952986,
          0.13024142312579415,
          0.13214739517153748,
          0.13278271918678525,
          0.13278271918678525,
          0.13468869123252858,
          0.13468869123252858,
          0.14040660736975857,
          0.1423125794155019,
          0.14294790343074967,
          0.14294790343074967,
          0.14421855146124524,
          0.144853875476493,
          0.14612452350698857,
          0.15120711562897077,
          0.15247776365946633,
          0.1531130876747141,
          0.15438373570520966,
          0.15756035578144853,
          0.1588310038119441,
          0.1613722998729352,
          0.1639135959339263,
          0.16518424396442186,
          0.16645489199491742,
          0.16772554002541296,
          0.16899618805590852,
          0.17090216010165185,
          0.1721728081321474,
          0.17344345616264295,
          0.17471410419313851,
          0.17598475222363405,
          0.1772554002541296,
          0.17916137229987295,
          0.18043202033036848,
          0.1823379923761118,
          0.18424396442185514,
          0.18678526048284624,
          0.1880559085133418,
          0.18996188055908514,
          0.1937738246505718,
          0.19504447268106734,
          0.19567979669631513,
          0.19949174078780177,
          0.20012706480304956,
          0.2013977128335451,
          0.2020330368487929,
          0.20330368487928843,
          0.20584498094027953,
          0.20838627700127066,
          0.21092757306226176,
          0.2121982210927573,
          0.21410419313850063,
          0.2153748411689962,
          0.21728081321473952,
          0.21982210927573062,
          0.22109275730622618,
          0.22299872935196952,
          0.22426937738246505,
          0.22490470139771285,
          0.22617534942820838,
          0.22681067344345615,
          0.22871664548919948,
          0.22998729351969505,
          0.23252858958068615,
          0.23570520965692504,
          0.23951715374841168,
          0.2420584498094028,
          0.24396442185514614,
          0.2445997458703939,
          0.24587039390088947,
          0.24650571791613723,
          0.24650571791613723,
          0.247141041931385,
          0.247141041931385,
          0.24841168996188057,
          0.24841168996188057,
          0.2496823379923761,
          0.25095298602287164,
          0.25349428208386277,
          0.2554002541296061,
          0.2560355781448539,
          0.2560355781448539,
          0.25730622617534943,
          0.25857687420584496,
          0.2604828462515883,
          0.2617534942820839,
          0.2617534942820839,
          0.2630241423125794,
          0.26937738246505716,
          0.27064803049555275,
          0.2719186785260483,
          0.2731893265565438,
          0.2738246505717916,
          0.2738246505717916,
          0.27509529860228715,
          0.27573062261753495,
          0.2770012706480305,
          0.2776365946632783,
          0.2795425667090216,
          0.28589580686149935,
          0.28589580686149935,
          0.28653113087674714,
          0.2878017789072427,
          0.29161372299872934,
          0.29224904701397714,
          0.2947903430749682,
          0.295425667090216,
          0.29669631512071154,
          0.29860228716645487,
          0.3017789072426938,
          0.3017789072426938,
          0.30432020330368487,
          0.30559085133418046,
          0.306861499364676,
          0.30813214739517153,
          0.31003811944091486,
          0.31067344345616266,
          0.3119440914866582,
          0.3138500635324015,
          0.31512071156289706,
          0.3170266836086404,
          0.318297331639136,
          0.3195679796696315,
          0.32083862770012705,
          0.3227445997458704,
          0.3240152477763659,
          0.3246505717916137,
          0.32719186785260485,
          0.3284625158831004,
          0.3303684879288437,
          0.3335451080050826,
          0.3360864040660737,
          0.33672172808132145,
          0.33799237611181704,
          0.3386277001270648,
          0.33989834815756037,
          0.34243964421855144,
          0.3443456162642948,
          0.3468869123252859,
          0.34815756035578144,
          0.3519695044472681,
          0.3519695044472681,
          0.35324015247776364,
          0.35387547649301143,
          0.35514612452350697,
          0.35578144853875476,
          0.3570520965692503,
          0.3576874205844981,
          0.3602287166454892,
          0.36149936467598476,
          0.3640406607369759,
          0.3646759847522236,
          0.36721728081321475,
          0.36721728081321475,
          0.3684879288437103,
          0.3703939008894536,
          0.3703939008894536,
          0.37166454891994916,
          0.3754764930114358,
          0.37738246505717915,
          0.3811944091486658,
          0.3850063532401525,
          0.3850063532401525,
          0.3856416772554003,
          0.3869123252858958,
          0.38881829733163914,
          0.39135959339263027,
          0.3951715374841169,
          0.39644218551461247,
          0.3983481575603558,
          0.39961880559085133,
          0.40088945362134687,
          0.40152477763659467,
          0.4027954256670902,
          0.40470139771283353,
          0.40533672172808133,
          0.40660736975857686,
          0.4110546378653113,
          0.41296060991105465,
          0.4142312579415502,
          0.4161372299872935,
          0.41804320203303685,
          0.41804320203303685,
          0.420584498094028,
          0.42249047013977126,
          0.4250317662007624,
          0.42820838627700125,
          0.4307496823379924,
          0.4320203303684879,
          0.4332909783989835,
          0.43392630241423125,
          0.4364675984752224,
          0.43900889453621345,
          0.44027954256670904,
          0.4409148665819568,
          0.44218551461245237,
          0.4440914866581957,
          0.44536213468869124,
          0.44663278271918677,
          0.44663278271918677,
          0.4485387547649301,
          0.4491740787801779,
          0.45171537484116897,
          0.4548919949174079,
          0.45616264294790343,
          0.45616264294790343,
          0.45870393900889456,
          0.45870393900889456,
          0.4593392630241423,
          0.46315120711562896,
          0.46378653113087676,
          0.46378653113087676,
          0.46823379923761116,
          0.4707750952986023,
          0.4714104193138501,
          0.4714104193138501,
          0.4720457433290978,
          0.4733163913595934,
          0.4783989834815756,
          0.47903430749682335,
          0.48030495552731894,
          0.4809402795425667,
          0.4809402795425667,
          0.48284625158831,
          0.4841168996188056,
          0.48602287166454894,
          0.4872935196950445,
          0.48856416772554,
          0.4923761118170267,
          0.4936467598475222,
          0.4936467598475222,
          0.49618805590851334,
          0.49682337992376113,
          0.5006353240152478,
          0.5006353240152478,
          0.5012706480304956,
          0.5031766200762389,
          0.5038119440914867,
          0.5050825921219823,
          0.5057179161372299,
          0.5088945362134689,
          0.5088945362134689,
          0.5108005082592122,
          0.5120711562897078,
          0.5120711562897078,
          0.5146124523506989,
          0.5152477763659467,
          0.5165184243964421,
          0.5177890724269377,
          0.5203303684879288,
          0.5209656925031766,
          0.52287166454892,
          0.5241423125794155,
          0.5279542566709021,
          0.5304955527318933,
          0.5317662007623888,
          0.5324015247776366,
          0.5336721728081322,
          0.5362134688691232,
          0.5374841168996188,
          0.5381194409148666,
          0.5393900889453621,
          0.5438373570520966,
          0.5451080050825922,
          0.5451080050825922,
          0.548284625158831,
          0.5495552731893265,
          0.5508259212198221,
          0.5520965692503177,
          0.5527318932655655,
          0.5527318932655655,
          0.5533672172808132,
          0.5559085133418044,
          0.5565438373570522,
          0.5584498094027954,
          0.5603557814485387,
          0.5628970775095299,
          0.5641677255400254,
          0.567979669631512,
          0.5692503176620076,
          0.5705209656925032,
          0.5705209656925032,
          0.571156289707751,
          0.5736975857687421,
          0.5736975857687421,
          0.5800508259212198,
          0.5813214739517154,
          0.5825921219822109,
          0.5844980940279543,
          0.5857687420584498,
          0.5870393900889453,
          0.5883100381194409,
          0.5895806861499364,
          0.5914866581956798,
          0.5927573062261754,
          0.5940279542566709,
          0.5959339263024143,
          0.5965692503176621,
          0.5978398983481575,
          0.5984752223634053,
          0.5997458703939009,
          0.5997458703939009,
          0.6003811944091486,
          0.602287166454892,
          0.6035578144853876,
          0.6073697585768743,
          0.6092757306226175,
          0.6105463786531131,
          0.6111817026683609,
          0.6124523506988564,
          0.6143583227445998,
          0.6156289707750953,
          0.6162642947903431,
          0.6181702668360864,
          0.6188055908513341,
          0.6226175349428208,
          0.6257941550190598,
          0.6257941550190598,
          0.6296060991105463,
          0.6321473951715375,
          0.633418043202033,
          0.6353240152477764,
          0.6353240152477764,
          0.6359593392630242,
          0.6359593392630242,
          0.6372299872935197,
          0.6372299872935197,
          0.6378653113087674,
          0.6378653113087674,
          0.6385006353240152,
          0.6410419313850063,
          0.6423125794155019,
          0.6423125794155019,
          0.6435832274459975,
          0.644853875476493,
          0.6467598475222364,
          0.6473951715374842,
          0.6493011435832274,
          0.6499364675984752,
          0.6512071156289708,
          0.6531130876747141,
          0.6531130876747141,
          0.6543837357052097,
          0.656289707750953,
          0.6575603557814486,
          0.6581956797966964,
          0.6594663278271918,
          0.6632782719186785,
          0.6645489199491741,
          0.6651842439644219,
          0.6664548919949174,
          0.6670902160101652,
          0.6683608640406608,
          0.6709021601016518,
          0.6759847522236341,
          0.6785260482846251,
          0.6804320203303685,
          0.681702668360864,
          0.6855146124523507,
          0.6874205844980941,
          0.6886912325285895,
          0.6918678526048284,
          0.6944091486658196,
          0.6956797966963151,
          0.6969504447268107,
          0.6969504447268107,
          0.6975857687420585,
          0.6988564167725541,
          0.7001270648030495,
          0.7001270648030495,
          0.7039390088945362,
          0.7052096569250318,
          0.7058449809402796,
          0.7071156289707751,
          0.7071156289707751,
          0.7102922490470139,
          0.7128335451080051,
          0.7141041931385006,
          0.716010165184244,
          0.7172808132147395,
          0.7172808132147395,
          0.7198221092757306,
          0.7210927573062261,
          0.7223634053367217,
          0.7242693773824651,
          0.7261753494282084,
          0.7280813214739518,
          0.7306226175349428,
          0.7325285895806861,
          0.7337992376111817,
          0.7344345616264295,
          0.7344345616264295,
          0.7369758576874206,
          0.738881829733164,
          0.7401524777636594,
          0.7452350698856417,
          0.7452350698856417,
          0.7465057179161372,
          0.7490470139771284,
          0.7503176620076238,
          0.7515883100381194,
          0.7547649301143583,
          0.7547649301143583,
          0.7560355781448539,
          0.7573062261753494,
          0.758576874205845,
          0.7592121982210928,
          0.7604828462515884,
          0.7604828462515884,
          0.7630241423125794,
          0.764294790343075,
          0.7674714104193139,
          0.7693773824650572,
          0.770012706480305,
          0.7719186785260482,
          0.7738246505717916,
          0.7738246505717916,
          0.7750952986022872,
          0.7763659466327827,
          0.7770012706480305,
          0.7770012706480305,
          0.7789072426937739,
          0.7795425667090216,
          0.7795425667090216,
          0.7808132147395171,
          0.7814485387547649,
          0.7827191867852605,
          0.7827191867852605,
          0.783989834815756,
          0.7865311308767471,
          0.7871664548919949,
          0.7890724269377383,
          0.7897077509529861,
          0.7916137229987293,
          0.7922490470139771,
          0.795425667090216,
          0.7966963151207116,
          0.7986022871664549,
          0.7998729351969505,
          0.7998729351969505,
          0.8017789072426937,
          0.8043202033036849,
          0.8062261753494282,
          0.806861499364676,
          0.806861499364676,
          0.8081321473951716,
          0.8081321473951716,
          0.8087674714104193,
          0.8087674714104193,
          0.8094027954256671,
          0.8106734434561627,
          0.8119440914866582,
          0.8119440914866582,
          0.8125794155019059,
          0.8125794155019059,
          0.8132147395171537,
          0.8144853875476493,
          0.8163913595933926,
          0.8170266836086404,
          0.8189326556543838,
          0.8214739517153749,
          0.8214739517153749,
          0.8227445997458704,
          0.8227445997458704,
          0.8240152477763659,
          0.8259212198221093,
          0.8259212198221093,
          0.8284625158831004,
          0.8290978398983482,
          0.8290978398983482,
          0.8329097839898348,
          0.8348157560355781,
          0.8360864040660737,
          0.8367217280813215,
          0.837992376111817,
          0.837992376111817,
          0.8405336721728082,
          0.8418043202033036,
          0.8418043202033036,
          0.8424396442185514,
          0.843710292249047,
          0.8449809402795425,
          0.8462515883100381,
          0.8468869123252859,
          0.8468869123252859,
          0.849428208386277,
          0.8513341804320204,
          0.8545108005082592,
          0.8564167725540025,
          0.8576874205844981,
          0.8576874205844981,
          0.8589580686149937,
          0.8589580686149937,
          0.8595933926302414,
          0.8595933926302414,
          0.8602287166454892,
          0.8614993646759848,
          0.8614993646759848,
          0.8627700127064803,
          0.8640406607369758,
          0.8653113087674714,
          0.8653113087674714,
          0.8659466327827192,
          0.8659466327827192,
          0.866581956797967,
          0.866581956797967,
          0.8678526048284625,
          0.8684879288437103,
          0.8697585768742059,
          0.8697585768742059,
          0.8710292249047014,
          0.872299872935197,
          0.8742058449809402,
          0.8742058449809402,
          0.8754764930114358,
          0.8792884371029225,
          0.8792884371029225,
          0.8811944091486659,
          0.8811944091486659,
          0.8818297331639136,
          0.8818297331639136,
          0.8831003811944091,
          0.8843710292249047,
          0.8843710292249047,
          0.8856416772554002,
          0.8869123252858958,
          0.8888182973316391,
          0.8900889453621347,
          0.8900889453621347,
          0.8913595933926303,
          0.8913595933926303,
          0.8951715374841169,
          0.8970775095298602,
          0.8970775095298602,
          0.8996188055908514,
          0.9008894536213469,
          0.9008894536213469,
          0.9027954256670903,
          0.9027954256670903,
          0.9034307496823379,
          0.9034307496823379,
          0.9040660736975857,
          0.9053367217280813,
          0.9059720457433291,
          0.9059720457433291,
          0.9072426937738246,
          0.9072426937738246,
          0.9078780177890724,
          0.9097839898348158,
          0.9104193138500636,
          0.9116899618805591,
          0.9116899618805591,
          0.9135959339263025,
          0.9148665819567979,
          0.9167725540025413,
          0.9167725540025413,
          0.9174078780177891,
          0.9174078780177891,
          0.9186785260482846,
          0.9199491740787802,
          0.9199491740787802,
          0.9212198221092758,
          0.923125794155019,
          0.9237611181702668,
          0.9237611181702668,
          0.9243964421855146,
          0.9243964421855146,
          0.9256670902160101,
          0.9263024142312579,
          0.9263024142312579,
          0.9269377382465057,
          0.9269377382465057,
          0.9275730622617535,
          0.9275730622617535,
          0.9282083862770013,
          0.9282083862770013,
          0.9307496823379924,
          0.9313850063532402,
          0.9313850063532402,
          0.9332909783989835,
          0.934561626429479,
          0.934561626429479,
          0.9351969504447268,
          0.9351969504447268,
          0.9364675984752223,
          0.9383735705209657,
          0.9383735705209657,
          0.9396442185514613,
          0.9396442185514613,
          0.940279542566709,
          0.940279542566709,
          0.9415501905972046,
          0.9415501905972046,
          0.9421855146124524,
          0.9421855146124524,
          0.9434561626429478,
          0.9434561626429478,
          0.9440914866581956,
          0.9440914866581956,
          0.9453621346886912,
          0.945997458703939,
          0.945997458703939,
          0.9479034307496823,
          0.9479034307496823,
          0.9485387547649301,
          0.9485387547649301,
          0.9491740787801779,
          0.9491740787801779,
          0.9510800508259212,
          0.9510800508259212,
          0.951715374841169,
          0.9536213468869124,
          0.9548919949174078,
          0.9548919949174078,
          0.9555273189326556,
          0.9555273189326556,
          0.9561626429479034,
          0.9561626429479034,
          0.9567979669631512,
          0.9567979669631512,
          0.9580686149936467,
          0.9580686149936467,
          0.9606099110546379,
          0.9606099110546379,
          0.9618805590851334,
          0.9618805590851334,
          0.9644218551461246,
          0.9644218551461246,
          0.96569250317662,
          0.96569250317662,
          0.9669631512071156,
          0.9669631512071156,
          0.9675984752223634,
          0.9675984752223634,
          0.9682337992376112,
          0.9682337992376112,
          0.9695044472681067,
          0.9695044472681067,
          0.9707750952986023,
          0.9707750952986023,
          0.9714104193138501,
          0.9714104193138501,
          0.9720457433290979,
          0.9720457433290979,
          0.9726810673443456,
          0.9726810673443456,
          0.9733163913595934,
          0.9733163913595934,
          0.9739517153748412,
          0.9739517153748412,
          0.974587039390089,
          0.974587039390089,
          0.9752223634053367,
          0.9752223634053367,
          0.9764930114358322,
          0.9764930114358322,
          0.97712833545108,
          0.97712833545108,
          0.9783989834815756,
          0.9783989834815756,
          0.9790343074968234,
          0.9790343074968234,
          0.9796696315120712,
          0.9796696315120712,
          0.9803049555273189,
          0.9803049555273189,
          0.9809402795425667,
          0.9809402795425667,
          0.9828462515883101,
          0.9828462515883101,
          0.9834815756035579,
          0.9834815756035579,
          0.9841168996188056,
          0.9841168996188056,
          0.9853875476493011,
          0.9853875476493011,
          0.9860228716645489,
          0.9860228716645489,
          0.9872935196950444,
          0.9872935196950444,
          0.98856416772554,
          0.98856416772554,
          0.9891994917407878,
          0.9891994917407878,
          0.9904701397712834,
          0.9904701397712834,
          0.9911054637865311,
          0.9911054637865311,
          0.9923761118170267,
          0.9923761118170267,
          0.9936467598475223,
          0.9936467598475223,
          0.9942820838627701,
          0.9942820838627701,
          0.9955527318932655,
          0.9955527318932655,
          0.9968233799237611,
          0.9968233799237611,
          0.9974587039390089,
          0.9974587039390089,
          0.9980940279542567,
          0.9980940279542567,
          0.9987293519695044,
          0.9987293519695044,
          0.9993646759847522,
          0.9993646759847522,
          1,
          1
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "height": 500,
        "legend": {
         "tracegroupgap": 0
        },
        "shapes": [
         {
          "line": {
           "dash": "dash"
          },
          "type": "line",
          "x0": 0,
          "x1": 1,
          "y0": 0,
          "y1": 1
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "LightGBM ROC Curve (AUC=0.9639)"
        },
        "width": 700,
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "False Positive Rate"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "scaleanchor": "x",
         "scaleratio": 1,
         "title": {
          "text": "True Positive Rate"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the (1) false positive rate, (2) true positive rate, and (3) thresholds\n",
    "y_pred_proba = list(y_pred_proba_lgb) # converting to list\n",
    "fpr, tpr, thresholds = roc_curve(y_test_lgb, y_pred_proba_lgb)\n",
    "\n",
    "# Plotting the chart\n",
    "fig = px.area(\n",
    "    x=fpr, y=tpr,\n",
    "    title=f'LightGBM ROC Curve (AUC={auc(fpr, tpr):.4f})',\n",
    "    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n",
    "    width=700, height=500\n",
    ")\n",
    "\n",
    "# This part is just for formatting & adding the dash-line \n",
    "fig.add_shape(\n",
    "    type='line', line=dict(dash='dash'),\n",
    "    x0=0, x1=1, y0=0, y1=1\n",
    ")\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.update_xaxes(constrain='domain')\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Model\\\\LightGBM.pkl']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(grid_search, r'..\\models\\LightGBM.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
